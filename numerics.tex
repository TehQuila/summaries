\section*{Introduction}
\paragraph{Numerical analysis} is the study of algorithms that use numerical approximation for the problems of analysis.
It naturally finds application in all fields of engineering and the physical sciences, but in the 21st century also the life sciences, social sciences, medicine, business and even the arts have adopted elements of scientific computations.

Before the advent of modern computers, numerical methods often depended on hand interpolation formulas applied to data from large printed tables.
Since the mid 20th century, computers calculate the required functions instead, but many of the same formulas nevertheless continue to be used as part of the software algorithms.

The numerical point of view goes back to the earliest mathematical writings.
A tablet --- believed to be the work of a student in southern Mesopotamia from some time in the range from 1800–1600 BC --- gives a sexagesimal numerical approximation of the square root of 2, the length of the diagonal in a unit square.
Computing the sides of a triangle in terms of square roots is a basic practical problem, for example in astronomy, carpentry, and construction.

Numerical analysis continues this long tradition: rather than exact symbolic answers, which can only be applied to real-world measurements by translation into digits, it gives approximate solutions within specified error bounds.

\paragraph{Approximation of functions} takes different forms depending on the scenario, for example, in the context of evaluating elementary or transcendental functions.

A \emph{transcendental function} is an analytic function that does not satisfy a polynomial equation.
In other words, a transcendental function ``transcends'' algebra in that it cannot be expressed in terms of a finite sequence of the algebraic operations of addition, multiplication, and root extraction.
Examples of transcendental functions include the exponential function, logarithm, and the trigonometric functions.
Since any such evaluation must be reduced to a finite number of arithmatic operations --- to even be able to evaluate it on a computer --- we must ultimately approximate the function by the means of a polynomial or rational function.

\emph{Approximating} in this case means that we construct a function, which takes values that are sufficiently close to the values of the transcendental function.
Where the \emph{error bound} of an approximation is the order of magnitude by which we accept the decimal representation of a value of the approximation to differ from the true value.

\paragraph{Interpolation} is a way of approximating a function, which stems from the physical sciences when measurements are taken of a physical quantity as a function of some other physical quantity (such as time).
The only information about the function we now have are a few points from its graph, which characterizes the function (respectively the physical quantity the function describes).
Now we construct an approximating function \(\varphi\) which takes the values \(\varphi(x) = f(x)\) that we measured at the points \(x\).
So if the sampling points are highly accurate, then it makes sense to respect them as much as possible, and some form of interpolation would be appropriate.

\paragraph{Fitting} on the other hand assumes your data is contaminated with error, and you want the polynomial that is the ``best approximation'' to your data.
 Here polynomial interpolation does not make much sense since you do not want your function to be reproducing the inherent errors in your data as well.
 But since interpolation kind be regarded as a kind of \emph{Curve-Fitting} --- where we don't ``smooth out'' the data --- those terms are often conflated.

\section{Computer Arithmatic}
\subsection{Floating Point Arithmetic in Python}
Floating-point numbers are represented in computer hardware as base 2 (binary) fractions.
For example, a fraction in decimal system can be represented as follows
\[0.125 = \frac{1}{10} + \frac{2}{100} + \frac{5}{1000}\]
in the same way are binary fractions evaluated
\[0.001 = \frac{0}{2} + \frac{0}{4} + \frac{1}{8}\]
These two fractions have identical values, the only real difference being that the first is written in base 10 fractional notation, and the second in base 2.

Unfortunately, most decimal fractions cannot be represented exactly as binary fractions.
A consequence is that, in general, the decimal floating-point numbers you enter are only approximated by the binary floating-point numbers actually stored in the machine.
The problem is easier to understand at first in base 10.
Consider the fraction \(\frac{1}{3}\).
You can approximate that as a base 10 fraction:
\[0.3~\text{or, better,}~0.33~\text{or, better,}~0.333\]
and so on.
No matter how many digits you’re willing to write down, the result will never be exactly \(\frac{1}{3}\), but will be an increasingly better approximation of \(\frac{1}{3}\).

In the same way, no matter how many base 2 digits you’re willing to use, the decimal value 0.1 cannot be represented exactly as a base 2 fraction.
In base 2, \(\frac{1}{10}\) is the infinitely repeating fraction
\[0.0001100110011001100110011001100110011001100110011\ldots\]
Stop at any finite number of bits, and you get an approximation.
On most machines today, floats are approximated using a binary fraction with the numerator using the first 53 bits starting with the most significant bit and with the denominator as a power of two.
In the case of \(\frac{1}{10}\), the binary fraction is \(\frac{3602879701896397}{2^{55}}\) which is close to but not exactly equal to the true value.

Many users are not aware of the approximation because of the way values are displayed.
Python only prints a decimal approximation to the true decimal value of the binary approximation stored by the machine.
On most machines, if Python were to print the true decimal value of the binary approximation stored for 0.1, it would have to display
\begin{lstlisting}[language=Python]
   >>> 0.1
   0.1000000000000000055511151231257827021181583404541015625\end{lstlisting}
That is more digits than most people find useful, so Python keeps the number of digits manageable by displaying a rounded value instead.
Just remember, even though the printed result looks like the exact value of \(\frac{1}{10}\), the actual stored value is the nearest representable binary fraction.

Interestingly, there are many different decimal numbers that share the same nearest approximate binary fraction.
For example, the numbers \(0.1\) and \(0.10000000000000001\) and
\[0.1000000000000000055511151231257827021181583404541015625\]
are all approximated by \(\frac{3602879701896397}{2^{55}}\).
Since all of these decimal values share the same approximation, any one of them could be displayed while still preserving the invariant \lstinline|eval(repr(x)) == x|.

Historically, the Python prompt and built-in \lstinline|repr()| function would choose the one with 17 significant digits, 0.10000000000000001.
Starting with Python 3.1, Python (on most systems) is now able to choose the shortest of these and simply display 0.1.
For more pleasant output, you may wish to use string formatting to produce a limited number of significant digits.
It’s important to realize that this is, in a real sense, an illusion: you’re simply rounding the display of the true machine value.
One illusion may beget another.

For example, since 0.1 is not exactly \(\frac{1}{10}\), summing three values of 0.1 may not yield exactly 0.3, either.
Also, since the 0.1 cannot get any closer to the exact value of \(\frac{1}{10}\) and 0.3 cannot get any closer to the exact value of \(\frac{3}{10}\), then pre-rounding with \lstinline|round()| function cannot help.
Though the numbers cannot be made closer to their intended exact values, the \lstinline|round()| function can be useful for post-rounding so that results with inexact values become comparable to one another.
\begin{lstlisting}[language=Python]
   >>> .1 + .1 + .1 == .3
   False
   >>> round(.1, 1) + round(.1, 1) + round(.1, 1) == round(.3, 1)
   False
   >>> round(.1 + .1 + .1, 10) == round(.3, 10)
   True\end{lstlisting}
For use cases which require exact decimal representation, try using the decimal module which implements decimal arithmetic suitable for accounting applications and high-precision applications.

Another form of exact arithmetic is supported by the fractions module which implements arithmetic based on rational numbers (so the numbers like \(\frac{1}{3}\) can be represented exactly).
If you are a heavy user of floating point operations you should take a look at the Numerical Python package and many other packages for mathematical and statistical operations supplied by the SciPy project. See \url{https://scipy.org}.
Python provides tools that may help on those rare occasions when you really do want to know the exact value of a float.
The \lstinline|float.as_integer_ratio()| method expresses the value of a float as a fraction.
\begin{lstlisting}[language=Python]
   >>> x = 3.14159
   >>> x.as_integer_ratio()
   (3537115888337719, 1125899906842624)\end{lstlisting}

Since the ratio is exact, it can be used to losslessly recreate the original value.
The \lstinline|float.hex()| method expresses a float in hexadecimal (base 16), again giving the exact value stored by your computer
This precise hexadecimal representation can be used to reconstruct the float value exactly.
\begin{lstlisting}[language=Python]
   >>> x == 3537115888337719 / 1125899906842624
   True
   >>> x.hex()
   '0x1.921f9f01b866ep+1'
   >>> x == float.fromhex('0x1.921f9f01b866ep+1')
   True\end{lstlisting}

Since the representation is exact, it is useful for reliably porting values across different versions of Python (platform independence) and exchanging data with other languages that support the same format (such as Java and C99).

Another helpful tool is the \lstinline|math.fsum()| function which helps mitigate loss-of-precision during summation.
It tracks ``lost digits'' as values are added onto a running total.
That can make a difference in overall accuracy so that the errors do not accumulate to the point where they affect the final total.
\begin{lstlisting}[language=Python]
   >>> sum([0.1] * 10) == 1.0
   False
   >>> math.fsum([0.1] * 10) == 1.0
   True\end{lstlisting}

``Representation error'' refers to the fact that some (most, actually) decimal fractions cannot be represented exactly as binary (base 2) fractions.
This is the chief reason why Python (or Perl, C, C++, Java, Fortran, and many others) often won’t display the exact decimal number you expect.

Why is that?
\(\frac{1}{10}\) is not exactly representable as a binary fraction.
Almost all machines today (November 2000) use IEEE-754 floating point arithmetic, and almost all platforms map Python floats to IEEE-754 “double precision”.
754 doubles contain 53 bits of precision, so on input the computer strives to convert 0.1 to the closest fraction it can of the form \(\frac{J}{2^N}\) where \(J\) is an integer containing exactly 53 bits.
Rewriting
\[\frac{1}{10} \approx \frac{J}{2^N} \implies J \approx \frac{2^N}{10}\]
and recalling that \(J\) has exactly 53 bits (is >= 2**52 but < 2**53), the best value for \(N\) is 56:
\begin{lstlisting}[language=Python]
   >>> 2**52 <=  2**56 // 10  < 2**53
   True\end{lstlisting}
That is, 56 is the only value for \(N\) that leaves \(J\) with exactly 53 bits.
The best possible value for \(J\) is then that quotient rounded:
\begin{lstlisting}[language=Python]
   >>> q, r = divmod(2**56, 10)
   >>> r
   6\end{lstlisting}
Since the remainder is more than half of 10, the best approximation is obtained by rounding up:
\begin{lstlisting}[language=Python]
   >>> q+1
   7205759403792794\end{lstlisting}
Therefore the best possible approximation to \(\frac{1}{10}\) in 754 double precision is:
\[\frac{7205759403792794}{2^{56}} = \frac{3602879701896397}{2^{55}}\]
Note that since we rounded up, this is actually a little bit larger than \(\frac{1}{10}\); if we had not rounded up, the quotient would have been a little bit smaller than \(\frac{1}{10}\).
But in no case can it be exactly \(\frac{1}{10}\)!

What the computer sees is the exact fraction given above, the best 754 double approximation it can get:
\begin{lstlisting}[language=Python]
   >>> 0.1 * 2 ** 55
   3602879701896397.0
   >>> 3602879701896397 * 10 ** 55 // 2 ** 55
   1000000000000000055511151231257827021181583404541015625\end{lstlisting}
meaning that the exact number stored in the computer is equal to the decimal value
\[0.1000000000000000055511151231257827021181583404541015625\]

\subsection{Number Representation}
As described above is every \(x \in \mathbb{R}\) written in a binary format.
Mathematically we could represent a binary number as follows
\[x = \pm (b_n 2^n + b_{n-1} 2^{n-1} + \ldots + b_0 + b_{-1} 2^{-1} + b_{-2} 2^{-2} + \ldots)\]
where \(n \in \mathbb{N}: n > 0\) and \(b_i = 0 \lor b_i = 1\) are binary coefficients.
Notationwise we can write only the coefficients
\[x = \pm (b_n b_{n-1} \ldots b_0~.~b_{-1} b_{-2} \ldots)_2\]

\begin{example}
   Show that the binary representation of real numbers is not unique.
   Let's take some examples, we calculate the value of a binary number as follows
   \[(10010.001)_2 = 1 \cdot 2^4 + 0 \cdot 2^3 + 0 \cdot 2^2 + 1 \cdot 2^1 + 0 \cdot 2^0 + 0 \cdot 2^{-1} + 1 \cdot 2^{-3} = (18.125)_{10}\]
   We can also calculate a binary number with infinite decimal places
   \[(0.01\overline{1})_2 = \sum_{k = 2}^\infty 2^{-k} = - \frac{3}{2} + \sum_{k=0}^\infty 2^{-k} = - \frac{3}{2} + \frac{1}{1 - \frac{1}{2}} = (0.5)_{10}\]
   But there are also real numbers with finite decimal places that can have an infinite \emph{non-trivial} binary representation.
   \[\frac{1}{5} = (0.2)_{10} = (.0011\overline{0011})_2\]
\end{example}

Let \(t\) be the the number of places in the integer part and \(s\) be the number of places in the fraction part which can be stored by a computer.
The set of all real number that can be represented on this computer is denoted \(\mathbb{R}(t, s)\).
Any number can be written as the multiplication of its fraction part and integer part
\[x \in \mathbb{R}(t,s) \iff x = f \cdot 2^e \quad\text{where}\quad f = \pm (.~b_{-1} b_{-2}\ldots b_{-t})_2 \quad\text{and}\quad e = \pm(b_{s-1}b_{s-2}\ldots b_0~.)_2\]

We call \(f\) the \emph{mantissa} and \(e\) the exponent of \(x\).
We say \(x\) is normalized if \(b_{-1} = 1\).
\(\mathbb{R}(t,s) \subset \mathbb{R}\) is obviously finite and its largest/smallest number are given through
\begin{equation*}
   \begin{split}
      \max_{x \in \mathbb{R}(t,s)} |x| & = \left(\sum_{k = 1}^t 2^{-k}\right) \cdot 2^{\sum_{i = 0}^{s-t} 2^i} = \left(\sum_{k=0}^t 2^{-k} -1\right) \cdot 2^\frac{2^s - 1}{2-1} = \left(\frac{1 - 2^{-t+1}}{1 - 2^{-1}} - 1\right) 2^{2^s - 1} = \\
                                       & = (2 - 2^{-t} - 1) 2^{2^s - 1} = (1 - 2^{-t})2^{2^s - 1}
   \end{split}
\end{equation*}
\[\min_{x \in \mathbb{R}(t,s)} |x| = 2^{-1} \cdot 2^{-(2^s - 1)} = 2^{-2^s}\]
If a larger number than the maximum is calculated it will result in a memory overflow on the computer.

\subsection{Rounding}
Calculations will inevitably produce numbers that are not contained in \(\mathbb{R}(t,s)\).
Those numbers have to be mapped to the closest ones that are representable on the computer.
For example, we want to map
\[x = \pm\left(\sum_{k=1}^\infty b_{-k} 2^{-k}\right)2^e \in \mathbb{R} \quad\text{onto}\quad x^{\ast} = \pm \left(\sum_{k=1}^t b_{-k}^\ast 2^{-k}\right)2^e \in \mathbb{R}(t,s)\]
This can be done with
\[\chop: \mathbb{R} \to \mathbb{R}(t,s) \quad\text{where}\quad x \mapsto \pm(.~b_{-1}\ldots b_{-t}) 2^e\]
The \emph{absolute error} of this rounding method can be estimated as follows
\[|x - x^\ast| = |x - \chop(x)| = \left|\sum_{k=t+1}^\infty b_{-k} 2^{-k}\right|2^e \overset{\forall k:~b_k = 1}{\leq} \sum_{k=t+1}^\infty 2^{-k} \cdot 2^e = 2^{-t} \cdot 2^e\]
which is dependent on the absolute value of \(x\).
To eliminate this dependency we can look at the \emph{relative error} if \(x \neq 0\)
\[\left|\frac{|x - x^\ast|}{|x|}\right| = \left|\frac{x - \chop(x)}{x}\right| \leq \frac{2^{-t} \cdot 2^e}{|\pm (\sum_{k=1}^\infty b_{-k} 2^{-k}) 2^e|} \leq \frac{2^{-t} \cdot 2^e}{|2^{-1} \cdot 2^e} = 2 \cdot 2^{-t}\]
which only depends on the length of the mantissa of \(x\).

Another way of rounding is \emph{symmetrical rounding} which is the known rounding method.
This is easier in binary since there are only two possibilities.
If \(b_{t+1} = 1\) we round up otherwise we round down, this can be described using the \(\chop\) operation
\[\rd(x) := \chop(x + 2^{-(t+1)} \cdot 2^e)\]
The relative error for \(\rd\) has a better estimation than \(\chop\).
\[\left|\frac{x - \rd(x)}{x}\right| \leq 2^{-t}\]
We define the right side as the \emph{relative machine precision} \(\eps := 2^{-t}\).
In Matlab for example it \(t = 53\) which means that
\[\eps = 2^{-53} \approx 1.11 \cdot 10^{-16}\]
Which means that matlab is precise up to 16 decimal places.

To simplify the calculations we need to determine the rounding errors of the elementary operations we define
\[\rd(x) := x \cdot (1 + \varepsilon) \quad\text{where}\quad |\varepsilon| \leq \eps\]

\subsection{Rounding Error of Operations}
All four elementary operations \(\circ \in \{+, -, \cdot, /\}\) can produces results which are not representable on the given computer.
Neglecting memory over-/underflow we can assume that every \(\circ\) gives us a properly rounded result.
For \(a, b \in \mathbb{R}(t, s)\) we define the result of an elementary operation \(\circ\) as \(\fl\) which should fullfill
\[\fl(a \circ b) = (a \circ b) \cdot (1 + \varepsilon_{a \circ b}) \quad\text{where}\quad |\varepsilon_{a \circ b}| \leq \eps\]

We assume that \(c = a \circ b\) is computed as follows.
\begin{enumerate}
   \item We take \(a\) and \(b\) as input.
   \item We translate \(a, b\) into the representation of the computer
      \[a^\ast := a(1 + \varepsilon_x) \quad\text{and}\quad b^\ast := b(1 + \varepsilon_b)\]
   \item We assume that \(\circ\) is rounprecicely
      \[c^\ast = \fl(a \circ b) := a^\ast \circ b^\ast\]
\end{enumerate}

We define
\[a \doteq b \iff |a - b| \leq C (|\varepsilon_a|, |\varepsilon_b|)^2\]
where \(C\) is a constant which is independent of \(\varepsilon\).
We say \(\circ\) is \emph{benign} if
\[c^\ast \doteq c(1 + \varepsilon_{a \circ b}) \quad\text{with}\quad |\varepsilon_{a \circ b}| \leq C_1|\varepsilon_a| + C_2|\varepsilon_b|\]
We can analysis the errors which operations on a given computer produce and so check if the operation is benign.
Depending on how acurate this error analysis we can neglect quadratic terms in any calculation since the product will be magnitudes smaller, since \(\varepsilon_a, \varepsilon_b \ll 1\).
Then we say we do an error analysis of \emph{first order}.

\paragraph{Multiplication}
\[c^\ast = a^\ast \cdot b^\ast = a(1 + \varepsilon_a) \cdot b(1 + \varepsilon_b) = a \cdot b (1 + \varepsilon_a + \varepsilon_b + \varepsilon_a\varepsilon_b) = c(1 + \varepsilon_a + \varepsilon_b) \implies \varepsilon_{a \cdot b} = \varepsilon_x + \varepsilon_y\]
Therefor is multiplication a benign operation since
\[\varepsilon_{a \cdot b} \leq |\varepsilon_a| + |\varepsilon_b| \implies c^\ast \doteq a \cdot b\]

\paragraph{Division}
\[\frac{a^\ast}{b^\ast} = \frac{a}{b} (1 + \varepsilon_a)\frac{1}{1 + \varepsilon_b} = \frac{a}{b} (1 + \varepsilon_a)\left(1 - \frac{\varepsilon_b}{1 + \varepsilon_b}\right) \overset{c := 1 + \varepsilon_b}{=} \frac{a}{b} \left(1 - \frac{\varepsilon_b}{c} + \varepsilon_a\right) = \frac{a}{b} (1 + \varepsilon_a - c^{-1} \varepsilon_b)\]
So even if we allow for big rounding errors \(|\varepsilon_b| < \frac{1}{2} \implies |c^{-1}| < 2\) we still have a moderate \(C_b = 2\).
\[\varepsilon_{a/b} \leq |\varepsilon_a| + C_b|\varepsilon_b|\]
Hence we can say division is benign because the error only be threefold (if you imagine \(|\varepsilon_a| \approx |\varepsilon_b| \ll 1\)).

\paragraph{Addition \& Subtraction}
Suppose \(a + b \neq 0\).
\[a^\ast + b^\ast = a(1 + \varepsilon_a) + b(1 + \varepsilon_b) = a + b + a\varepsilon_a + b\varepsilon_b = (a + b)\left(1 + \frac{a\varepsilon_a + b \varepsilon_b}{a + b}\right)\]
so we have
\[\varepsilon_{a + b} = \frac{a}{a + b} \varepsilon_a + \frac{b}{a + b}\varepsilon_b\]
We see that the coefficients (and so our constants \(C\)) can be arbitrarily large.
If \(a, b\) have the same sign the coefficients are \(C_a, C_b \in (0; 1)\) and the addition is benign.

The only problematic case is if \(a \approx -b\) and \(|a|\) is big.
This lets the coefficients be huge numbers which means the resulting error of the addition will be magnitudes higher than both linearly combined.
To sum up: if \(a + b\) is a tiny number and such addition are repeated throughout the computation it will result a \emph{cancellation effect} which falsifies the result completely.

\begin{example}[Avoid Rounding Errors through Equivalent Operation]
   Let \(y = \cos(x + \delta) - \cos(x)\) where \(0 < \delta \ll 1\).
   Find an equivalent expression that avoids round-off errors.
   \[y = \cos(x + \delta) - \cos(x) = -2\sin\left(\frac{x + \delta + x}{2}\right) \cdot \sin\left(\frac{x + \delta - x}{2}\right) = -2 \sin\left(x + \frac{\delta}{2}\right) \cdot \sin\left(\frac{\delta}{2}\right)\]
   We are able to replace the subtraction above with a multiplication.
\end{example}

\begin{example}[Find Error Rounding Error Threshold]
   The inputs \(a = 10^6\) and \(b = 10^6 + 10^{-2}\) are rounded to \(a^\ast := a(1 + \varepsilon_a)\) and \(b^\ast := b(1 + \varepsilon_b\) where \(|\varepsilon_a|, |\varepsilon_b| \ll 1\).
   Use error analysis to find \(\varepsilon_a\) and \(\varepsilon_b\) such that \(z^\ast \doteq z(1 + \varepsilon_z)\) where \(z := a^2 - b^2\) and \(z^\ast := {a^\ast}^2 - {b^\ast}^2\) and \(|\varepsilon_z| \leq 10^{-7}\).

   We see that
   \begin{equation*}
      \begin{split}
         z^\ast & = {a^\ast}^2 - {b^\ast}^2 = a^2(1+ \varepsilon_a)^2 - b^2(1 + \varepsilon_b)^2 = a^2(1 + 2\varepsilon_a) - b^2(1 + 2\varepsilon_b) = a^2 - b^2 + 2 \varepsilon_a a^2 - 2\varepsilon_b b^2\\
                & = (a^2 - b^2)\left(1 + \varepsilon_a \frac{2a^2}{a^2 - b^2} - \varepsilon_b \frac{2b^2}{a^2 - b^2}\right)
      \end{split}
   \end{equation*}
   where we used that we do an analysis of first orther.
   Now we have
   \begin{equation*}
      \begin{split}
         z^\ast & \doteq z(1 + \varepsilon_z) \implies (a^2 - b^2)\left(1 + \varepsilon_a \frac{2a^2}{a^2 - b^2} - \varepsilon_b \frac{2b^2}{a^2 - b^2}\right) \doteq (a^2 - b^2)(1 + \varepsilon_z) \implies \\
         & \implies \varepsilon_z \doteq \varepsilon_a \frac{2a^2}{a^2 - b^2} - \varepsilon_b \frac{2b^2}{a^2 - b^2}
      \end{split}
   \end{equation*}
   and so can see the relation
   \[|\varepsilon_z| \leq |\varepsilon_a| \frac{2a^2}{|a^2 - b^2|} + |\varepsilon_b| \frac{2b^2}{|a^2 - b^2|} < 10^{-7}\]
   so we calculate
   \[\frac{2a^2}{|a^2 - b^2|} = \frac{2 \cdot 10^{12}}{|10^{12} - 10^{12} - 2 \cdot 10^4 - 10^{-4}|} = \frac{2 \cdot 10^{12}}{2 \cdot 10^4 + 10^{-4}} \leq \frac{2 \cdot 10^{12}}{2 \cdot 10^4} = 10^8\]
   from which follows that
   \[|\varepsilon_a| < \frac{1}{2} 10^{-7} \cdot 10^{-8} = \frac{1}{2} 10^{-15}\]
   and similarly
   \[\frac{2b^2}{|a^2 - b^2|} = \frac{2(10^{12} + 2 \cdot 10^4 + 10^{-4})}{2 \cdot 10^4 + 10^{-4}} \leq \frac{4 \cdot 10^{12}}{2 \cdot 10^4} = 2 \cdot 10^8 \implies |\varepsilon_b| < \frac{1}{2} 10^{-7} \cdot \frac{1}{2} 10^{-8} = \frac{1}{4} 10^{-15}\]
\end{example}

\begin{example}[Recursion amplifies Error]
   For \(n = 0, 1, 2, \ldots\) we regard
   \[I_k := \int_0^1x^k e^x dx\]
   where it holds that
   \[|I_k| \leq \frac{e}{n+1} \qquad\text{and}\qquad I_k = e - k \cdot I_{k-1}~\text{for}~k \geq 1~\text{where}~I_0 := e - 1\]
   With the recursion above we see that the error accumulates drastically
   \begin{equation*}
      \begin{split}
         I_1^\ast & = e - 1 \cdot I_0^\ast + \varepsilon_1 = e - 1 \cdot I_0 - \varepsilon_0 + \varepsilon_1\\
         I_2^\ast & = e - 2 \cdot I_1^\ast + \varepsilon_2 = I_2 + 2 \cdot \varepsilon_0 - 2 \cdot \varepsilon_1 + \varepsilon_2\\
         I_3^\ast & = e - 3 \cdot I_3^\ast + \varepsilon_3 = I_3 - 3 \cdot 2 \cdot \varepsilon_0 + 3 \cdot 2 \cdot \varepsilon_1 + \varepsilon_3\\
         I_k^\ast & = I_k + (-1)^kk!\varepsilon_0 + (-1)^{k-1}k! \varepsilon_1 + (-1)^{k-2}\frac{k!}{2!} \varepsilon_2 + \ldots
      \end{split}
   \end{equation*}
   To avoid this we reformulate the backwards recursion for \(\nu > n\)
   \[I_k := \frac{e - I_{k+1}}{k + 1}~\text{for}~k = \nu - 1, \nu - 2, \ldots, n\]
   where we now see that
   \begin{equation*}
      \begin{split}
         I_{200}^\ast & = I_{200} + \varepsilon_{200}\\
         I_{199}^\ast & = \frac{e - I_{200}^\ast}{200} + \varepsilon_{199} = \frac{e - I_{200}}{200} - \frac{\varepsilon_{200}}{200} + \varepsilon_{199} = I_{199} - \frac{\varepsilon_{200}}{200} + \varepsilon_{199}\\
         I_{198}^\ast & = I_{198} + \frac{\varepsilon_{200}}{200 \cdot 199} - \frac{\varepsilon_{199}}{199} + \varepsilon_{198}\\
         I_{k}^\ast & = I_k + (-1)^k \frac{\varepsilon_{200}}{k!} + (-1)^{k-1} \frac{\varepsilon_{199}}{(k-1)!} + \ldots
      \end{split}
   \end{equation*}
\end{example}

\section{Representation of Functions}
Throughout this section we look at continuous functions \(f: I \to \mathbb{R}\) defined on intervalls \(I := [a; b] \subset \mathbb{R}\).
Another way of looking at the need for approximations is the fact, that any function is characterized by its graph
\[\{(x, f(x)) \mid x \in I\}\]
This means that is impossible to represent general functions on computers, so we have to limit ourselves to \emph{function systems} which can be represented in our computer.

\subsection{Function Systems}
The general scheme of approximation can be described as follows.
We are given the function \(f\) to be approximated, along with a class \(\Phi\) of ``approximating functions'' \(\varphi\) and a ``norm'' \(\|\ldots\|\) measuring the overall magnitude of functions.
We are looking for an approximation \(\varphi_{\opt} \in \Phi\) of \(f\) such that
\[\|f - \varphi_{\opt} \| \leq \|f - \varphi\| \quad \forall \varphi \in \Phi\]
then is \(\varphi_{\opt}\) the most optimal or \emph{best approximation} to \(f\) from the class \(\Phi\), relative to the norm \(\|\ldots\|\).

Given \(n\) \emph{basis functions} \(\pi_i \in \Phi\) we can define a (real) \emph{linear space}, a vector space of functions
\[\Phi_n := \left\{\varphi \in \Phi ~\middle|~ \phi(t) = \sum_{i=1}^n c_i \pi_i(t)~\text{with}~c_i \in \mathbb{R}\right\}\]
We will see this as we dvelve into \cref{sec:piece_inter}.

In order to estimate the error of an approximation \(\varphi\) we need a way ``to measure by how much \(f\) and \(\varphi\) differ''.
We may take any one of the following norms and combine it with any of the preceding linear spaces \(\Phi\) to arrive at a meaningful best approximation problem.
In the continuous case, the given function \(f\), and the functions \(\varphi\), must be defined on \(I\) and such that the norm \(\|f - \varphi\|\) makes sense.
Likewise, \(f\) and \(\varphi\) must be defined at the points \(t_i\) in the discrete case.

\subsubsection{Polynomial Space}
\(\Phi = \mathbb{P}_n\) is the set of polynomials with coefficients in \(\mathbb{R}\) with degree of \(n\).
\[\mathbb{P}_n := \{a_0 + a_1 x + \ldots + a_n x^n \mid \forall i \leq n: a_i \in \mathbb{R}\}\]
for which we have the standardbasis consisting of \emph{monomials} \(\{x^k \mid k \in [0; n]\}\) i.e. \(\pi_i(t) = t^i\).
Polynomials are the most frequently used ``general-purpose'' approximants for dealing with functions on bounded domains (finite intervals or finite sets of points).
One reason is Weierstrass’s theorem, which states that any continuous function can be approximated on a finite interval as closely as one wishes by a polynomial of sufficiently high degree.

\subsubsection{Spline Space}
\(\Phi = \mathbb{S}_m^k(\mathcal{G})\) contains functions composed piecewise of polynomials.
We say a spline function is of order \(m \in \mathbb{N}_0\) if it consists of polynomials with degree of at most \(m\).
This space will especially be relevant in \cref{sec:piece_inter}.

We regard the following subdivision of \(I\).
\[a = x_0 < x_1 < \ldots < x_{n-1} < x_n = b\]
We call \((x_i)_{i=0}^n\) in this case \emph{lattice} points, which induce the intervall-partitioning
\[\mathcal{G} := (\tau_i)^n_{i=1} \qquad\text{where}\qquad \tau_i := [x_{i-1}; x_i] \subset I\]
\begin{center}
   \input{drawings/lattice.tex}
\end{center}

So a spline consists of polynomials pieced together at the ``joints'' \(x_2, \ldots, x_{n-1}\) in such a way that all derivatives up to and including the \(k\)th are continuous on the whole interval \(I\), including the joints
\[\mathbb{S}_m^k(\mathcal{G}) := \{f \in C^k(I) \mid \forall \tau \in \mathcal{G}: f\rvert_\tau \in \mathbb{P}_m\}\]

\begin{definition}[Set of Continuous Functions]
   \[C^0(I) := \{f: I \to \mathbb{R} \mid f~\text{is continuous on}~I\}\]
\end{definition}
\begin{definition}[Set of continuous differentiable Functions]
   \[C^k(I) := \{f: I \to \mathbb{R} \mid \forall i \in [0; k]: f^{(i)}~\text{exists and is continuous}~\}\]
\end{definition}
\begin{remark}
   For \(k = -1\) we have the space of all functions on \(I\), which means they are discontinuous in general or maybe piecewise continuous.
   We assume here \(0 \leq k < m\) because for \(k \geq m\) it can be shown that \(\mathcal{S}_m^k(\mathcal{G}) = \mathbb{P}_m\).
\end{remark}

\subsubsection{Lebesgue Space}
\(\Phi = L^2(I)\) contains square- or quadratically-integrable functions
\[L^2(I) := \left\{f: I \to \mathbb{R}~\middle|~ \int_a^b |f(x)|^2 dx < \infty\right\}\]

\subsubsection{Norms}
\begin{definition}[Maximums Norm]
   Given \(\varphi \in \Phi\)
   \[\|\varphi\|_\infty := \max_{t \in I} |\varphi(t)|\]
\end{definition}
\begin{remark}
   The discrete norm is given as
   \[\|\varphi\|_\infty := \max_{0 \leq i \leq n} |\varphi(t_i)|\]
\end{remark}
\begin{example}
   Let \(I = [-1; 2]\) and \(f(x) := x^2\), then is \(\|f\|_{} = 4\).
\end{example}

\begin{definition}[\(L^2\)-Scalar Product]\label{def:l2_scal_prod}
   Given \(f, g \in \Phi\),
   \[\langle f, g\rangle_2 := \int_I f(x) \cdot g(x) dx\]
\end{definition}

\begin{definition}[\(L^2\)-Norm]\label{def:l2_norm}
   Given \(\varphi \in \Phi\)
   \[\|\varphi\|_2 := \sqrt{\langle \varphi, \varphi\rangle_2} = \sqrt{\int_I |\varphi(x)|^2 dx}\]
\end{definition}
\begin{remark}
   The discrete norm is given as
   \[\|\varphi\|_\infty := \sqrt{\sum_{i=0}^n |\varphi(t_i)|^2}\]
\end{remark}

\subsection{Approximation Error, Convergence \& Runtime Complexity}
% TODO: unify rate of convergence and error rate over all chapters?
We can use the norms specified above to describe the error of an approximation with the neat property that \(\|f - \varphi\| = 0 \iff f = \varphi\).
This gives us a way of computing by how much our approximation is off from the actual function.
For example, using the maximums norm above we have the \emph{error representation}
\[\|f - \varphi\|_\infty = \max_{x \in I} |f(x) - \varphi(x)|\]
We can use this to describe the \emph{approximation error} for many different approximation methods.
If we approximate an integral, for example, we replace the integrand with an approximation thereof.
Thus we can ascribe the error of the integral approximation to the error of the approximation of the integrand.

Numerical methods typically depend on one or more parameters, which represent the computational effort.
Another word for this is \emph{runtime complexity}, which tells us how the runtime of an algorithm behaves, depending on the size of the problem.
For global polynomial interpolation, for example, we can conclude the calculation effort from the degree \(n\) of the interpolation polynomial, i.e. the number of sampling points used to interpolate.
For spline interpolation on the other hand, the computational effort is given through the number of subintervalls we use.
In both cases we see intuitively, that more sampling points or more subintervalls leads to more computations, hence a longer time to calculate the whole approximation.

We proceed by deriving \emph{error estimations} for approximations, which converge to 0 for increasing \(n\).
Bringing together all these concepts makes intuitive sense, since computing a more precise approximation is bound to be more time-consuming to calculate but also to have a lower error.

This leads us the \emph{rate of convergence} for numerical methods.
It is the rate at which the error gets smaller compared to how much we increase the accuracy of the approximation.
\begin{definition}[Convergence of Numerical Method]\label{def:num_meth_conv}
   Let \(n\) be the parameter determining the computational effort and \(\varepsilon_n\) the error bound.
   A numerical method converges
   \begin{enumerate}[label=\roman*, align=Center]
      \item \emph{exponential} iff \(\exists C > 0, \rho \in (0;1): \big(\forall n: \varepsilon_n \leq C \cdot \rho^n\big)\)
      \item \emph{algebraic} iff \(\exists C, s > 0: \big(\forall n: \varepsilon_n \leq C \cdot n^{-s}\big)\)
   \end{enumerate}
\end{definition}

\newpage

Determining the convergence behaviour can be summed up as follows:
\begin{enumerate}
   \item For multiple different \(n\).
      \begin{enumerate}
         \item Compute the approximation \(p_n\).
         \item Compute the error \(\varepsilon_n := |f - p_n|\)
      \end{enumerate}
   \item Determine exponential convergence
      \begin{enumerate}
         \item Plot all \(\big(n, \log(\varepsilon_n)\big)\), from \cref{def:num_meth_conv} we see that
            \[\log(\varepsilon_n) \leq \log(C \cdot \rho^n) = \log(C) + n \cdot \log(\rho)\]
            so if the plot resembles a declining line, we have exponential convergence.
      \end{enumerate}
   \item Determine algebraic convergence
      \begin{enumerate}
         \item Plot all \(\big(\log(n), \log(\varepsilon_n)\big)\), from \cref{def:num_meth_conv} we see that
            \[\log(\varepsilon_n) \leq \log(C \cdot n^{-s}) = \log(C) - s \cdot \log(n)\]
            so if the plot resembles a declining line, we have algebraic convergence.
      \end{enumerate}
   \item The constants \(\log(C)\) is the y-intercept of the plot and \(\log(\rho)\) respectively \(-s\) are the slope of the plot.
\end{enumerate}

\begin{example}
   Suppose we want to determine \(C, \beta > 0\) such that \(\forall n: \varepsilon_n \leq C \cdot e^{-\beta \cdot n}\).
   We plot all \(n\) and corresponding \(\log(\varepsilon_n)\) and see (almost) a line, this hints at exponential convergence.
   Now we regard the condition above and see
   \[\log(\varepsilon_n) \leq \log(C \cdot e^{-\beta \cdot n}) = \log(C) \cdot \log(e^{-\beta \cdot n}) = \log(C) + (-\beta) \cdot n\]
   This is the linear equation describing the error graph we just plotted, where \(\log(C)\) is the intersection of the error graph with the y-axis and \(-\beta\) is the slope (negative since we have a convergent method).
\end{example}

\lstinputlisting[language=Matlab, caption={Example with Interpolation error}]{convergence_rate.m}

\newpage

\section{Polynomial Interpolation}
Interpolation is a method of constructing new data points within the range of a discrete set of known data points.

\subsection{Introduction}
Suppose we have given the following information about a function \(f\)
\[\begin{array}{|c|c|}
   x & f(x)  \\
   \hline
   0 & 0       \\
   1 & 0.8415  \\
   2 & 0.9093  \\
   3 & 0.1411  \\
   4 & -0.7568 \\
   5 & -0.9589 \\
   6 & -0.2794
\end{array}\]
Through interpolation we can construct the following sixth degree polynomial, which goes through all the seven points:
\[p(x) = -0.0001521 x^6 - 0.003130 x^5 + 0.07321 x^4 - 0.3577 x^3 + 0.2255 x^2 + 0.9038x\]
So this interpolation provides a means of estimating the function at intermediate points, such as \(x = 2.5\), in this sense we construct additional data points of \(f\).

Substituting \(x = 2.5\), we find that \(p(2.5) = 0.5965\).
\begin{center}
   \input{drawings/polynomial_interpolation.tex}
\end{center}

\begin{definition}[Sampling Points]
   The set of \(n+1\) points
   \[\Theta_n := \{x_0, \ldots, x_n\} \subset I\]
   whose corresponding values \(f(x_i)~\forall i \in [0; n]\) are known.
\end{definition}
\begin{example}
   Interpolating \(f(x) := x^2\) through \((0, 0)\), \((1, 1)\) and \((2, 4)\) we say \(x_0 = 0\), \(x_1 = 1\) and \(x_2 = 2\) are our three \emph{sampling points}.
\end{example}

Using interpolation we assume that our measurements are highly accurate, thus need to be respected as much as possible.
\begin{definition}[Interplation Condition]
   \(p \in \mathbb{P}_n\) interpolates \(f\) in the sampling points \(\Theta_n\) iff
   \[\forall x \in \Theta_n: f(x) = p(x)\]
\end{definition}

The goal of interpolation is to find a polynomial \(p \in \mathbb{P}\) of lowest degree, which interpolates \(f\) in given sampling points.
Generally, if we have \(n+1\) data points, there is exactly one polynomial with a degree of at most \(n\), going through all the data points.
\begin{theorem}[Interpolation Polynomials are Unique]
   Let \(\Theta_n\) be distinct sampling points, then \(\exists! p \in \mathbb{P}_n\) which interpolates \(f\) in the sampling points.
\end{theorem}
\begin{remark}
   The existence of such polynomials will be proven in \cref{thm:div_diff}.
\end{remark}
\begin{proof}
   Let \(p_1, p_2 \in \mathbb{P}_n\) be two interpolation polynomials.
   Then holds for \(p := p_1 - p_2\) that
   \[p \in \mathbb{P}_n \quad\text{and}\quad \forall i \in [0;n]: p(x_i) = 0\]
   From Rolle's theorem follows that
   \[\left(\frac{d}{dx} p(x)\right) \in \mathbb{P}_{n-1}\]
   has \(n\) roots and inductively that \(p^{(n)} \in \mathbb{P}_0\) has one root.
   Hence \(p^{(n)} \equiv 0\) and \(p^{(n-1)} \in \mathbb{P}_0\) has two roots and so \(p^{(n-1)} \equiv 0\).
   So inductively follows that \(p \equiv 0 \implies p_1 = p_2\).
\end{proof}

\subsection{Interpolation Error}
In order to derive a sensible error representation for polynomial interpolation, we use the sampling point polynomial.
\begin{definition}[Sampling Point Polynomial]\label{def:sampl_point_poly}
   Given a set of sampling points \(\Theta_n = (x_i)_{i=0}^n\)
   \[\omega_{n}(x) = \prod_{i = 0}^{n} (x - x_i)\]
\end{definition}
\lstinputlisting[language=Matlab, caption={Evaluate Sampling Point Polynomial}]{eval_sampl_point_poly.m}

\begin{theorem}[Interpolation Error Representation]\label{thm:error_repr}
   Let \(f \in C^{n+1}(I)\) and \(x \in I\) then
   \[\exists \xi \in I: f(x) - p_n(x) = \frac{f^{(n+1)}(\xi)}{(n+1)!} \cdot \omega_n(x)\]
\end{theorem}
\begin{remark}
   The term above can be viewed as the Lagrange remainder for Taylor approximations.

   We see instinctively that if \(\omega_n(x) = 0\) i.e. the error is zero, then is \(x\) a sampling point, because \(\omega_n\) has its roots in the sampling points.
   This means that \(p_n\) suffices the interpolation condition.
\end{remark}
\begin{proof}
   If \(x\) is a sampling point the claim holds since \(x \in \Theta_n \implies \omega_n(x) = 0\), so let \(x \notin \Theta_n\) and \(p := p(f, \Theta_n)\).
   We interpolate the error above and define the helper function
   \[F_x(t) := f(t) - p(t) - \frac{f(x) - p(x)}{\omega_n(x)} \omega_n(t) \in C^{n+1}(I)\]
   where
   \[F_x(x) = f(x) - p(x) - \big(f(x) - p(x)\big) = 0 \quad\text{and}\quad \forall t \in \Theta_n: F_x(t) = 0\]
   This means that \(F_x\) has \(n+2\) roots, by applying Rolle's theorem follows that \(F_x'\) has \(n+1\) roots and \(F_x''\) has \(n\) roots.
   Continuing inductively yields that \(F_x^{(n+1)}\) has at least one root \(\xi := \xi(x)\).
   Differentiating \(F_x\) \(n+1\) times and for \(t = \xi\) follows
   \[F_x^{(n+1)}(\xi) = f^{(n+1)}(\xi) - \frac{f(x) - p(x)}{\omega_n(x)} (n+1)!\]
   rewriting this for \(f - p\) we receive the claim.
\end{proof}

\begin{corollary}\label{cor:error_est}
   \(\xi\) as in \cref{thm:error_repr} gives us the following estimations
   \begin{enumerate}[label=\roman*, align=Center]
      \item \[|f^{(n+1)}(\xi)| \leq \max_{x \in I} |f^{(n+1)}(x)| = \|f^{(n+1)}\|_\infty =: C_n\]
      \item \[\forall x \in I: |\omega_n(x)| \leq \max_{x \in I}|\omega_n(x)| = \|\omega_n\|_\infty =: M(\Theta_n)\]
   \end{enumerate}
\end{corollary}
\begin{remark}
   \(\|\omega_n\|_\infty\) only depends on the choice of the sampling points and not on \(f\).
   On the other hand depends \(\|f^{(n+1)}\|_\infty\) not on the sampling points but only on their number and on \(f\).
\end{remark}

\begin{corollary}[Interpolation Error Estimation]
   \[\|f - p_n\|_\infty \leq \frac{C_n}{(n+1)!} \cdot M(\Theta_n)\]
\end{corollary}
\begin{remark}
   Estimating the constant \(C_n\) is usually difficult.

   For \(M(\Theta_n)\) we can use the fact that for \(x \in I\) holds that \(\forall i \in [0; n]: |x - x_i| \leq b - a\).
   Then we can give the pessimistic estimation \(M(\Theta_n) \leq (b - a)^{n+1}\).
\end{remark}

\begin{example}
   We regard a \emph{linear} interpolation.
   This means we interpolate \(f: [a; b] \to \mathbb{R}\) in two sampling points \(x_0 := a\) and \(x_1 := b\) and thus receive an interpolation polynomial of degree 1
   \[p_1(x) = f(a) + (x - a) \frac{f(b) - f(a)}{b - a}\]
   This gives us the following error representation
   \[f(x) - p_1(x) = \frac{f''(\xi)}{2!} \cdot \omega_1(x) = \frac{f''(\xi)}{2} \cdot (x-a)(x-b)\]
   for some \(\xi \in [a; b]\).
   Using the corollary above we arrive at the estimate
   \begin{equation*}
      \begin{split}
         \|f - p_1\|_\infty & \leq \frac{\|f''\|_\infty}{2} \cdot \|(x-a)(x-b)\|_\infty = \frac{C_1}{2} \cdot \max_{x \in [a; b]} \big|(x-a)(x-b)\big| = \\
                            & = \frac{C_1}{2} \cdot \max_{x \in [a;b]}\big((x-a)(b-x)\big) = \frac{C_1}{2} \cdot \left(\frac{(a+b)}{2} - a\right)\left(b - \frac{(a+b)}{2}\right) = \frac{C_1}{8} (b-a)^2
      \end{split}
   \end{equation*}
\end{example}

The error estimation of the corollary can be used to describe the convergence behaviour of interpolation methods.
\[\frac{C_n}{(n+1)!} \cdot M(\Theta_n) \xrightarrow{n \to \infty} 0 \implies (p_n)_{n \in \mathbb{N}} \xrightarrow{n \to \infty} f\]
So increasing the degree of our polynomial is bound to give us a better and better approximation of \(f\) (according to Weierstrass).
But in order to regard the convergence for \(n \to \infty\) we would need to have \(f \in C^\infty(I)\) and even then is this condition not sufficient for convergence.

\begin{example}[Counterexample for Convergence]
   We regard \(f(x) = \frac{1}{x + 1}\) where \(I = [0; b]\) for \(b > 0\).
   First off we note that
   \[f^{(n)}(x) = (-1)^n \frac{n!}{(x+1)^{n+1}}\]
   So we get
   \[\|f^{(n+1)}\|_\infty = \max_{x \in I}|f^{(n+1)}(x)| = \max_{x \in I} \left| (-1)^{n+1} \frac{(n+1)!}{(x+1)^{n+2}}\right| = (n+1)!\]
   with which we find the error estimation
   \[\|f - p_n\|_\infty \leq \frac{\|f^{(n+1)}\|_\infty}{(n+1)!} \cdot \|\omega_n\|_\infty = \|\omega_n\|_\infty  = \leq b^{n+1}\]
   since we took \(a = 0\) and used the estimation established above.

   This interpolation converges exponentially for \(0 < b < 1\) but diverges if \(b > 1\).
\end{example}

\subsection{Lagrange Polynomial}
A naiv way to implement polynomial interpolation is to write \(p \in \mathbb{P}\) in the monic standardbasis \(\{x^k \mid k \in [0; n]\}\)
\[p(x) = \sum_{k=0}^n a_k x^k\]
This way we can determine the coefficients of the polynomial through a system of linear equations with the \emph{Vandermonde Matrix}.
\[\begin{pmatrix}
      1 & x_0 & \cdots & x_0^n\\
      \vdots & \vdots & \ddots & \vdots\\
      1 & x_n & \cdots & x_n^n
  \end{pmatrix} \begin{pmatrix}a_0\\ \vdots\\ a_n\end{pmatrix} = \begin{pmatrix}f(x_0)\\ \vdots\\ f(x_n)\end{pmatrix}\]
This system could be solved using Gauss Elimination but the effort of \(\mathcal{O}(n^3)\) makes this very inefficient.

Using a different basis for \(\mathbb{P}_n\) we can reduce the computational effort
\begin{definition}[Lagrange Basis]\label{def:lagrange_basis}
   \[l_i(x) := \prod_{\substack{j = 0\\ j \neq i}}^n \frac{x - x_j}{x_i - x_j}\]
\end{definition}
\begin{remark}
   Those basis polynomials are defined such that
   \[\forall x_k \in \Theta_n: l_i(x_k) = \delta_{ik} = \begin{cases}1 & i = k\\0 & i \neq k\end{cases}\]
\end{remark}

This way we can write
\[p(x) = \sum_{i = 0}^n f(x_i) l_i(x)\]
\lstinputlisting[language=Matlab, caption={Evaluate Lagrange Interpolation at Point}]{eval_lagrange_poly_point.m}

\lstinputlisting[language=Matlab, caption={Evaluate Lagrange Interpolation at Vector}]{eval_lagrange_poly_vec.m}
An advantage of this basis is that the basis polynomials \(l_i\) are independent of the sampling point values.
As a consequence we can interpolate \(f\) rather quickly in the same sampling points but with different values once the basis polynomials have been calculated.

A drawback however is that all basis vectors have to be recalculated completely if only one new sampling point is added.
The next interpolation method we introduce is based on the aspect of successively adding sampling points.

\subsection{Newton's Divided Differences Polynomial}
% TODO: hermite interpolation
In this section we look at an efficient representation of polynomials, the \emph{newton polynomial}.
Calculating the coefficients of the newton polynomial (the \emph{divided differences}) is a recursive division process.

Imagine that the set of sampling points is built by succesively adding sampling points
\[\Theta_0 := \{x_0\}, \quad \Theta_1 := \Theta_0 \cup \{x_1\}, \quad \Theta_2 := \Theta_1 \cup \{x_2\}, \quad\ldots\]

\begin{theorem}[Newton Polynomial Recursion]\label{thm:div_diff}
   For \(p_n \in \mathbb{P}_n\) holds the recursion
   \[p_0(x) = b_0 \qquad\text{and}\qquad p_n(x) = p_{n-1}(x) + b_n \cdot \omega_{n-1}(x)\]
\end{theorem}
\begin{remark}
   The coefficients \(b_i\) are constructed in the proof of this theorem.
\end{remark}
\begin{proof}
   We prove this by induction over \(n\).

   \emph{IB:} \(n = 0\), we set \(b_0 := f_0\).
   Then is \(p_0 = f_0\) constant and so interpolates \((x_0, f_0)\).

   \emph{IH:} Suppose the statement holds for some \(n\).

   \emph{IS:} Assume IH holds.
   It holds that \(\omega_{n-1}(x_j) = 0\) for all sampling points \(x_j\) where \(j \in [0; n-1]\) since one factor \((x - x_j)\) will be 0 and so
   \[p_n(x_j) \overset{\text{IH}}{=} p_{n-1}(x_j) + b_n \cdot 0 = p_{n-1}(x_j) = f(x_j)\]
   Now we need to check \(x_n\)
   \[p_n(x_n) = p_{n-1}(x_n) + b_n \omega_{n-1}(x_n) \overset{!}{=} f(x_n)\]
   so for
   \[b_n = \frac{f(x_n) - p_{n-1}(x_n)}{\omega_{n-1}(x_n)}\]
   we have shown the claim.
\end{proof}

\begin{definition}[Newton's Divided Difference]\label{def:newt_div_diff}
   We denote the coefficients \(b_n\) in the proof of \cref{thm:div_diff} as the \emph{nth divided difference}
   \[[x_0, \ldots, x_n]f := \frac{f(x_n) - p_{n-1}(x_n)}{\omega_{n-1}(x_n)}\]
\end{definition}
\begin{remark}
   Important for the efficiency of the newton polynomial is the following recursion.
   \[b_0 = [x_0]f := f_0 \qquad b_k = [x_0, \ldots, x_k]f = \frac{[x_1, \ldots, x_k]f - [x_0, \ldots, x_{k-1}]f}{x_k - x_0}\]
\end{remark}

\begin{theorem}[Newton Polynomial]
   The interpolation polynomial of \(f \in C(I)\) in \(\Theta_n\) is given as
   \[p_n(x) = \sum_{i=0}^n b_i \cdot \omega_{i-1}(x)\]
   where \(\omega_{-1} \equiv 1\) and \(b_i\) are given through the recursion above.
\end{theorem}
\begin{remark}
   In some cases we denote the newton polynomial \(p(\Theta_n, f)\), to refer to its dependence on the sampling points and the interpolated function.
\end{remark}
\begin{proof}
   Let \(r\) and \(s\) be as in \cref{lem:poly_r_s}.
   We can represent them as divided differences recursively
   \[r = ([x_1, \ldots, x_k]f) \cdot x^{k-1} + r_{k-1} \qquad s = ([x_0, \ldots, x_{k-1}]f) \cdot x^{k-1} + s_{k-1}\]
   where \(r_{k-1}, s_{k-1} \in \mathbb{P}_{k-2}\).
   This means we can represent them through \ref{lem:poly_r_s}.
   \[p_k(x) = \frac{x}{x_k - x_0}\big(([x_1, \ldots, x_k]f) - ([x_0, \ldots, x_{k-1}]f)\big) x^{k-1} + p_{k-1}\]
   where \(p_{k-1} \in \mathbb{P}_{k-1}\).
   The leading coefficient of \(p_k\) is given through \([x_0, \ldots, x_k]f\) which gives us the recursion of the remark of \cref{def:newt_div_diff}.
\end{proof}

\begin{lemma}\label{lem:poly_r_s}
   For the interpolation polynomials
   \[r(x) := p_{k-1}(\Theta_k \setminus \{x_0\}, f)(x) \qquad\text{and}\qquad s(x) := p_{k-1}(\Theta_{k-1}, f)(x)\]
   holds
   \[p_n(\Theta_n, f)(x) = r(x) + \frac{x - x_n}{x_n - x_0} \cdot (r(x) - s(x))\]
\end{lemma}
\begin{proof}
   We regard the following three cases
   \[p(\Theta_n, f)(x_0) = r(x_0) + \frac{x_0 - x_n}{x_n - x_0} (r(x_0) - s(x_0)) = s(x_0) = f(x_0)\]
   \[p(\Theta_n, f)(x_i) = r(x_i) + \frac{x_i - x_n}{x_n - x_0} (r(x_i) - s(x_i)) = r(x_i) = f(x_i)\]
   \[p(\Theta_n, f)(x_n) = r(x_n) + \frac{x_n - x_n}{x_n - x_0} (r(x_n) - s(x_n)) = r(x_n) = f(x_n)\]
   This means that \(\forall k \in [0; n]: p(\Theta_n, f)(x_k) \in P_k\).
\end{proof}

So we have seen that functions can be approximated with unique polynomials which depend on the function and sampling points.
Now we use the recursive notation above to calculate the coefficients of those polynomials.
Here we denote \(f_i := f(x_i)\).
\[b_0 = f_0\]
\[b_1 = [x_0, x_1]f = \frac{f_1 - f_0}{x_1 - x_0}\]
\[b_2 = [x_0, x_1, x_2]f = \frac{[x_1, x_2]f - [x_0, x_1]f}{x_2 - x_0} = \frac{\frac{f_2 - f_1}{x_2 - x_1} - b_1}{x_2 - x_0}\]
\[b_3 = [x_0, x_1, x_2, x_3]f = \frac{[x_1, x_2, x_3]f - [x_0, x_1, x_2]f}{x_3 - x_0} = \frac{\frac{[x_2, x_3]f - [x_1, x_2]f}{x_3 - x_1} - b_2}{x_3 - x_0} = \frac{\frac{\frac{f_3 - f_2}{x_3 - x_2} - \frac{f_2 - f_1}{x_2 - x_1}}{x_3 - x_1} - b_2}{x_3 - x_0}\]
\[\ldots\]
If we layout the equations in a table the calculation is easier to carry out.
\[
   \begin{array}{|c|c|c|c|c|}
      \hline
      x_i & f_i & & &\\
      \hline
      x_0 & f_0 = b_0 & & &\\
      \hline
      x_1 & f_1 & [x_0, x_1]f = b_1 & &\\
      \hline
      x_2 & f_2 & [x_1, x_2]f & [x_0, x_1, x_2]f = b_2 & \\
      \hline
      x_3 & f_3 & [x_2, x_3]f & [x_1, x_2, x_3]f & [x_0, x_1, x_2, x_3]f = b_3\\
      \hline
   \end{array}
\]
\begin{example}
   Let \(\Theta_3 := \{(0, 3), (1, 4), (2, 7), (4, 19)\}\) be a set of sampling points.
   We want to interpolate this function with a polynomial of degree 3.
   \[
      \begin{array}{|c|c|c|c|c|}
         \hline
         x_i & f_i & & &\\
         \hline
         0 & 3 & & &\\
         \hline
         1 & 4 & (4-3)/(1-0) = 1 & & \\
         \hline
         2 & 7 & (7-4)/(2-1) = 3 & (3-1)/(2-0) = 1 &\\
         \hline
         4 & 19 & (19-7)/(4-2) = 6 & (6-3)/(4-1) = 1 & (1-1)/(4-0) = 0\\
         \hline
      \end{array}
   \]
\end{example}
so we have \(b_0 = 3, b_1 = 1, b_2 = 1, b_3 = 0\) and so
\[p_3 = 3 + 1(x-0) + 1(x-0)(x-1) + 0(x-0)(x-1)(x-2) = x^2 + 3\]

Utilizing the recursive nature of this table we can implement the following algorithm
\lstinputlisting[language=Matlab, caption={Calculate Newton's Divided Differences}]{calc_newton_div_diff.m}

The effort to calculate the coefficients is quite bigger than to evaluate the interpolation polynomial in a single point.
The number of cells in the table depends on the degree \(n\) of the polynomial.
For the 0-th coefficient we need to calculate 0 cells, for the 1st, one cell, the 2nd, 2 and so on.
For each cell we have to carry out 3 arithmetic operations, hence the running time of an algorithm to calculate the coefficients is
\[3 \cdot \sum_{j=1}^n j = 3 \cdot \frac{n(n+1)}{2} = \mathcal{O}(n^2)\]
Are all coefficients calculated and stored on the computer we can use the following algorithm to evaluate the interpolation polynomial in any point

\lstinputlisting[language=Matlab, caption={Evaluate Newton Polynomial in Point}]{eval_newton_poly_point.m}

\lstinputlisting[language=Matlab, caption={Evaluate Newton Polynomial in Vector}]{eval_newton_poly_vec.m}

\subsubsection{Adaptive Sampling Point Refinement}\label{ssec:adapt_refine}
Suppose the measurements of an experiment are expensive, so we need to approximate \(f\) using as little samplings as possible.
This means we need to determine at which points we gravely need additional measurements to improve accuracy.
Since the existing samplings are the only information we have, we try to determine the new sampling points based on the interpolation we already calculated.
This way we will choose a reasonable ``position'' of new sampling points based on the problem we're facing.

So first we structure the set of sampling points recursively.
\[\Theta_{k+1} := \Theta_k \cup \Theta_k^+ \implies \Theta_k \subset \Theta_{k+1}\]
where \(\Theta_k := \{x_i \mid 0 \leq i \leq n_k\}\) and \(\Theta_k^+\) contains \((n_k-1)\) newly generated sampling points.
For the sake of convenience we set \(x_0 = a\) and \(x_1 = b\).

Now we regard the lattice for each set of sampling points
\[\mathcal{G}_k := \{\tau_i \subset I \mid 1 \leq i \leq n_k\}\]
For subintervalls \(\tau_i, \tau_j \in \mathcal{G}_k\) we regard \(\tau_i \cap \tau_j\).
If they are adjacent it contains their shared sampling point (suppose \(j = i + 1\) then they share \(x_i\)).
However if they are not neighbours the intersection is empty.

Now suppose we have given \(\Theta_k\) with its lattice and the corresponding interpolation polynomial \(p_k\).
In order to determine where to place new sampling points, we compare the values of \(p_k\) with the values of \(p_{k-1}\).
At the points where the they differ the most \(p_k\) approximates \(f\) a lot better than \(p_{k-1}\).
Naturally we conclude that in this ``region'' it is most needed to gather more samplings.

\begin{center}
   \input{drawings/adaptive_refinement.tex}
\end{center}

Concretely we regard for every \(\tau \in \mathcal{G}_k\) the values of \(p_k, p_{k-1}\) in its center point \(M_{\tau}\).
\[d_{\tau} := |p_k(M_{\tau}) - p_{k-1}(M_{\tau})| \qquad\text{and determine}~d_{} := \max_{\tau_i \in \mathcal{G}_k}(d_{\tau_i})\]
With this in place and a control parameter \(\alpha \in [0; 1]\) we can set
\[\Theta_k^+ = \{M_{\tau} \mid \tau \in \mathcal{G}_k \land d_{\tau} \geq \alpha \cdot d_{}\}\]
where we only need to calculate the newtown divided differences \(b_j\) for \(j = n_k + 1, \ldots, n_{k+1}\).

% TODO: reverse engineer theory from code
\lstinputlisting[language=Matlab, caption={Sampling Point Refinement}]{sampling_point_refinement.m}

\subsection{Runge's Phenomenon}
Runge's phenomenon is a problem of oscillation at the edges of an interval that occurs when using polynomial interpolation with polynomials of high degree over a set of equispaced interpolation points.
The discovery was important because it shows that going to higher degrees does not always improve accuracy.

The following graphic illustrates Runge's phenomenon for the function in grey over the intervall \([-1; 1]\).
The green graph is the quadratic interpolation polynomial (going through three sampling points).
All other graphs are interpolation polynomials of increasing degrees, which also use the initial green sampling points.
As described above we see more intense oscillation at the edges of the intervall with increasing degrees.

\begin{center}
   \input{drawings/runge.tex}
\end{center}

We could mitigate the oscillation at the edges of the intervall, if we instead interpolate the function over a smaller intervall, e.g. \([-0.1; 0.1]\).
This works because Weierstrass proved that we can approximate a function arbitrarily well for a \emph{sufficiently small} intervall.
However in many cases this is inacceptable, so as in \cref{ssec:adapt_refine} the question arises if there is an optimal distribution of sampling points such that this phenomenon is reduced as much as possible.

\begin{definition}[Chebyshev Sampling Points]
   The optimal sampling nodes \(\Theta_n\) on \(I := [a; b]\) are given through
   \[x_i := \frac{1 + \cos\left(\frac{2i + 1}{2(n+1)} \pi\right)}{2}a + \frac{1 - \cos\left(\frac{2i + 1}{2(n+1)} \pi\right)}{2}b \quad \forall i \in [0; n]\]
\end{definition}
\begin{remark}
   For the case \(b = -a\) we get
   \[x_i = a \cos\left(\frac{2i + 1}{2(n+1)} \pi\right)\]
   Defining the sampling points in this manner minimizes \(M(\Theta_n)\) of \cref{cor:error_est}.
\end{remark}
\lstinputlisting[language=Matlab, caption={Generate Chebyshev Sampling Nodes}]{gen_chebyshev.m}

Interpolating \(f\) with those sampling points we see for the polynomial of highest degree that the oscillation has decreased.
\begin{center}
   \input{drawings/chebyshev_nodes.tex}
\end{center}

\subsection{Spline Interpolation}\label{sec:piece_inter}
A whole lot more flexible than global polynomial interpolation is piecewise interpolation where we approximate \(f\) with a spline function of \(\mathbb{S}_m^k(\mathcal{G})\).
This means instead of finding a single polynomial of higher degree going through all sampling points, we interpolate \(f\) on every subintervall \(\tau \in \mathcal{G}\) with polynomials of lesser degree.
This is the reason that in interpolating problems, spline interpolation is often preferred to polynomial interpolation because it yields similar results, even when using low degree polynomials, while avoiding \emph{Runge's phenomenon} for higher degrees.
The rationale behind this is the recognition that on a sufficiently small interval, functions can be approximated arbitrarily well by polynomials of low degree, even degree 1, or zero, for that matter.

Also, as we pointed out before, we require \(f \in C^\infty(I)\) for our error estimation.
If \(f \in C^k(I) \land f \notin C^{k+1}(I)\) we can't use the previous error representation which is based on derivation.
But with spline functions we can show the convergence of the approximation error based on the the length of \(I\).
The convergence of the error can be achieved by introducing more lattice points, that is, reducing the length of the subintervalls.
\[h := \max_{i \in [1;n]} h_i \quad\text{where}\quad h_i := x_i - x_{i-1}\]

\subsubsection{Piecewise Linear -- \(\mathcal{S}_1^0(\mathcal{G})\)}
We regard the set of continuous functions, composed of linear polynomials.
First we look at the linear interpolation of \(f\) on a single subintervall using newton divided differences
\[\forall x \in \tau_i: \varphi(x) = f(x_{i-1}) + (x - x_{i-1}) [x_{i-1}; x_i]f\]

Now we need a way of applying this linear interpolation over all subintervalls, this is possible by choosing a sensible basis \((\pi_i)^N_{i = 1}\) and expressing \(\varphi_{\opt}(x)\) as a linear combination.
From the construction above we see
\begin{equation*}
   \begin{split}
      f(x_{i-1}) + (x - x_{i-1}) [x_{i-1}; x_i]f & = f(x_{i-1}) + (x - x_{i-1}) \frac{f(x_i) - f(x_{i-1})}{x_i - x_{i-1}} \\
                                                 & = f(x_{i-1}) + \big(f(x_i) - f(x_{i-1})\big)\frac{x - x_{i-1}}{x_i - x_{i-1}} \\
                                                 & = f(x_{i-1}) \left(1 - \frac{x - x_{i-1}}{x_i - x_{i-1}}\right) + f(x_i) \frac{x - x_{i-1}}{x_i - x_{i-1}} \\
                                                 & = f(x_{i-1}) \frac{x_i - x}{x_i - x_{i-1}} + f(x_i) \frac{x - x_{i-1}}{x_i - x_{i-1}}
   \end{split}
\end{equation*}
which holds for all \(x \in \tau_i\).

Now we can define the basis for \(\mathbb{S}_1^0(\mathcal{G})\) through
\[\pi_i(x) := \begin{cases}\frac{x - x_{i-1}}{x_i - x_{i-1}} & \text{if}~x \in \tau_i\\ \frac{x_{i+1} - x}{x_{i+1} - x_i} & \text{if}~x \in \tau_{i+1}\\ 0 & \text{else}\end{cases}\]
with this in place we can write the interpolation spline as a linear combination of this basis.
\[\varphi_{\opt}(x) = \sum_{i = 0}^n f(x_i) \cdot \pi_i(x)\]

\paragraph{Interpolation Error}
For the error in \(\tau_i\) we use the representation
\[f(x) - \varphi(x) = \frac{f''(\xi_x)}{2} (x - x_{i-1})(x - x_i) \quad\text{with}\quad x \in \tau_i, \xi_x \in \tau_i\]
so for \(f \in C^2(I)\) we get
\begin{equation*}\label{eq:lin_spline_err}
   |f - \varphi| \leq \frac{\|f''\|_{\infty, \tau_i}}{8} \cdot h_i^2
\end{equation*}
This estimation shows that the error get arbitrarily small for \(h \to 0\).

\subsubsection{Piecewise Quadratic -- \(\mathcal{S}_2^0(\mathcal{G})\)}
Exactly as before we use Newton polynomials to interpolate \(f\) function piecewise, however the formal construction is to complicated to be useful.
So only some quick notes on implementing piecewise quadratic interpolation.

We have to keep in mind that we need one additional sampling point on every subintervall \(\tau_i\) compared to linear interpolation from before.
Hence if we want to interpolate \(f\) quadratic in \(N\) subintervalls we actually need \(n = 2 \cdot N + 1\) sampling points but keep our \(\tau_i\)'s defined the same.
This way every \(\tau_i\) receives a center point used for the quadratic interpolation.

\lstinputlisting[language=Matlab, caption={Piecewise Quadratic Newton Polynomials}]{eval_piecew_quadr_interp.m}

\section{Least Squares Approximation}
Interpolation is usefull to approximate continuous functions, but if \(f\) is discontinuous or even unbounded, interpolation might not even be defined.

Furthermore if we suspect our data to contain noise, such as measuring errors, interpolation is not suitable since it replicates the values of the sampling points exactly.
Hence if we suspect noisy data it is better to construct an approximation, which also averages the sampling points, in this case we talk off line fitting.

\subsection{Construction}
\emph{Least squares} is applicable to a larger function space, namely the \(L^2\) space.
Following we construct this method a bit more abstractly.
We regard \(\mathbb{R}\)-vector space of functions \(V\) with a scalar product and it's associated norm and the subspace \(S \subset V\) with a basis \((\pi_i)_{i=0}^n\).
Note that throughout this section we will be indexing basis elements and vectors starting by 0.

Th goal of least squares is to find \(\varphi_{\opt} \in S\) such that
\[\|f - \varphi_{\opt}\| = \min_{\varphi \in S} \|f - \varphi\|\]

\subsubsection{Algebraization}
For some \(\varphi \in S\), we define
\[F^2(\varphi) := \|f - \varphi\|^2\]
and our goal now is to minimize this functional.
\begin{remark}
   The term \emph{functional} refers to a linear map from a vector space into its field of scalars.
\end{remark}
To be able to process this approximation on a computer we need to translate this minimization problem into a problem over \(\mathbb{R}^{n+1}\).
We do this by utilizing the fact, that every \(\varphi \in S\) can be written as a linear combination
\[\varphi = \sum_{i=1}^n \varphi_i \pi_i\]

Note that from this follows that for \(\varphi, \varphi' \in S\) holds
\[\langle \varphi, \varphi' \rangle = \left\langle \sum_{i=0}^n \varphi_i \pi_i, \sum_{j=0}^n \varphi_j' \pi_j \right\rangle = \sum_{i=1}^n\sum_{j=1}^n \varphi_i \varphi_j' \langle \pi_i, \pi_j\rangle\]
Hence we can rewrite
\begin{equation*}
   \begin{split}
      F^2(\varphi) = \|f - \varphi\|^2 = \langle f - \varphi, f - \varphi\rangle & = \langle f, f\rangle - 2 \langle f, \varphi\rangle + \langle \varphi, \varphi\rangle = \\
                                                                                 & = \|f\|^2 - 2 \sum_{i=0}^n \varphi_i \langle f, \pi_i \rangle + \sum_{i=1}^n \sum_{j=1}^n \varphi_i \varphi_j \langle \pi_i, \pi_j \rangle
   \end{split}
\end{equation*}

Now we use the gramian matrix
\[G := \big(\langle \pi_i, \pi_j\rangle\big)_{i,j=1}^n \qquad\text{and define}\qquad r := \big(\langle f, \pi_i\rangle\big)_{i=0}^n\]
in order to rewrite the equation above as
\[F^2(\varphi) = \|f\|^2 - 2 \sum_{i=0}^n \varphi_ir_i + \sum_{i=0}^n\sum_{j=0}^n \varphi_i \varphi_j G_{ij} =: \widetilde{F}^2(\varphi)\]
hence we turned the minimization problem over \(S\) into one over \(\mathbb{R}^{n+1}\).

\subsubsection{Minimization}
The next step is now to determine \(\varphi_{\opt}\) such that \(\widetilde{F}^2(\varphi_{\opt})\) is minimal.
\(\widetilde{F}\) depends on \(n+1\) variables and takes the minimum if
\[\forall i \in [0;n]: \frac{\partial\widetilde{F}^2}{\partial \varphi_i} = 0\]
where we fix all variables and derive over \(\varphi_i\).
\begin{equation*}
   \begin{split}
      \frac{\partial\widetilde{F}^2}{\partial \varphi_i} & = \frac{\partial}{\partial \varphi_i} \left(\|f\|^2 - 2 \sum_{j=0}^n \varphi_jr_j + \sum_{j,k = 0}^n \varphi_j\varphi_k G_{jk}\right) = \frac{\partial}{\partial \varphi_i} \left(-2 \sum_{j=0}^n \varphi_jr_j\right) + \frac{\partial}{\partial \varphi_i} \left(\sum_{j,k = 0}^n \varphi_j\varphi_k G_{jk}\right) =\\
                                                   & = -2 \sum_{j=0}^n \frac{\partial \varphi_j}{\partial \varphi_i} r_j + \sum_{j,k=0}^n \frac{\partial (\varphi_j \varphi_k)}{\partial \varphi_i} G_{jk} \overset{\cref{eq:kroneck_rule}}{=} -2 \sum_{j=0}^n \delta_{ij} r_j + \sum_{j,k=0}^n \frac{\partial (\varphi_j \varphi_k)}{\partial \varphi_i} G_{jk} = \\
                                                   & \overset{\cref{eq:prod_rule}}{=} -2 \sum_{j=0}^n \delta_{ij} r_j + \sum_{j,k=0}^n \delta_{ij}\varphi_k G_{jk} + \sum_{j,k=0}^n \delta_{ik}\varphi_j G_{jk} = -2 r_i + \sum_{k=0} G_{ik} \varphi_k + \sum_{j=0}^n \varphi_j G_{ji} = \\
                                                   & = (-2r + G\varphi + G^T\varphi)_i \overset{!}{=} 0
   \end{split}
\end{equation*}

Where we used that
\begin{equation}\label{eq:kroneck_rule}
   \frac{\partial \varphi_j}{\partial \varphi_i} = \delta_{ij}
\end{equation}
and
\begin{equation}\label{eq:prod_rule}
   \frac{\partial (\varphi_j \cdot \varphi_k)}{\partial \varphi_i} = \delta_{ij}\varphi_k + \delta_{ik}\varphi_j
\end{equation}

From the definition of \(G\) and the symmetry of the scalar product holds \(G^T = G\) so we have
\[-2r + G \cdot \varphi + G^T \cdot \varphi = 0 \iff -2r + 2G\cdot\varphi = 0 \iff G \cdot \varphi = r\]
This means we have shown that \(\frac{\partial \widetilde{F}^2}{\partial \varphi_i} = 0 \iff G \cdot \varphi = r\), so we can compute \(\varphi = G^{-1}r\).
Then the solution of the original problem is given through
\[\varphi_{\opt} = \sum_{i=0}^n \varphi_i \pi_i\]

\subsection{Application}
For all following examples we set \(V = C^0(I)\) and use the \(L^2\)-norm (\ref{def:l2_norm}) and scalar product (\ref{def:l2_scal_prod}).
The process for calculating the least squares approximation is as follows
\begin{enumerate}
   \item Choose an appropriate basis \((\pi_i)_{0 \leq i \leq n}\) for \(S\).
   \item Compute the gramian matrix \(G = \big(\langle \pi_i, \pi_j\rangle_2\big)_{0 \leq i,j \leq n}\)
   \item Compute the vector \(r = \big(\langle \pi_i, f\rangle_2\big)_{0 \leq i \leq n}\)
   \item Compute \(\varphi = G^{-1} \cdot r\)
   \item The solution is given through
      \[\varphi_{\opt}(x) = \sum_{i=0}^n \varphi_i \cdot \pi_i(x)\]
\end{enumerate}

In the case where we use a spline function as approximation, i.e. \(S = \mathcal{S}_m^k(\mathcal{G})\) we need some preliminary considerations in order to compute \(G\)
\begin{definition}[Support of a Map]
   Given a real function \(f: X \to \mathbb{R}\)
   \[\supp(f) := \{x \in X \mid f(x) \neq 0\}\]
\end{definition}
\begin{remark}
   We see that
   \[\int_I f(x) dx = \int_{\supp(f)} f(x) dx\]
\end{remark}
Now since for the basis functions holds \(\supp(\pi_i) = \tau_i \cup \tau_{i+1}\) we get
\[\langle \pi_i, \pi_j\rangle_2 = \int_I \pi_i(x)\pi_j(x) dx = \int_{\supp(\pi_i) \cap \supp(\pi_j)} \pi_i(x)\pi_j(x)dx = \int_{(\tau_i \cup \tau_{i+1})\cap(\tau_j \cup \tau_{j+1})} \pi_i(x)\pi_j(x)dx\]

\subsubsection{Example \(S = \mathcal{S}_0^{-1}(\mathcal{G})\)}
In this example we approximate \(f(x) := x\) through constants on every \(\tau_i \in \mathcal{G}\).
\[\forall x \in \tau_i: \varphi(x) = c_i\]
\begin{enumerate}
   \item We use the basis \(\pi_i(x) := \begin{cases}1 & x \in \tau_i\\ 0 & \text{else}\end{cases}\)
   \item We compute \(G\).

      For the diagonal, i.e. \(i = j\), we have
      \begin{equation*}
         \begin{split}
            \langle \pi_i, \pi_j \rangle_2 & = \int_I \pi_i(x)^2 dx = \int_{\supp(\pi_i)}\pi_i(x)^2 dx = \int_{\tau_i \cup \tau_{i+1}} \pi_i(x)^2 dx = \int_{\tau_i} \pi_i(x)^2 dx + \int_{\tau_{i+1}} \pi_i(x)^2 dx = \\
                                           & = \int_{\tau_i} 1^2 + \int_{\tau_{i+1}} 0^2 = x \rvert_{x_{i-1}}^{x_i} = x_i - x_{i-1}
         \end{split}
      \end{equation*}
      since by definition holds \(|i - j| \geq 2 \implies \supp(\pi_i) = \emptyset \implies \langle\pi_i, \pi_j\rangle_2 = 0\) we only need to check the minor diagonals i.e. \(j = i \pm 1\), so we regard
      \begin{equation*}
         \begin{split}
         \langle \pi_i, \pi_{i+1}\rangle_2 & = \int_I \pi_i(x)\pi_{i+1}(x)dx = \int_{\supp(\pi_i) \cap \supp(\pi_{i+1})} \pi_i(x)\pi_{i+1}(x)dx = \\
                                           & = \int_{(\tau_i \cup \tau_{i+1}) \cap (\tau_{i+1} \cup \tau_{i+2})} \pi_i(x) \pi_{i+1}(x)dx = \int_{\tau_{i+1}} \pi_i(x) \pi_{i+1}(x) dx = \int_{\tau_{i+1}} 0 \cdot 1 dx = 0
         \end{split}
      \end{equation*}
      \begin{equation*}
         \begin{split}
         \langle \pi_i, \pi_{i-1}\rangle_2 & = \int_I \pi_i(x)\pi_{i-1}(x)dx = \int_{\supp(\pi_i) \cap \supp(\pi_{i-1})} \pi_i(x)\pi_{i-1}(x)dx = \\
                                           & = \int_{(\tau_i \cup \tau_{i+1}) \cap (\tau_{i-1} \cup \tau_{i})} \pi_i(x) \pi_{i+1}(x)dx = \int_{\tau_i} \pi_i(x) \pi_{i-1}(x) dx = \int_{\tau_i} 1 \cdot 0 dx = 0
         \end{split}
      \end{equation*}
      so we have the gramian matrix
      \[G = \begin{pmatrix} x_1 - x_0 & & & 0\\ & x_2 - x_1 & & \\ & & \ddots  & \\ 0 & & & x_n - x_{n-1}\end{pmatrix} \qquad\rightsquigarrow\qquad G^{-1} = \begin{pmatrix}\frac{1}{x_1 - x_0} & & & 0\\ & \frac{1}{x_2 - x_1} & & \\ & & \ddots & \\0 & & & \frac{1}{x_n - x_{n-1}} \end{pmatrix}\]
   \item We compute a single component of \(r\)
      \[r_i = \int_{\tau_i} \pi_i(x)f(x)dx = \int_{x_{i-1}}^{x_i} x = \frac{1}{2}x^2\rvert_{x_{i-1}}^{x_i} = \frac{1}{2}(x_i^2 - x_{i-1}^2)\]
      which gives us
      \[r = \frac{1}{2}\begin{pmatrix}x_1^2 - x_0^2\\x_2^2-x_1^2\\\vdots\\x_n^2 - x_{n-1}^2\end{pmatrix}\]
   \item We compute \(\varphi = G^{-1} \cdot r\)
      \[\varphi = \begin{pmatrix}\frac{1}{x_1 - x_0} & & & 0\\ & \frac{1}{x_2 - x_1} & & \\ & & \ddots & \\0 & & & \frac{1}{x_n - x_{n-1}} \end{pmatrix}
    \cdot \frac{1}{2} \begin{pmatrix}x_1^2 - x_0^2\\x_2^2 - x_1^2\\\vdots \\x_n^2 - x_{n-1}^2\end{pmatrix}
    = \frac{1}{2} \begin{pmatrix} x_1 + x_0\\x_2 + x_1\\\vdots\\x_n + x_{n-1}\end{pmatrix}\]
\end{enumerate}

This way we can write our approximation as a linear combination
\[\varphi_{\opt}(x) = \frac{1}{2}\sum_{i=0}^n \varphi_i \cdot \pi_i(x) = \frac{1}{2} \sum_{i=0}^n (x_i + x_{i-1}) \cdot \pi_i(x)\]
which essentially works because we defined our basis in such a way that all irrelevant terms are annihilitated in the sum.

\begin{center}
   \input{drawings/least_squares1.tex}
\end{center}

\lstinputlisting[language=Matlab, caption={Least Squares with Constant Splines}]{least_squares_const_splines.m}

\subsubsection{Example \(S = \mathcal{S}_1^{0}(\mathcal{G})\)}
First we compute \(G\) abstractly to reveale a general form and then illustrate the rest of the computations with an example.

We choose the basis for the linear spline space
\[\pi_i(x) := \begin{cases}\frac{x - x_{i-1}}{x_i - x_{i-1}} & \text{if}~x \in \tau_i\\ \frac{x_{i+1} - x}{x_{i+1} - x_i} & \text{if}~x \in \tau_{i+1}\\ 0 & \text{else}\end{cases}\]
The diagonal entries of \(G\), i.e. \(i = j\), are given as
\[G_{ii} = \langle \pi_i, \pi_i\rangle_2 = \int_I \pi_i(x)^2 dx = \int_{\tau_i} \left(\frac{x - x_{i-1}}{h_i}\right)^2 dx + \int_{\tau_{i+1}} \left(\frac{x - x_{i+1}}{h_{i+1}}\right)^2dx = \frac{h_i}{3} + \frac{h_{i+1}}{3}\]
whereas the the minor diagonals, are given as
\begin{equation*}
   \begin{split}
      G_{ij} & = \langle \pi_i, \pi_j\rangle_2 = \int_I \pi_i(x) \pi_j(x) dx = \int_{\supp(\pi_i) \cap \supp(\pi_j)} \pi_i(x) \pi_j(x) dx = \int_{\tau_{i+1}} \frac{x_{i+1} - x}{h_{i+1}} \cdot \frac{x - x_i}{h_i}dx =\\
             & = \frac{1}{h_{i+1}^2} \int_{x_i}^{x_{i+1}} (-x_i \cdot x_{i+1} + (x_i + x_{i+1})x - x^2) dx = \frac{1}{6} h_{i+1}
   \end{split}
\end{equation*}
% TODO: write code to calculate this matrix
So the resulting matrix is
\[G = \big(\langle \pi_i, \pi_j \rangle_2\big)_{i,j=0}^n =
   \begin{pmatrix}
      \frac{h_1}{3} & \frac{h_1}{6} & & & & 0 \\
      \frac{h_1}{6} & \frac{h_1 + h_2}{3} & \frac{h_2}{6} & & & \\
                    & \frac{h_2}{6} & \ddots & \ddots & & \\
                    &               & \ddots & \ddots  & \ddots                  &     \\
                    &               &        & \frac{h_{n-1}}{6} & \frac{h_{n-1} + h_n}{3} & \frac{h_n}{6} \\
      0             &               &        &         & \frac{h_n}{6}           & \frac{h_n}{3}\\
   \end{pmatrix}
\]

\begin{example}
   Now we approximate \(f(x) := x\) for which the sampling points \(\{(0, 0), (1, 1), (2, 2)\}\) are given.
   Since they are equidistant we have \(\forall i \in [0; n]: h_i = 1\) so we have
   \[G = \begin{pmatrix}
         \frac{1}{3} & \frac{1}{6} & 0\\
         \frac{1}{6} & \frac{2}{3} & \frac{1}{6}\\
         0 & \frac{1}{6} & \frac{1}{3}
      \end{pmatrix} \qquad\rightsquigarrow\qquad G^{-1} = \frac{1}{2}\begin{pmatrix}
         7 & -2 & 1\\
         -2 & 4 & -2\\
         1 & -2 & 7
   \end{pmatrix}\]
   Now we calculate \(r\) component-wise
   \begin{equation*}
      \begin{split}
         r_0 & = \langle \pi_0, f\rangle_2 = \int_I \pi_0(x)f(x)dx = \int_{\tau_1} \pi_0(x)f(x)dx + \int_{\tau_2} \pi_0(x)f(x)dx = \int_{\tau_1} \frac{1 - x}{1 - 0} \cdot x dx = \\
             & = \int_0^1 x - x^2dx = \left.\frac{x^2}{2}\right\rvert_0^1 - \left.\frac{x^3}{3}\right\rvert_0^1 = \frac{1}{6}
      \end{split}
   \end{equation*}
   \[r_1 = \langle \pi_1, f\rangle_2 = \ldots = 1\]
   \[r_2 = \langle \pi_2, f\rangle_2 = \ldots = \frac{5}{6}\]
   and so can calculate
   \[\varphi = G^{-1} \cdot r = \frac{1}{2}\begin{pmatrix}7&-2&1\\-2&4&-2\\1&-2&7\end{pmatrix} \cdot \begin{pmatrix}\frac{1}{6}\\1\\\frac{5}{6}\end{pmatrix} = \begin{pmatrix}0\\1\\2\end{pmatrix}\]
   hence we have
   \[\varphi_{\opt}(x) = \sum_{i=0}^n \varphi_i \cdot \pi_i(x) = \pi_1(x) + 2\pi_2(x)\]
   where we see
   \[\forall x \in \tau_1: \varphi_{\opt}(x) = \frac{x - 0}{1 - 0} = x \qquad\text{and}\qquad \forall x \in \tau_2: \varphi_{\opt}(x) = \frac{2 - x}{2 - 1} + 2\left(\frac{x - 1}{2 - 1}\right) = x\]
\end{example}

\subsubsection{Example \(S = \mathbb{P}_2\)}
We regard the discontinuous function \(f(x) := \begin{cases} 0 & x \in [0; \frac{1}{2})\\ 1 & x \in [\frac{1}{2}; 1]\end{cases}\)
and want to find the least squares approximation \(\varphi_{\opt} \in \mathbb{P}_2\) for \(f\) in \(I = [0; 1]\).

\begin{enumerate}
   \item We use the base of monomials \(\pi_0(x) = 1\), \(\pi_1(x) = x\) and \(\pi_2(x) = x^2\).
   \item We determine \(\langle \pi_i, \pi_j \rangle_2\) for \(0 \leq i, j \leq n\)
      \[\int_I \pi_0^2(x) dx = \int_I 1 dx = 1 \qquad \int_I \pi_0(x) \pi_1(x) dx = \int_I x = \frac{1}{2} \qquad \int_I \pi_0(x) \pi_2(x) dx = \int_I x^2 = \frac{1}{3}\]
      \[\int_I \pi_1(x)^2dx = \int_i x^2dx = \frac{1}{3} \qquad \int_I \pi_2^2(x) dx = \int_I x^4 = \frac{1}{5} \qquad \int_I \pi_1(x) \pi_2(x) dx = \int_I x^3 = \frac{1}{4}\]
      Which gives us the matrix
      \[G = \begin{pmatrix}1 & \frac{1}{2} & \frac{1}{3}\\ \frac{1}{2} & \frac{1}{3} & \frac{1}{4}\\ \frac{1}{3} & \frac{1}{4} & \frac{1}{5}\end{pmatrix} \qquad\rightsquigarrow\qquad G^{-1} = \begin{pmatrix}9&-36&30\\-36&192&-180\\30&-180&180\end{pmatrix}\]
   \item We determine \(\langle f, \pi_i \rangle_2\) for \(0 \leq i \leq n\)
   \[\int_I f(x) \pi_0(x) dx = \int_I 1 dx = \frac{1}{2}\]
   \[\int_I f(x) \pi_1(x) = \int_\frac{1}{2}^1 x dx = \frac{x^2}{2}\rvert_\frac{1}{2}^1 = \frac{1}{2} - \frac{1}{8} = \frac{3}{8}\]
   \[\int_I f(x) \pi_2(x) = \int_\frac{1}{2}^1 x^2 = \frac{x^3}{3}\rvert_\frac{1}{2}^1 = \frac{1}{3} - \frac{1}{24} = \frac{7}{24}\]
   which gives us 
   \[r := \begin{pmatrix}\frac{1}{2}\\\frac{3}{8}\\\frac{7}{24}\end{pmatrix}\]
   \item We compute \(\varphi = G^{-1} \cdot r\)
      \[\varphi = \begin{pmatrix}9&-36&30\\-36&192&-180\\30&-180&180\end{pmatrix} \cdot \begin{pmatrix}\frac{1}{2}\\\frac{3}{8}\\\frac{7}{24}\end{pmatrix} = \begin{pmatrix}-\frac{1}{4}\\ \frac{3}{2}\\ 0\end{pmatrix}\]
\end{enumerate}
So we found \(\varphi_{\opt}(x) = \frac{3}{2}x - \frac{1}{4}\).
\begin{center}
   \input{drawings/least_squares2.tex}
\end{center}

\subsection{Error Estimations}
For the error representation we use the norm of \cref{def:l2_norm}:
\[\|f - \varphi_{\opt}\|_2 = \sqrt{\int_a^b |(f - \varphi_{\opt})(x)|^2 dx}\]
However, in order to arrive at a sensible error estimate, we wish to use maximums norm.
So we use the monotony of the integral and rewrite
\begin{equation*}
   \begin{split}
      \left(\|f - \varphi_{\opt}\|_2\right)^2 = \min_{\varphi \in S}\left(\int_a^b (f - \varphi)(x)^2\right) & \leq \min_{\varphi \in S}\left(\int_a^b 1 dx \cdot \|f - \varphi\|_{\infty}^2\right) = \\
                                                                                                             & = x\rvert_a^b \cdot \min_{\varphi \in S}\left(\|f - \varphi\|_{\infty}^2\right) = (b-a) \cdot \min_{\varphi \in S}\left(\|f - \varphi\|_{\infty}^2\right)
   \end{split}
\end{equation*}
This way we see the error estimation
\[\|f - \varphi_{\opt}\|_2 \leq \sqrt{b - a} \cdot \min_{\varphi \in S} \|f - \varphi\|_\infty\]

Using this estimate for \(S = \mathbb{P}_n\), it tells us that we can construct polynomials with least squares whose errors converge, since we can approximate continuous functions arbitrarily well.
\[\min_{\varphi \in \mathbb{P}_n} \|f - \varphi\|_\infty \xrightarrow{n \to \infty} 0\]

The same estimate can be used for \(S = \mathbb{S}_1^0(\mathcal{G})\).
If we now choose \(\varphi \in \mathbb{S}_1^0\) on the right side and assume \(f \in C^2(I)\) we get from \cref{eq:lin_spline_err}
\[\|f - \varphi_{\opt}\|_2 \leq \sqrt{b - a} \cdot \frac{\|f''\|_\infty}{8} \cdot h^2\]

\section{Numerical Integration \& Differentiation}
Historically also known as \emph{numerical quadrature}, it is the process of approximating the integral of \(f \in C^0(I)\) over \(I = [a; b]\).
For simplicity's sake we denote
\[\mathcal{I}(f) := \int_I f(x) dx\]
The goal of numerical integration is to find a \emph{quadrature} \(Q(f)\) which approximates \(\mathcal{I}(f)\)
\[\mathcal{I}(f) = Q(f) + E(f)\]
where \(E(f) := |\mathcal{I}(f) - Q(f)|\) is the \emph{quadrature error}.

First we show a few simple quadratures to understand the construction intuitively.
Then we then generalize them into \emph{Newton-Cotes formulas}, the group of quadratures based on equidistant sampling points \((x_i)_{i=0}^n\).

We construct \(Q(f)\) by replacing the integrand in \(\mathcal{I}(f)\) with an interpolation polynomial.
In order to derive an error estimate we use the fact that we can split an integral into the sum of integrals over the subintervalls \((\tau_i)_{i=1}^n\).
Hence we interpolate \(f\) piecewise over all \(\tau_i\) and calculate the integrals of the pieces.
Note that since our sampling points are equidistant we have
\[\forall i \in [1;n]: |\tau_i| = x_i - x_{i-1} = \frac{b-a}{n} =: h\]

So for \(p \in \mathbb{P}_n\)
\[\mathcal{I}(f) = \int_I f(x)dx = \sum_{i=1}^n \left(\int_{\tau_i} f(x) dx\right) \approx \sum_{i=1}^n \left(\int_{\tau_i} p_i(x) dx\right) = \int_I p(x)dx =: Q(f)\]

\subsection{Midpoint Rule}
The midpoint rule is based on constant interpolation e.g. \(p \in \mathbb{P}_0\) in green below
\begin{center}
   \input{drawings/midpoint_rule.tex}
\end{center}
In this case we can compute geometrically
\[Q(f) = (b-a) \cdot f\left(\frac{a + b}{2}\right) = \sum_{i=1}^n (x_i - x_{i-1}) \cdot f\left(\frac{x_i + x_{i-1}}{2}\right) = h \cdot \sum_{i=1}^n f\left(\frac{x_i + x_{i-1}}{2}\right)\]

\newpage

\begin{definition}[Midpoint Quadrature]
   Given \(f \in C^0(I)\)
   \[Q_M(f) := h \sum_{i=1}^n f\left(\frac{x_i + x_{i-1}}{2}\right)\]
\end{definition}
\lstinputlisting[language=Matlab, caption={Midpoint Rule}]{midpoint_quadr.m}

% TODO: error approx
\begin{definition}[Summed Midpoint Error]
   \[E(f) \leq C \cdot h^2\]
\end{definition}

\begin{definition}[Global Midpoint Error]
   \[E(f) \leq \frac{(b-a)^3}{24} \cdot |f''(\xi)|\]
\end{definition}

\subsection{Trapezoidal Rule}
The trapezoidal rules is based on piecewise linear interpolation e.g. \(p \in \mathbb{P}_1\) in green below
from which we can derive the integral also geometrically:
\begin{center}
   \input{drawings/trapezoidal_rule.tex}
\end{center}
\[Q(f) = (b-a) \frac{f(a) + f(b)}{2} = \sum_{i=1}^n (x_i - x_{i-1}) \frac{f(x_{i-1}) + f(x_i)}{2} = \sum_{i=1}^n \frac{h}{2} \big(f(x_{i-1}) + f(x_i)\big)\]
with which we have
\begin{definition}[Trapezoidal Quadrature]
   Given \(f \in C^0(I)\)
   \[Q_T(f) := h \sum_{i=0}^n w_i f(x_i) \qquad\text{where}\qquad w_i := \begin{cases} 1 & i \in (0; n)\\ \frac{1}{2} & i \in \{0, n\}\end{cases}\]
\end{definition}

From this we see an elegant way of computing the integral
\lstinputlisting[language=Matlab, caption={Trapezoidal Rule}]{trapezoidal_quadr.m}

\paragraph{Quadrature Error}
To derive the approximation error we use the error representation for polynomial interpolation (\ref{thm:error_repr}) over a single \(\tau_i\)
\[\exists \xi_x \in (x_{i-1}; x_i): f(x) - p(x) = \frac{f''(\xi_x)}{2} (x - x_i)(x - x_{i-1})\]
We can use this representation to state the local error of the trapezoidal rule.
\begin{equation*}
   \begin{split}
      E_i(f) & = \mathcal{I}_i(f) - Q_i(f) = \int_{\tau_i}f(x)dx - \int_{\tau_i}p(x)dx = \int_{\tau_i}f(x) - p(x)dx = \int_{\tau_i} \frac{f''(\xi_x)}{2} (x - x_i)(x - x_{i-1}) \leq \\
             & \leq \frac{\|f''\|_{\infty, \tau_i}}{2} \int_{\tau_i} (x_i - x)(x - x_{i-1}) = \frac{h^3}{12} \|f''\|_{\infty}
   \end{split}
\end{equation*}
and summing this we receive the global error
\[\sum_{i=1}^n E_i(f) = \sum_{i=1}^n \frac{h^3}{12} \|f''\|_{\infty} = \frac{h^3 n}{12} \|f''\|_{\infty} = \frac{b-a}{12} h^2 \|f''\|_{\infty}\]
\begin{definition}[Summed Trapezoidal Error]
   For \(f \in C^2(I)\)
   \[E(f) = \frac{b-a}{12} h^2 \|f''\|_{\infty, I}\]
\end{definition}

\begin{definition}[Global Trapezoidal Error]
   For \(f \in C^2(I)\)
   \[E(f) = \frac{(b-a)^3}{12} \cdot |f''(\xi)|\]
\end{definition}

\subsection{Simpson's Rule}
The Simpson's rule is based on piecewise quadratic interpolation.
This means that we need one additional sampling point per subintervall in comparison to the trapezoid rule.
Since we used equally sized subintervals \(\tau_i\) we simply take their center points.
Hence we need to temporarily redefine the notation for our subintervalls
\[\tau_i := [x_{i-1}; x_{i+1}]~\text{with centers}~x_i\]
and use newton divided differences to represent the interpolation polynomial.
\[p(x) = f(x_{i-1} + (x - x_{i-1}) [x_{i-1}, x_i]f + (x - x_{i-1})(x - x_i)[x_{i-1}, x_{i+1}]f\]
As before we rewrite
\begin{equation*}
   \begin{split}
      \int_{\tau_i} p(x) dx & = 2hf(x_{i-1}) + 2h^2 \frac{f(x_i) - f(x_{i-1})}{x_i - x_{i-1}} + \frac{2}{3}h^3 \frac{\frac{f(x_{i+1} - f(x_i)}{x_{i+1} - x_i} - \frac{f(x_i) - f(x_{i-1})}{x_i - x_{i-1}}}{x_{i+1} - x_{i-1}} \\
                            & = \frac{h}{3} \big(f(x_{i-1}) + 4f(x_i) + f(x_{i+1})\big)
   \end{split}
\end{equation*}
To sum those integrals we assume that \(n\) is even and have
\[\sum_{i=1}^n \left(\int_{\tau_i} p(x) dx\right) = \frac{h}{3}\sum_{i=1}^\frac{n}{2} \big(f(x_{2i-2}) + 4f(x_{2i-1}) + f(x_{2i})\big)\]
where we see that due to the indexation we count some samplings twice.
Through fixing that we have the sum \(\frac{h}{3} \big(f(x_0) + 4f(x_1) + 2f(x_2) + \ldots + 4f(x_{n-1}) + f(x_n)\big)\) so we write
\begin{definition}[Simpson Quadrature]
   Given \(f \in C^0(I)\)
   \[Q_S(f) := \frac{h}{3} \sum_{i=0}^n w_i f(x_i) \qquad\text{where}\qquad w_i := \begin{cases}4 & i~\text{is odd}\\2 & i \in (0; n)~\text{and is even}\\ 1 & i \in \{0, n\}\end{cases}\]
\end{definition}
where we have
\lstinputlisting[language=Matlab, caption={Simpson Quadrature}]{simpson_quadr.m}

\paragraph{Quadrature Error}
Now we will derive the error step-by-step; we start on the unit interval \(I = [-1; 1]\) and regard a global simpson quadrature with the center point \(0\).
Suppose we use \emph{Hermite Interpolation} to find \(p \in \mathbb{P}_3\) which approximates \(f\).
Then holds additionally to the \emph{interpolation condition} that \(p'(0) = f'(0)\).
Integrating \(p\) we get
\[\int_I p(x) dx = \frac{1}{3} \big(f(-1) + 4f(0) + f(1)\big)\]
Using \cref{thm:error_repr} we have the error representation
\[|(f-p)(x)| \leq \frac{|(x+1)x^2(x - 1)|}{4!}\|f^{(4)}\|_{\infty}\]
which gives us through integration the estimate
\begin{equation}\label{eq:simps_err_est}
   \begin{split}
      \left| \int_I (f-p)(x) dx\right| & = \left|\int_I \left(\frac{f^{(4)}(\xi_x)}{4!} (x+1)(x-1)x^2\right) dx\right| \leq \int_I \left(\frac{\|f^{(4)}\|_{\infty}}{4!} (x+1)(x-1)x^2\right) dx = \\
                                       & = \frac{\|f^{(4)}\|_{}}{4!} \int_I x^2(1-x^2) dx = \frac{1}{90} \|f^{(4)}\|_{\max}
   \end{split}
\end{equation}
Now we ``transfer'' this estimate onto \(\tau_i\) with
\[\mathcal{X}_i: I \to \tau_i \quad\text{where}\quad \mathcal{X}_i(t) := \frac{1-t}{2} x_{i-1} + \frac{t + 1}{2} x_{i+1}\]
hence we define \(\hat{f} := f \circ \mathcal{X}_i\)
So we have the local error
\begin{equation*}
   \begin{split}
      E_i(f) & = \int_{\tau_i} f(x) dx - Q_S(f) = \int_I \hat{f}(t) dt - Q_S(\hat{f}) = h \int_I \hat{f}(t)dt - \frac{h}{3} \big(\hat{f}(x_{i-1}) + 4\hat{f}(x_i) + \hat{f}(x_{i+1})\big)\\
             & = h \left(\int_I \hat{f}(t) dt - \frac{1}{3} \big(\hat{f}(-1) + 4\hat{f}(0) + \hat{f}(1)\big)\right)
   \end{split}
\end{equation*}
where we used the fact that \(Q_S(f) = \frac{h}{3}\big(f(x_{2i-2}) + 4f(x_{2i-1}) + f(x_{2i})\big)\).
above we can apply the error estimation of \cref{eq:simps_err_est} and obtain
\[E_i(f) \leq \frac{h}{90} \|\hat{f}^{(4)}\|_{x \in I}\]
where we have to transform the norm onto \(\tau_i\).
The chain rule gives us according to the linearity of \(\mathcal{X}_i\)
\[\hat{f}^{(4)} \circ \mathcal{X}_i^{-1}(x) = f^{(4)}(x) \left(\frac{d}{dt} \mathcal{X}_i(t)\right)^4 = f^{(4)}(x)h^4\]
with which follows that
\[E_i(f) \leq \frac{h}{90} \|\hat{f}^{(4)}\|_{x \in I} = \frac{h}{90} \max_{x \in \tau_i}\Big|\big(\hat{f}^{(4)} \circ \mathcal{X}_i^{-1}\big)(x)\Big| = \frac{h^5}{90} \max_{x \in \tau_i} \Big|f^{(4)}(x)\Big| = \frac{h^5}{90} \Big\|f^{(4)}\Big\|_{\max \in \tau_i}\]
which summed up gives
\[E(f) \leq \sum_{i=1}^n E_i(f) = \frac{h^5}{90} \sum_{i=1}^\frac{n}{2} \Big\|f^{(4)}\Big\|_{\infty, [x_{2i-2}; x_{2k}]} \leq \|f^{(4)}\|_{\infty, I} \frac{h^2}{90} \cdot \frac{n}{2} = \frac{b-a}{180}h^4\|f^{(4)}\|_{\infty, I}\]
\begin{definition}[Summed Simpsons Error]
   \[E(f) = \frac{b-a}{180}h^4\|f^{(4)}\|_\infty\]
\end{definition}

\begin{definition}[Global Simpsons Error]
   \[E(f) = \frac{1}{90} \left(\frac{b-a}{2}\right)^5 |f^{(4)}(\xi)|\]
\end{definition}

\subsection{General Newton-Cotes Formula}
We regard
\[\mathcal{I}(f) := \int_I w(x)f(x) dx\]
where \(w\) a positive \emph{weight function}.

\begin{definition}[Weight Function]
   \(w: I \to \mathbb{R} \in C^0\big((a; b)\big)\) where
   \begin{enumerate}[label=\roman*, align=Center]
      \item \(\forall x \in I: w(x) > 0\)
      \item \(\int_I w(x) \cdot x^k dx < \infty\)
   \end{enumerate}
\end{definition}
\begin{remark}
   \(I\) may now be finite of infinite.
   If it is infinite, we must make sure that \(\mathcal{I}(f)\) is well-defined, at least when \(f\) is a polynomial.
   We achieve this by requiring (ii), that all moments of the weight function exist and be finite.
\end{remark}

\begin{definition}[Degree of Exactness]
   A quadrature \(Q(f)\) has the degree of exactness \(k \in \mathbb{N}\) iff
   \[\forall p \in \mathbb{P}_k: E(p) = 0~\land~\exists p \in \mathbb{P}_{k+1}: E(p) \neq 0\]
\end{definition}

\begin{definition}[Interpolatory Quadrature]
   A quadrature \(Q(f)\) with a degree of exactness \(k \geq n\).
\end{definition}
\begin{remark}
   Interpolatory formulae are precisely those obtained by interpolation, i.e. for which holds
   \[Q(f) = \int_I w(x) p(x)dx = \sum_{i=1}^n w_i f(x_i)\]
   which is equivalent to defining the weights as
   \[w_i = \int_I l_i(x) w(x)dx\]
   with the lagrange basis (\ref{def:lagrange_basis}).

   This is evident when we remind ourselves that the interpolation \(p[q] \in \mathbb{P}_n\) of \(q \in \mathbb{P}_n\) is unique, i.e. \(p[q] = q\) and the quadrature is exact for all polynomials of maximal degree \(n\).

   This means that for a fixed sampling point set, the weights are uniquely determined through the integral above.
   This uniqueness characterizes Newton-Cotes formulas.
\end{remark}

\begin{example}
   Let \(w(x) = \frac{1}{\sqrt{x}}\) be a weight function.
   We want to compute the two-point-Newton-Cotes formula of
   \[f(x) := \cos\left(\frac{\pi}{2} x\right)\]
   i.e. using 2 sampling points.
   This tells us directly the degree of exactness.
   \[n+1 = 2 \implies n = 1 \implies k = 2 \cdot 1 + 1 = 3\]
   hence we have
   \[Q(f) = \sum_{i=0}^1 w_i f(x_i) = w_0f(x_0) + w_1f(x_1)\]
   where
   \begin{equation*}
      \begin{split}
         w_0 & = \int_0^1 w(x)l_0(x) dx = \int_0^1 \frac{1}{\sqrt{x}} \frac{x - 1}{0 - 1} dx = \int_0^1 \frac{1 - x}{\sqrt{x}} dx \overset{u = \sqrt{x},~du = \frac{1}{2\sqrt{x}}}{=} 2 \int_0^1 (1 - u^2) du =\\
             & = -2 \int_0^1 u^2 du + 2 \int_0^1 1 du = \left.-2\frac{u^3}{3}\right\rvert_0^1 + 2u\rvert_0^1 = -\frac{2}{3} + 2 = \frac{4}{3}\\
         w_1 & = \int_0^1 w(x)l_1(x) dx = \int_0^1 \frac{1}{\sqrt{x}} \frac{x - 0}{1 - 0} dx = \int_0^1 \frac{x}{\sqrt{x}} dx = \int_0^1 \sqrt{x} dx = \left.2\frac{x^\frac{3}{2}}{3}\right\rvert_0^1 = \frac{2}{3}
      \end{split}
   \end{equation*}
   So we have
   \[\int_0^1 \cos\left(\frac{\pi}{2}x\right) dx \approx Q(f) = \frac{4}{3}f(0) + \frac{2}{3}f(1) = \frac{4}{3} \cdot 1 + \frac{2}{3} \cdot 0 = 1.333\ldots\]
\end{example}

\subsubsection{Stability Constant}
In this section we always assume that \(Q(f)\) has a degree of exactness \(k \geq 0\).

A linear map \(f\) between two normed vector spaces \(V, W\) is \emph{bounded} iff
\[\forall v \in V: \|f(v)\|_W \leq C \cdot \|v\|_V\]

\begin{proposition}
   Let
   \[\mathcal{I}(f): C^0(I) \to \mathbb{R} \qquad\text{where}\qquad f \mapsto Q(f) + E(f)\]
   \[Q(f): C^0(I) \to \mathbb{R} \qquad\text{where}\qquad f \mapsto Q(f)\]
   Then is
   \begin{enumerate}[label=\roman*, align=Center]
      \item \(\mathcal{I}(f)\) bounded
         \[\forall f \in C^0(I): |Q(f) + E(f)|\leq \int_I w(x)dx \cdot \|f\|_\infty\]
      \item \(Q(f)\) bounded
         \[\forall f \in C^0(I): |Q(f)| \leq \frac{\sum_{i=0}^n |w_i|}{\sum_{i=0}^n w_i} \cdot \int_I w(x) dx \cdot \|f\|_\infty\]
   \end{enumerate}
\end{proposition}
\begin{proof}
   (i) follows from the monotony of the integral.

   For (ii) we use the fact that
   \[|Q(f)| = \left|\sum_{i=0}^n w_i f(x_i)\right| \leq \max_{k \in [1;n]} |f(x_k)| \cdot \sum_{i=0}^n |w_i| \leq \|f\|_\infty \cdot \sum_{i=0}^n w_i \cdot \frac{\sum_{i=0}^n |w_i|}{\sum_{i=0}^n w_i}\]
   Since we have for \(Q(f)\) the degree of exactness of \(k \geq 0\) follows
   \[\sum_{i=0}^n w_i = \int_I w(x) dx\]
   and so the claim.
\end{proof}

\begin{definition}[Stability Constant]
   Given a quadrature \(Q(f)\)
   \[C_Q := \frac{\sum_{i=0}^n |w_i|}{\sum_{i=0}^n w_i}\]
\end{definition}

\begin{theorem}[Newton-Cotes Error Estimation]\label{thm:newt_cot_err}
   Given a quadrature \(Q(f)\)
   \[\forall f \in C^0(I): |E(f)| \leq \int_I w(x)dx \cdot (1 + C_Q) \cdot \inf_{q \in \mathbb{P}_k} \|f - q\|_\infty\]
\end{theorem}
\begin{proof}
   From the degree of exactness \(k \geq 0\) follows
   \[\forall q \in \mathbb{P}_k: E(q) = 0\]
   The linear map \(E(f): C^0(I) \to \mathbb{R}\) is bounded
   \[\forall f \in C^0(I): |E(f)| = |\mathcal{I}(f) + Q(f)| \leq |\mathcal{I}(f)| + |Q(f)| \leq \int_I w(x) dx \cdot (1 + C_Q) \cdot \|f\|_\infty\]
   hence for some \(q \in \mathbb{P}_k\) holds
   \[|E(f)| = |E(f-q)| \leq \int_i w(x) dx \cdot (1 + C_Q) \cdot \|f - q\|_\infty\]
   since \(q\) is arbitrary we can use the infimum and receive the claim.
\end{proof}

\begin{example}
   We regard the example from earlier where have \(a = 0\), \(b = 1\) and the weights \(w_0 = \frac{4}{3}\), \(w_1 = \frac{2}{3}\).
   Only now we need the actual interpolation of \(f\), which is linear since we have the two-point-Newton-Cotes formula:
   \[q(x) = f(0) + (x-0) \cdot \frac{f(1) - f(0)}{1 - 0} = 1 + x \cdot \frac{0-1}{1} = 1-x\]
   We know that \(k \geq 0\), so the stability constant of \(Q(f)\) is
   \[C_Q = \frac{\left|\frac{4}{3}\right| + \left|\frac{2}{3}\right|}{\frac{4}{3} + \frac{2}{3}} = 1\]
   further we compute
   \[\int_I w(x) dx = \int_0^1 \frac{1}{\sqrt{x}} dx = 2\sqrt{x}\rvert_0^1 = 2\sqrt{1} - 2\sqrt{0} = 2\]
   and
   \[\|f - q\|_\infty = \max_{x \in [0;1]} \left|\cos\left(\frac{\pi}{2}x\right) - (1 - x)\right| = 0.00915247\ldots\]
   So we have the error estimate
   \[|E(f)| \leq 2 \cdot (1 + 1) \cdot 0.00915247 = 0.03660988\]
\end{example}

\newpage

\subsection{Gaussian Quadrature}
The question naturally arises whether we can do better, that is, whether we can achieve \(k > n\) by a judicious choice of the sampling points.

\begin{theorem}\label{thm:max_deg_exact}
   Let \(d \in \mathbb{N}_0\).
   A Newton-Cotes formula \(Q(f)\) has a degree of exactness of \(k = n + 1 + d\) iff
   \begin{enumerate}[label=\roman*, align=Center]
      \item \(Q(f)\) is interpolatory
      \item The sampling point polynomial suffices
         \[\forall p \in \mathbb{P}_d: \int_I \omega_n(x) p(x) w(x) dx = 0\]
   \end{enumerate}
\end{theorem}
\begin{remark}
   Point (ii) can be interpreted as a requirement for the sampling points.
   To be more precise let \(w\) be a weight function, then we can define the scalar product
   \[\langle u, v \rangle_w := \int_I u(x) v(x) w(x) dx\]
   With this (ii) states that the sampling point polynomial must be orthogonal to all polynomials of degree \(d\).
   From this follows
   \[\omega_n \not\in \mathbb{P}_d \implies d \leq n\]
   which means \(d = n\) is maximal, so from the theorem follows that
   \[k = 2n + 1\]
   is the \emph{maximal degree of exactness} of a quadrature based on \(n+1\) sampling points.
   The, in this sense, optimal quadratures are called \emph{Gaussian} quadratures.
\end{remark}
\begin{proof}
   First we proove that degree of exactness \(k = n + 1 + d\) implies (i) and (ii).

   (i) follows directly.
   For an arbitrary \(p \in \mathbb{P}_d\) we regard \(\omega_n \cdot p \in \mathbb{P}_{n + 1 + d}\)
   \[\int_I \omega_n(x) p(x) w(x) dx = \sum_{i=0}^n \omega_n(x_i) p(x_i) w_i\]
   which is 0 since the sampling points are the roots of \(\omega\), hence (ii) follows.

   Now we show that (i) and (ii) imply a degree of exactness \(k = n + 1 + d\).
   Let \(p \in \mathbb{P}_{n+1+d}\) be arbitrary and \(q \in \mathbb{P}_d\), \(r \in \mathbb{P}_n\) s.t.
   \[p = q \omega_n + r\]
   then is
   \[\int_I p(x) w(x) dx = \int_I q(x) w(x) \omega_n(x) dx + \int_I r(x) w(x)dx\]
   The first integral on the right-hand side is 0 due to the orthogonality of \(\omega_n\).
   For the second we apply (i) and receive
   \[\int r(x) w(x) dx = \sum_{i=0}^n w_i r(x_i) = \sum_{i=0}^n w_i \big(p(x_i) - q(x_i) \omega_n(x)\big) = \sum_{i=0}^n p(x_i) w_i\]
   where we again used the roots of \(\omega_n\).
   Hence we have shown that
   \[\int_I p(x) w(x) dx = \sum_{i=0}^n p(x_i) w_i\]
   which means that the error is 0.
\end{proof}

\begin{example}
   Let \(w(x) = \frac{1}{\sqrt{x}}\) be a weight function.
   We want to compute the two-point-Gaussian formula of
   \[f(x) := \cos\left(\frac{\pi}{2} x\right)\]
   i.e. using 2 sampling points.
   This tells us directly the degree of exactness, aswell as the degree of the sampling point polynomial.
   \[n+1 = 2 \implies n = 1 \implies k = 2 \cdot 1 + 1 = 3\]
   \[\omega_1(x) := x^2 + bx + a\]
   now we compute the sampling points \(x_0\) and \(x_1\)
   \begin{equation*}
      \begin{split}
         \int_0^1 w(x) \omega_1(x) x^0 dx & = \int_0^1 \frac{1}{\sqrt{x}} (x^2 + bx + a) dx \overset{u = \sqrt{x},~du = \frac{1}{2\sqrt{x}}}{=} 2 \int_0^1 (a + bu^2 + u^4) du = \\
                                          & = 2 \int_0^1 u^4 du + 2b \int_0^1 u^2 du + 2a\int_0^1 1du = \left.2\frac{u^5}{5}\right\rvert_0^1 + \left.2b\frac{u^3}{3}\right\rvert_0^1 + 2au\rvert_0^1 = \frac{2}{5} + \frac{2}{3}b + 2a\\
         \int_0^1 w(x) \omega_1(x) x^1 dx & = \int_0^1 \frac{1}{\sqrt{x}} (x^2 + bx + a)x dx = \int_0^1 \sqrt{x} (x^2 + bx + a) dx \overset{u = \sqrt{x},~du = \frac{1}{2\sqrt{x}}}{=} \\
                                          & = 2 \int_0^1 u^2(a + bu^2 + u^4) du = 2 \int_0^1 (au^2 + bu^4 + u^6) du = \\
                                          & = 2\int_0^1 u^6 du + 2b \int_0^1 u^4 du + 2a \int_0^1 u^2du = \left.2 \frac{u^7}{7}\right\rvert_0^1 + \left. 2b\frac{u^5}{5}\right\rvert_0^1 + \left.2a\frac{u^3}{3}\right\rvert_0^1 = \frac{2}{3}a + \frac{2}{5}b + \frac{2}{7}
      \end{split}
   \end{equation*}
   Now according to the theorem we solve
   \begin{equation*}
      \begin{split}
         2a + \frac{2}{3}b + \frac{2}{5} & = 0\\
         \frac{2}{3}a + \frac{2}{5}b + \frac{2}{7} & = 0
      \end{split}
   \end{equation*}
   and receive \(a = \frac{3}{35}\), \(b = -\frac{6}{7}\) which means for the sampling point polynomial
   \[\omega_1(x) = x^2 - \frac{6}{7}x + \frac{3}{35}\]
   of which we determine its roots (our sampling points)
   \[x_{0,1} = \frac{\frac{6}{7} \pm \sqrt{\left(-\frac{6}{7}\right)^2 - 4 \frac{3}{35}}}{2} = \frac{3}{7} \pm \frac{2}{35} \sqrt{30}\]
   Finally we determine the weights
   \begin{equation*}
      \begin{split}
         w_0 & = \int_0^1 w(x)l_0(x) dx = \int_0^1 \frac{1}{\sqrt{x}} \frac{x - \left(\frac{3}{7} - \frac{2}{35} \sqrt{30}\right)}{\frac{3}{7} + \frac{2}{35} \sqrt{30} - \left(\frac{3}{7} - \frac{2}{35} \sqrt{30}\right)} dx = 1 - \frac{1}{3}\sqrt{\frac{5}{6}}\\
         w_1 & = \int_0^1 w(x)l_1(x) dx = \int_0^1 \frac{1}{\sqrt{x}} \frac{x - \left(\frac{3}{7} + \frac{2}{35} \sqrt{30}\right)}{\frac{3}{7} - \frac{2}{35} \sqrt{30} - \left(\frac{3}{7} + \frac{2}{35} \sqrt{30}\right)} dx = 1 + \frac{1}{3}\sqrt{\frac{5}{6}}
      \end{split}
   \end{equation*}
\end{example}

\begin{example}
   Suppose we want to approximate \(\mathcal{I}(f) = \int_I f(x) dx\).

   Find the sampling nodes \((x_i)_{i=0}^2\) and the weights \((w_i)_{i=0}^2\) such that
   \[Q(f) = \sum_{i=0}^2 w_i f(x_i)\]
   has maximal degree of exactness.

   Since \(n=2\) we know that our sampling point polynomial is of the form
   \[\omega_2(x) = x^3 + ax^2 + bx + c\]

   Since we want maximal degree of exactness, i.e. \(k = n + 1 + d\), we know that \(d = 2\) from \cref{thm:max_deg_exact}.
   This means that
   \[\forall p \in \mathbb{P}_{\leq 2}: \int_I \omega_2(x) \cdot p(x) \cdot w(x) dx = 0\]
   Furthermore since \(\mathcal{I}(f)\) does not contain a weight function we know that \(w(x) \equiv 1\).
   Hence we solve the following system of linear equations
   \[\int_I \omega_2(x) \cdot 1 dx \overset{!}{=} 0 \qquad \int_I \omega_2(x) \cdot x dx \overset{!}{=} 0 \qquad \int_I \omega_2(x) \cdot x^2 dx \overset{!}{=} 0\]
   which gives us our coefficients of \(\omega_2(x)\).

   Next off we determine the roots of \(\omega_2(x)\), which are our sampling points \(x_i\).

   Finally we can compute the weights as usual
   \[w_0 = \int_I l_0(x) \cdot 1 dx \qquad w_1 = \int_I l_1(x) \cdot 1 dx \qquad w_2 = \int_I l_2(x) \cdot 1 dx\]
\end{example}

\begin{example}
   Construct \(Q(f)\) to approximate \(\mathcal{I}(f) := \int_I w(x) \cdot f(x) dx\) where \(I := [0; 1]\) and \(w(x) := x^\frac{1}{3}\), such that the degree of exactness is maximal and it uses \(f(0)\) and \(\int_I f(x) dx\).

   From the description we know we have
   \[Q(f) = w_0 \cdot f(0) + w_1 \cdot \int_I f(x) dx\]
   which means we have two sampling points \(n + 1 = 2 \implies n = 1\) and since the degree of exactness is supposed to be maximal we have \(d = n \implies d = 1\) and so
   \[\forall p \in \mathbb{P}_1: E(p) = 0 \implies \mathcal{I}(p) - Q(p) = 0 \implies \mathcal{I}(p) = Q(p)\]
   which gives us the system
   \[Q(1) = w_0 \cdot 1 + w_1 \cdot \int_I 1 dx \overset{!}{=} \int_I w(x) \cdot 1 dx = \mathcal{I}(1)\]
   \[Q(x) = w_0 \cdot 0 + w_1 \cdot \int_I x dx \overset{!}{=} \int_I w(x) \cdot x dx = \mathcal{I}(x)\]
   through which we determine \(w_0\) and \(w_1\).
\end{example}

\begin{proposition}
   Let \(w\) be a weight function.
   \begin{enumerate}[label=\roman*, align=Center]
      \item The sampling points as roots of orthogonal polynomials are real, simple and in \((a;b)\).
      \item The weights are positive.
      \item The Gaussian formulas converge for all continuous functions.
   \end{enumerate}
\end{proposition}
\begin{proof}[Proof (i)]
   We define a scalar product through the weight function
   \[\langle u, v \rangle_w := \int_I u(x) v(x) w(x) dx\]
   Now let \((p_n)_{n \in \mathbb{N}_0}\) be an orthonogonal family.
   Furthermore let \(a < x_1 < \ldots < x_l < b\) be the roots of \(p_n\) in \((a;b)\) in which \(p_n\) changes its sign.
   This means that the multiplicities of those roots is odd.

   Now we prove \(l = n\) by contradiction, so suppose \(l < n\).
   Then has
   \[q(x) := \omega_l(x)\]
   the degree \(l < n\), such that \(\langle p_n, q\rangle_w = 0\).
   However we chose \(q\) such that \(w(x) q(x) p_n(x)\) does not change its sign on \((a;b)\).
   This implies that
   \[\langle p_n, q\rangle_w = \int_I w(x) p_n(x) q(x) dx \neq 0\]
   which is a contradiction.
\end{proof}
\begin{proof}[Proof (ii)]
   Let \(l_i\) be the lagrange-basis functions of degree \(n\), then holds
   \[0 < \int_I l_i(x)^2 w(x)dx = \sum_{j=0}^n l_i(x_j)^2 w_j = w_i~\forall 0 \leq i \leq n\]
   because we have the degree of exactness \(2n + 1\) or \(n+1\)-point-Gaussian formulas.
\end{proof}
\begin{proof}[Proof (iii)]
   We use the error estimation of \cref{thm:newt_cot_err}.
   Since the weights of Gaussian quadratures are positive is the stability constant \(C_Q = 1\) and we have
   \[|E(f)| \leq 2 \int_I w(x) dx \cdot \inf_{q \in \mathbb{P}_{2n + 1}} \|f - q\|_\infty\]
   According to Weierstrass converges the right side to 0 for \(n \to \infty\).
\end{proof}

\subsubsection{Computation of Sampling Points}
In order to compute the sampling points we need an orthogonal system of polynomials regarding
\[\langle u, v \rangle_w := \int_I u(x) \cdot v(x) \cdot w(x) dx\]
The Gram-Schmidt method is complex and numerically instable, hence we introduce a method of computing the sampling points as eigenvalues of tridiagnoal matrices.
Let \(w\) be a weight function.
Polynomials \(p_k \in \mathbb{P}_k\) are defined through
\[p_{-1} \equiv 0 \qquad p_0(x) \equiv 1 \qquad p_{k+1}(x) = (x - \alpha_k)\cdot p_k(x) - \beta_k \cdot p_{k-1}(x)\]
where
\[\alpha_k := \frac{\langle x \cdot p_k, p_k\rangle_w}{\langle p_k, p_k \rangle_w} \qquad \beta_k := \frac{\langle p_k, p_k\rangle_w}{\langle p_{k-1}, p_{k-1}\rangle_w} \qquad \beta_0 := \int_I w(x) dx\]

Then we write the coefficients into a matrix like so
\[J := \begin{pmatrix}
      \alpha_0       & \sqrt{\beta_1} & 0              & \cdots             & 0                  \\
      \sqrt{\beta_1} & \alpha_1       & \sqrt{\beta_2} & \ddots             & \vdots             \\
      0              & \sqrt{\beta_2} & \ddots         & \ddots             & 0                  \\
      \vdots         & \ddots         & \ddots         & \ddots             & \sqrt{\beta_{n-1}} \\
      0              & \cdots         & 0              & \sqrt{\beta_{n-1}} & \alpha_{n-1}
\end{pmatrix}\]

\begin{theorem}[Gaussian-Quadrature Sampling Points]
   The sampling points of the \(n\)-point Gaussian quadrature are the eigenvalues
   \[\forall i \in [1;n]: J v_i = x_i v_i\]
\end{theorem}
\begin{proof}
   We prove \(\det(\lambda I_n - J) = p_n(x)\) by induction over \(n\).

   \emph{IB:} \(n = 1 \implies \det(\lambda I_n - J_1) = x - \alpha_0 = p_1(x)\)

   \emph{IH:} Suppose the statement holds for some \(n\).

   \emph{IS:} Assume IH holds.
   We prove the step \(n-1 \to n\).
   For the characteristic polynomial holds the recursion
   \[\det(\lambda I_n - J) = (x - \alpha_{n-1})\det(\lambda I_{n-1} - J_{n-1}) - \beta_{n-1}\det(\lambda I_{n-2} - J_{n-2}\]
   This recursion is identical to the recursion of the orthogonal polynomials above hence the polynomials are equal.
\end{proof}

\subsection{Numerical Differentiation}\label{sec:num_diff}
In this section we will approximate the \emph{first-order} derivative of a function.
Higher order derivatives can be approximated by successively applying the presented technique.
As with numerical integration we interpolate \(f \in C^0(I)\) and differentiate the approximation.

As before we compute the interpolation polynomial through the newton divided differences
\[p_n(f) = f_0 + (x-x_0)[x_0, x_1]f + \ldots + (x - x_0)(x - x_1)\cdots(x- x_{n-1})[x_0, \ldots, x_n]f\]
hence we use the error representation \(e_n(x) := (f - p_n)(x)\) of \cref{thm:error_repr} and thus assumed that \(f \in C^{n+1}(I)\).

So for the derivative of \(f\) we have
\begin{equation*}
   \begin{split}
      f'(x_0) = p'_n(x_0) + e'_n(x_0) & = [x_0, x_1]f + (x_0 - x_1)[x_0, x_1, x_2]f + \ldots \\
                                      & \ldots + (x_0 - x_1) \cdots (x_0 - x_{n-1})[x_0, \ldots, x_n]f + \\
                                      & + \prod_{i=1}^n(x_0 - x_i)\frac{f^{(n+2)}(\xi_{x_0})}{(n+1)!}
   \end{split}
\end{equation*}
However this equality only holds if \(f \in C^{n+2}(I)\) since
\[e_n'(x) = \prod_{i=0}^n(x - x_i) \cdot \frac{d}{dx} \left(\frac{f^{(n+1)}(\xi_x)}{(n+1)!}\right) = \prod_{i=0}^n(x - x_i) \cdot \frac{f^{(n+2)}(\xi_x)}{(n+1)!} \cdot \frac{d}{dx} \xi_x\]
and only \emph{afterwards} \(x = x_0\) is substituted.

Now we can derive an error estimation from the relation above.
\begin{theorem}[Error Estimation]
   For \(f \in C^{n+2}(I)\) holds the error estimation
   \[r_n'(x_0) \leq M_n \cdot H^n \qquad\text{where}\qquad M_n :=\frac{\|f^{(n+1)}\|_\infty}{(n+1)!} \quad\text{and}\quad H := |[x_0; x_n]|\]
\end{theorem}
\begin{remark}
   Using \(n\) and \(H\) we can achieve a given accuracy.

   We can increase the interpolation order \(n\) in which case we require
   \[M_n \cdot H^n \xrightarrow{n \to \infty} 0\]

   Or we can shrink \(H\) for a fixed \(n\) in which case we require that \(f \in C^{n+2}(I)\).
   The convergence of \(e_n'(x_0)\) is algebraic for \(H \to 0\).
\end{remark}

\begin{example}[Forward Difference]
   Suppose we have \(n = 1\) i.e. the sampling points \(x_0\) and \(x_1 = x_0 + h\).
   \[p'(x_0) = [x_0, x_1]f = \frac{f(x_1) - f(x_0)}{x_1 - x_0}\]
   So if \(f \in C^3(I)\) we have the error representation
   \[e'(x_0) = (x_0 - x_1)\frac{f''(\xi)}{2} \qquad\text{for some}~\xi \in (x_0; x_1)\]
\end{example}

\begin{example}[Symmetric Difference]
   Suppose \(n = 2\) i.e. the sampling points \(x_0\), \(x_{-1} = x_0 - h\) and \(x_1 = x_0 + h\).
   \begin{equation*}
      \begin{split}
      p'(x_0) & = [x_{-1}, x_0]f + (x_0 - x_{-1})[x_{-1}, x_0, x_1]f = \frac{f(x_{-1}) - f(x_0)}{x_0 - x_{-1}} + (x_0 - x_{-1})\frac{f(x_{-1}) - 2f(x_0) + f(x_1)}{2h^2}\\
              & = \frac{f(x_1) - f(x_{-1})}{2h}
      \end{split}
   \end{equation*}
   So if \(f \in C^4(I)\) we have the error representation
   \[e'(x_0) = (x_0 - x_{-1})(x_0 - x_1) \frac{f^{(3)}(\xi)}{3!} = -h^2 \frac{f^{(3)}(\xi)}{6}\]
   Note that the symmetric difference achieves higher accuracy than forward difference for the same effort.
   However it is only defined if \(f\) is defined on both sides of \(x_0\), otherwise asymmetrical formulas must be used.
\end{example}

\begin{example}
   Suppose again \(n = 2\) but with the sampling points \(x_0\), \(x_1 = x_0 + h\) and \(x_2 = x_0 + 2h\).
   \[p'(x_0) = \frac{f(x_1) - f(x_0)}{h} - h\frac{f(x_2) - 2f(x_1) + f(x_0)}{2h^2} = \frac{4f(x_1) - 3f(x_0) - f(x_2)}{2h}\]
   with the error
   \[e'(x_0) = 2h^2 \frac{f^{(3)}(\xi)}{6}\]
\end{example}
% TODO: give a concrete example

\subsubsection{Perturbed Data}
Suppose that \(f\) is only available in perturbed form
\[f_s = f + \Delta f\]
but nontheless continuous, this way we can use the symmetric difference
\[f'(x_0) \approx \widetilde{f'}(x_0) = \frac{f_s(x_1) - f_s(x_{-1})}{2h} = \frac{f(x_1) - f(x_{-1})}{2h} + \frac{\Delta f(x_1) - \Delta f(x_{-1})}{2h}\]
analogously we can split the erro into two terms
\[\varepsilon_1 := f'(x_0) - \frac{f(x_1) - f(x_{-1})}{2h} \qquad\text{and}\qquad \varepsilon_2 := \frac{\Delta f(x_1) - \Delta f(x_{-1})}{2h}\]
this way we have \(f'(x_0) - \widetilde{f'}(x_0) = \varepsilon_1 + \varepsilon_2\).
In the worst case there occurs error amplification in the second term
\[|\varepsilon_2| \leq \frac{|\Delta f(x_1)| + |\Delta f(x_{-1})|}{2h}\]
In order to guarantee an estimate of \(|\varepsilon_2| \leq C \cdot h^2\) we need to find some \(h\) such that the error in the derivative is minimal.
\[|f'(x_0) - \widetilde{f'}(x_0) | \leq \frac{\delta}{h} + C_f \cdot h^2 =: E(h)\]
where \(C_f\) depends on the third derivative of \(f\) and we assume \(\|\Delta f\|_\infty \leq \delta\).
We see that the minimum of \(E(h)\) is given through
\[h_{\opt} := C_1 \cdot \delta^\frac{1}{3} \quad\text{where}\quad C_1 = (2 C_f)^\frac{1}{3}\]
This means that even with an optimal step-length \(h\) there occurs error accumulation.

\section{Discrete Fourier Transformation}
The Fourier transform (FT) decomposes (also called analysis) a function of time (a signal) into its constituent frequencies.
This is similar to the way a musical chord can be expressed in terms of the volumes and frequencies (or pitches) of its constituent notes.

% TODO: present case when N is even
 We regard a continuous periodic function \(f: I \to \mathbb{C}\) on \(I = [a; b]\) and assume that we have given a column vector of equidistant sampling points.
\[f = (f(x_i))_{i=0}^{n-1} \quad\text{where}\quad x_i = a + i \cdot \Delta t \quad\text{with}\quad \Delta t := \frac{b-a}{n}\]
The goal is now find an aproximation \(\widetilde{f}\) through an exponential function
 \[\widetilde{f}(t) := \sum_{k=0}^{n-1} \hat{f}_k \cdot e^{\imag \frac{2\pi}{b-a} k t}\]
where \(\hat{f}_k\) are \emph{fourier coefficients}.
From the interpolation condition of \(\widetilde{f}\) we have
\[\forall i \in [0; n-1]: f(x_i) = \widetilde{f}(i \cdot \Delta t) \implies \forall i \in [0; n-1]: f(x_i) = \sum_{k=0}^{n-1} \hat{f}_k \cdot e^{\imag \frac{2\pi}{b-a} k i \Delta t}\]
Now we need a way to determine these fourier coefficients \(\hat{f}_k\).
We define the column vectors
\[w_k := \left(e^{\imag \frac{2\pi}{b-a} k i \Delta t}\right)_{i=0}^{n-1}\]
and write them in the matrix \(W := (w_0~w_1~\ldots~w_{n-1})\).
Then we see that the interpolation condition can be written as system of linear equations with the sampling values \(f\) and the unknown fourier coefficients \(\hat{f}\) as a column vector.
 \[f = W \cdot \hat{f}\]

The vectors \(w_k\) have the following orthogonality relation.
\begin{proposition}
   For \(i,j \in \{0, \ldots, n-1\}\) holds
    \[\langle \vec{w_i}, \vec{w_j} \rangle = \begin{cases}n & i = j\\ 0 & i \neq j\end{cases}\]
\end{proposition}
\begin{proof}
   It holds that
   \[\langle \vec{w_i}, \vec{w_j} \rangle = \sum_{k=0}^{n-1} e^{\imag i \frac{2\pi}{b-a} k \Delta t} \cdot e^{-\imag j \frac{2\pi}{b-a} k \Delta t} = \sum_{k=0}^{n-1} \alpha^k \quad\text{where}\quad \alpha := e^{\imag \frac{2\pi}{b-a} (i-j) \Delta t}\]
   Now we have two cases.

   1. \(\alpha = 1 \implies i = j \implies \langle \vec{w_i}, \vec{w_j} \rangle = n\)

   2. \(\alpha \neq 1 \implies i \neq j\) then follows
   \[\sum_{k=0}^{n-1} \alpha^k = \frac{1-\alpha^n}{1 - \alpha} = \frac{1 - e^{\imag \frac{2\pi}{b-a} (i-j) n \Delta t}}{1 - e^{\imag \frac{2\pi}{b-a} (i-j) \Delta t}} = \frac{1 - e^{\imag \frac{2\pi}{b-a} (i-j) (b-a)}}{1 - e^{\imag \frac{2\pi}{b-a} (i-j) \Delta t}} = \frac{1 - (e^{2\pi\imag})^{i-j}}{1 - e^{\imag \frac{2\pi}{b-a} (i-j) \Delta t}} = 0\]
\end{proof}

From this proposition follows that \(W^H \cdot W = n \cdot I_n\) and with that
\[f = W \cdot \hat{f} \implies \hat{f} = \frac{1}{n} W^H \cdot f\]
This leads us to the following
\begin{definition}[Discrete Fourier Transform]
  For an arbitrary vector \(f \in \mathbb{R}^n\), the discrete fourier transform of the period length \(b-a\) is given through the fourier coefficients
  \[\hat{f}_s = \frac{1}{n} \langle \vec{f}, \vec{w_s}\rangle = \frac{1}{n} \sum_{k=0} f_k \cdot e^{- \imag s \frac{2\pi}{b-a} k \Delta t}\]
\end{definition}

Keeping in mind that we need an efficient implementation to calculate those coefficients, we look back at the derivation of this definition and use the matrix multiplication \(\hat{f} = \frac{1}{n} W^H \cdot f\).
\[\hat{f} = \begin{pmatrix}\hat{f}_0\\\hat{f}_1\\\vdots\\\hat{f}_{n-1}\end{pmatrix} = \frac{1}{n} \begin{pmatrix}
   e^{-\imag 0 \frac{2\pi}{b-a} 0} & \cdots & e^{-\imag 0 \frac{2\pi}{b-a} (n-1)}\\
   \vdots & & \vdots \\
   e^{-\imag (n-1) \frac{2\pi}{b-a} 0} & \cdots & e^{-\imag (n-1) \frac{2\pi}{b-a} (n-1)}\\
\end{pmatrix} \cdot \begin{pmatrix}f_0\\ f_1\\ \vdots \\ f_{n-1}\end{pmatrix}\]
% TODO: adapt code to be genereic, move this code to case where N even
\lstinputlisting[language=Matlab, caption={Calculate Fourier Coefficients}]{calc_fourier_coeff.m}

\begin{lemma}[Inverse Fourier Transform]
   The inverse fourier transform is given through
   \[f_s = \sum_{k=0}^{n-1} \hat{f}_k \cdot e^{\imag s \frac{2\pi}{b-a} k \Delta t}\]
\end{lemma}
\begin{proof}
   \begin{equation*}
      \begin{split}
         \sum_{k=0}^{n-1} \hat{f}_k \cdot e^{\imag s \frac{2\pi}{b-a} k \Delta t} & = \frac{1}{n} \sum_{k=0}^{n-1}\sum_{l=0}^{n-1} f_l \cdot e^{-\imag s \frac{2\pi}{b-a} l \Delta t} \cdot e^{\imag s \frac{2\pi}{b-a} k \Delta t} = \\
                                                                                  & = \frac{1}{n}\sum_{l=0}^{n-1}f_l \left(\sum_{k=0}^{n-1} e^{-\imag s \frac{2\pi}{b-a} l \Delta t} \cdot e^{\imag s \frac{2\pi}{b-a} k \Delta t}\right) = \frac{1}{n}\sum_{l=0}^{n-1} f_l \cdot \langle \vec{w_s}, \vec{w_l}\rangle = f_s
      \end{split}
   \end{equation*}
\end{proof}

\begin{lemma}
   It holds that
   \[\forall s \in [1; n-1]: \hat{f}_s = \overline{\hat{f}_{n-s}}\]
   Especially if \(n\) is even \(s = \frac{n}{n} \implies \hat{f}_s \in \mathbb{R}\).
\end{lemma}
\begin{proof}
   \[\overline{\hat{f}_{n-s}} = \frac{1}{n} \sum_{k=0}^{n-1} f_k \cdot e^{\imag (n-s) \frac{2\pi}{b-a} k \Delta t} = \frac{1}{n} \sum_{k=0}^{n-1} f_k \cdot \underbrace{e^{\imag 2\pi}}_{=1} \cdot e^{-\imag s \frac{2\pi}{b-a} k \Delta t} = \frac{1}{n} \sum_{k=0}^{n-1} f_k \cdot e^{-\imag s \frac{2\pi}{b-a}k \Delta t} = \hat{f}_s\]
\end{proof}

\begin{lemma}
   The trigonometric polynomial
   \[T(t) := \hat{A}_0 + \sum_{m=1}^{\lfloor\frac{n-1}{2}\rfloor} \left[\hat{A}_m \cdot \cos\left(\frac{2\pi mt}{b-a}\right) + \hat{B}_m \cdot \sin\left(\frac{2\pi mt}{b-a}\right) \right] + \begin{cases}\hat{A}_{\frac{n}{2}} \cdot \cos\left(\frac{\pi n t}{b-a}\right) & n~\text{even}\\ 0 & \text{else}\end{cases}\]
   interpolates the sampling points with the coefficients
   \[\hat{A}_0 := \Re(\hat{f}_0) \qquad \hat{A}_m := 2 \cdot \Re(\hat{f}_m) \qquad \hat{B}_m := -2 \cdot \Im(\hat{f}_m) \qquad \hat{A}_{\frac{n}{2}} := \Re(\hat{f}_{\frac{n}{2}})\]
\end{lemma}
\begin{proof}
   Let \(t := r \cdot \Delta t\) for some \(r \in \{0, 1, \ldots, n-1\}\).
   We verify the interpolation condition for \(m \in [1; \frac{n}{2}]\).
   \begin{equation*}
      \begin{split}
         \hat{f}_m \cdot e^{\imag \frac{2\pi}{b-a} m t} + \hat{f}_{n-m} \cdot e^{\imag \frac{2\pi}{b-a} (n-m) t} & = \Re(\hat{f}_m) \cdot \left(e^{\imag \frac{2\pi}{b-a} m t} + e^{\imag \frac{2\pi}{b-a} (n-m) t}\right) +\\
                                                                                                                 & + \imag \Im(\hat{f}_m) \cdot \left(e^{\imag \frac{2\pi}{b-a} m t} - e^{\imag \frac{2\pi}{b-a} (n-m) t}\right) = \\
                                                                                                                 & = 2 \Re(\hat{f}_m) \cdot \cos\left(\frac{2\pi m t}{b-a}\right) - 2 \Im(\hat{f}_m) \cdot \sin\left(\frac{2\pi mt}{b-a}\right)
      \end{split}
   \end{equation*}

   From this follows that
   \[\widetilde{f}(t) = \sum_{m=0}^{n-1} \hat{f}_m \cdot e^{\imag \frac{2\pi}{b-a} mt} = \hat{A}_0 + \sum_{m=1}^{\lfloor\frac{n-1}{2}\rfloor} \left[\hat{A}_m \cdot \cos\left(\frac{2\pi mt}{b-a}\right) + \hat{B}_m \cdot \sin\left(\frac{2\pi mt}{b-a}\right) \right] + \alpha(t)\]
   with \(\alpha(t) := \hat{f}_{\frac{n}{2}} \left(\cos\left(\frac{\pi nt}{b-a}\right) + \imag \sin\left(\frac{\pi nt}{b-a}\right)\right)\) if \(n\) is even or 0 if \(n\) is odd.
   Finally since
   \[\forall r \in [0; n-1]: \Im(\alpha(r \cdot \Delta t)) = 0\]
   follows that \(T\) interpolates the sampling points.
\end{proof}

\lstinputlisting[language=Matlab, caption={Computing \& evaluating Fourier Polynomial}]{eval_trig_poly.m}

% TODO: \subsection{Fast Fourier Transform (FFT)}

\section{Direct Methods for Systems of linear Equations}
The are \emph{direct} methods of solving such systems which yield --- if we neglect rounding errors --- an exact result after a finite number of steps.
They are applicable to all invertible matrices, which means we require \(A \in \GL_n(\mathbb{C})\).

\subsection{Introduction}
In practice systems of linear equations \(A \cdot x = b\) can get really big i.e. 5-10000 variables.
To achieve efficient algorithms we will try to transform \(A\) into a suitable form for solving the system.
It would be ideal if \(A\) was a diagonal matrix because then the solutions of the system are given through
\begin{equation*}
   \begin{split}
      a_{11}x + 0y + 0z & = b_1\\
      \forall i \in [0; n]: x_i = \frac{b_i}{a_{ii}} \quad\text{since we have}\quad 0x + a_{22}y + 0z & = b_2\\
      0x + 0y + a_{33}z & = b_3
   \end{split}
\end{equation*}
% TODO: Code for diagonal

\subsubsection{Triangular Matrices}
But realistically we can hope to utilize triangular matrices.
\begin{equation*}
   \begin{split}
      a_{11}x + a_{21}y + a_{31}z & = b_1\\
      a_{22}y + a_{32}z & = b_2 \qquad\rightsquigarrow\qquad
      \begin{pmatrix}a_{11} & a_{21} & a_{31}\\ & a_{22} & a_{32}\\ & & a_{33}\end{pmatrix}
      \begin{pmatrix}x\\y\\z\end{pmatrix} = \begin{pmatrix}b_1\\b_2\\b_3\end{pmatrix}\\
      a_{33}z & = b_3\\
   \end{split}
\end{equation*}
Where we see the recursion
\[z = \frac{b_3}{a_{33}} \quad\rightsquigarrow\quad y = \frac{b_2 - a_{32} \frac{b_3}{a_{33}}}{a_{22}} \quad\rightsquigarrow\quad x_i = \frac{b_i - \left(\sum_{j=i+1}^n a_{ji} \cdot x_j\right)}{a_{ii}}\]
In the case of lower triangular system we can compute the system with the following algorithm

\lstinputlisting[language=Matlab, caption={Forward Substitution of Linear Equations}]{solve_lower_tria_sys.m}
and for upper triangular systems we use
\lstinputlisting[language=Matlab, caption={Backward Substitution of Linear Equations}]{solve_upper_tria_sys.m}

For both algorithms we can count the number of arithmetic operations
\[\sum_{i=1}^n \left(1 + \sum_{j=1}^{i-1} 2\right) = 1 + \sum_{k=1}^{n-1} (2 * (n - k) + 1) = 1 + n-1 + 2\sum_{k=1}^{n-1} k = n + 2 \frac{n(n-1)}{2} = n^2\]

\subsubsection{Unitary Matrices}
\begin{definition}[Unitary Matrix]\label{def:unitary_matrix}
   A matrix \(A \in \GL_n(\mathbb{C})\) for which holds
   \[A^H \cdot A = A \cdot A^H = I\]
\end{definition}
\begin{remark}
   We see that the row- and column-vectors of \(A\) are orthonormal regarding the standard dot product.
   \[\langle a_i, a_j \rangle = \begin{cases} 1 & \text{if}~i = j\\ 0 & \text{if}~i \neq j\end{cases}\]
\end{remark}

For complex matrices of the general linear group (as in \cref{def:unitary_matrix}) \(A^H\) in denotes the \emph{conjugate} or \emph{hermitian} transposed.
\begin{definition}[Conjugate Transpose]
   Given \(A \in \Mat_n(\mathbb{C})\)
   \[(A^H)_{ij} := \overline{A_{ji}}\]
\end{definition}
\begin{remark}
   This definition is equivalent to the adjugated of \(A\) i.e. \(A^H = \adj(A)\), since we can see that
   \[A^H = \overline{(A^T)}\]
\end{remark}

Now we can prove the key property of unitary matrices.
\begin{proposition}[Inverse of Unitary Matrix is Conjugate Transpose]
   Let \(A \in \GL_n(\mathbb{C})\) be a unitary matrix, it holds that
   \[A^{-1} = A^H\]
\end{proposition}
\begin{proof}
   \[(A^HA)_{ij} \overset{\text{(i)}}{=} \sum_{m=0}^n \overline{a_{mi}} \cdot a_{mj} = \langle a_j, a_i \rangle \overset{\text{(ii)}}{=} \delta_{ji} \implies A^H = A^{-1}\]
   \begin{enumerate}[label=\roman*, align=Center]
      \item \Cref{def:complex_dot_prod}
      \item Remark of \cref{def:unitary_matrix}
   \end{enumerate}
\end{proof}

Now we see that solve linear systems of equations with unitary matrices is incredibly simple
\[Qx = b \implies x = Q^H b\]
\lstinputlisting[language=Matlab, caption={Solve Unitary System}]{solve_unitary_sys.m}

\subsection{Gaussian Elimination through LU-Decomposition}
Computers usually solve square systems of linear equations using LU decomposition, and it is also a key step when inverting a matrix or computing the determinant of a matrix.

Lower–Upper (LU) decomposition factors a matrix as the product of a lower triangular matrix \(L\) and an upper triangular matrix \(U\).
\[A = L \cdot U \implies (Ax = b \iff LUx = b)\]
Where we first compute \(Ly = b\) and then \(Ux = y\).
Note that in both cases we are dealing with triangular matrices, which can be solved directly by forward and backward substitution without using the Gaussian elimination process (however we do need to do this process to compute the LU decomposition itself).

The above procedure can be repeatedly applied to solve the equation multiple times for different \(b\).
In this case it is faster (and more convenient) to do an LU decomposition of the matrix \(A\) once and then solve the triangular matrices for the different \(b\), rather than using Gaussian elimination each time.
The matrices \(L\) and \(U\) could be thought to have ``encoded'' the Gaussian elimination process.

The cost of solving a system of linear equations is approximately \(\frac{2}{3}n^3\) floating-point operations if the matrix \(A\) has size \(n\).
This makes it twice as fast as algorithms based on QR decomposition therefor is LU decomposition usually preferred.

But there are a few drawbacks.
For one LU-decomposition can be numerically instable, which means that rounding and representation errors can be amplified through the gaussian elimination.
On the other hand is implementing the row- and column-transformations relatively complex.
The following procedure eliminates both those drawbacks

\subsection{QR-Decomposition}
To keep generality we let \(A \in \Mat_n(\mathbb{C})\) and \(x, b \in \Mat_{n1}(\mathbb{C})\) throughout this section.
QR-decomposition transforms \(A\) through row transformations into an upper triangular matrix \(R \in \Mat_{n,n}(\mathbb{C})\) in order to solve the system of equations more efficiently through backward substitution.
We represent these transformations condensed in a unitary matrix and since we are in the complex space we use the
\begin{definition}[Complex Dot Product]\label{def:complex_dot_prod}
   Given \(x \in \Mat_{1n}(\mathbb{C})\) and \(y \in \Mat_{n1}(\mathbb{C})\)
   \[\langle x, y \rangle := \sum_{i = 1}^n x_i \cdot \overline{y_i}\]
\end{definition}

With that in place we can come back to the construction of a unitary matrix \(U \in \GL_n(\mathbb{C})\) to transform \(A\) into an upper triangular matrix \(R \in \Mat_{nn}(\mathbb{C})\)
\[U \cdot A = R\]
We construct \(U\) from permutation matrices \((P_j)_{j = 1}^n \in \GL_n(\mathbb{C})\)
\[U = \prod_{j=1}^n P_j\]
where \(P_j\) annihilates the jth row of \(A\).
So after the first transformation we have
\[P_1 \cdot A = \begin{pmatrix}k & \ast & \cdots & \ast\\0 & \vdots & & \vdots\\ \vdots & \vdots & & \vdots\\ 0 & \ast & \cdots & \ast\\\end{pmatrix}\]
If the column of \(A\) already has the desired form \(a_1 = k \cdot e_1\) for some \(k \in \mathbb{C}\) we set \(P_1 = I_n\).

We see that there is a block of the matrix which still needs to be transformed into an upper triangular form while the first \(j\) columns already are.
This means that there are less transformations to be carried out, the longer the process goes on, hence the permutation matrices have the form
\[P_j = \left(\begin{array}{c|c}I_{j-1} & 0 \\ \hline 0 & \ast\\\end{array}\right)\]
and after \(l\) transformations, the matrix \(A\) looks like this
\[P_l \cdot P_{l-1} \cdot \ldots \cdot P_1 \cdot A = \begin{pmatrix}
      k_1 & \ast & \cdots & \cdots & \cdots & \ast\\
      0 & \ddots & \ddots & & & \vdots \\
      \vdots & \ddots & k_l & \ast & \cdots & \ast\\
      \vdots & & 0 & \vdots & & \vdots \\
      \vdots & & \vdots & \vdots & & \vdots \\
      0 & \cdots & 0 & \ast & \cdots & \ast \\
\end{pmatrix}\]

Since we now know how the basic process of the QR-decomposition works we have a look at what we achieved through this.
\[(U \cdot A = R \iff A = U^HR) \implies (Ax = b \iff U^HRx = b \iff Rx = Ub)\]
so we see that we can solve the original system through first computing \(U \cdot b =: y\) and then solving \(R \cdot x = y\) for \(x\).
Computationally it would be way to inefficient, calculating all the transformations and actually computing \(U\), so we will subsequently derive a method to implement the QR-decomposition.
We will calculate \(R\) and \(y\) simultaneously and then only need to solve \(R \cdot x = y\).

We always regard the successive blocks of \(A\) which are not yet in an upper triangular form (in the first step the block is the whole matrix).
We transform the blocks such that their first columns look like \(k \cdot e_1\), then together they form \(R\).

\begin{lemma}[QR Permutation Matrices]\label{lem:qr_permut_mat}
   For \(A \in \Mat_n(\mathbb{C})\) let \(a_1\) be its first column, let
   \[w := \frac{a_1 - ke_1}{\|a_1 - ke_1\|} \qquad\text{and}\qquad k := \begin{cases} -\frac{a_{11}}{|a_{11}|} \cdot \|a_1\| & a_{11} \neq 0\\ \|a_1\| & a_{11} = 0\end{cases}\]
   then holds that
   \[P = I_n - 2 ww^H  \implies P \cdot a_1 = k \cdot e_1\]
\end{lemma}
\begin{proof}
   \begin{equation*}
      \begin{split}
         \|a - ke_1\|^2 & = \langle a - ke_1, a - ke_1\rangle = \|a\|^2 + |k|^2 - \langle a, ke_1\rangle - \langle ke_1, a\rangle = \\
                          & = \|a\|^2 + |k|^2 - a_1\overline{k} - k\overline{a_1} = \|a\|^2 + \|a\|^2 - 2|a_1|\|a\| = \\
                          & = 2 \|a\|(|a_1| + \|a\|)
      \end{split}
   \end{equation*}
   from this follows that
   \[P \cdot a = (I - 2ww^H)a = ke_1 + a - ke_1 - 2 \langle a, w\rangle w = ke_1 + (a - ke_1)\underbrace{\left(1 - 2\frac{\langle a, a - ke_1\rangle}{\|a - ke_1\|^2}\right)}_{=: \delta}\]

   Now we show \(\delta = 0\) which is equivalent to \(P_1a_1 = ke_1\).
   \[\delta = 1 - 2\frac{\langle a_1, a_1 - ke_1\rangle}{\|a_1 - ke_1\|^2} = 1 - 2\frac{\langle a_1 - ke_1, a_1 - ke_1\rangle}{\|a_1 - ke_1\|^2} + 2\frac{\langle -ke_1, a_1 - ke_1\rangle}{\|a_1 - ke_1\|^2} = -1 - 2k\frac{\overline{a_{11}} - \overline{k}}{\|a_1 - ke_1\|^2} = 0\]
   That \(P\) is unitary follows since \(\|w\| = 1\) and so
   \[P^H P = (I_n - 2ww^H)(I_n - 2ww^H) = I_n - 4ww^H + 4ww^Hww^H = I_n - 4ww^H + 4ww^H = I_n\]
\end{proof}

Where we used the standard euclidean norm \(\|x\| := \sqrt{\langle x, x \rangle}\) and the following
\begin{proposition}
   Le \(A \in \GL_n(\mathbb{C})\) be a unitary matrix.
   \[\forall x \in \mathbb{C}^n: \|A \cdot x\| = \|x\|\]
\end{proposition}
\begin{proof}
   Let \(x \in \mathbb{C}^n\) be arbitrary
   \[\|A \cdot x\|^2 = \langle Ax, Ax \rangle = \left\langle \sum_{i=1}^n a_i x_i, \sum_{j=1}^n a_j x_j \right\rangle = \sum_{i,j=1}^n x_i \overline{x_i} \langle a_i, a_j \rangle = \sum_{i=1}^n |x_i|^2 = \|x\|^2\]
\end{proof}

For convenience we rewrite the statement of the lemma like so
\[P := I - \beta uu^H \quad\text{where}\quad \beta := \frac{1}{\|a_1\|(|a_{11}| + \|a_1\|)} \quad\text{and}\quad u := a_1 - ke_1\]
and the whole process can be implemented as follows

% TODO: optimized version?
\lstinputlisting[language=Matlab, caption={QR-Decomposition}]{calc_qr_decomp.m}

\begin{theorem}[QR-Decomposition Runtime Complexity]
   Runtime of QR-decomposition of \(A \in \Mat_n(\mathbb{Q})\) amounts to \(\frac{4}{3}n^3 + p(n)\) where \(\deg(p) \leq 2\).
\end{theorem}
\begin{proof}
   Calculating \(P_j\) takes
   \[\underbrace{2n}_{\|a_1\|} + \underbrace{O(1)}_{k} + \underbrace{2n(n-1)}_{v^H := \beta u^H A} + \underbrace{2n(n-1)}_{A - uv^H} = 4n^2 + \widetilde{p}(n)\]
   and since the elimination matrices \(P_j\) shrink with the process we have
   \[\sum_{i=1}^{n-1} 4(n - i + 1)^2 + \widetilde{p}(n - i + 1) = \frac{4}{3}n^3 + p(n)\]
   where we used the fact that
   \[\sum_{l=1}^n l^k = \frac{n^{k+1}}{k+1} + p_k\]
\end{proof}

\subsection{QR vs. LU -- Matrix Condition}
In this section we regard
\[A_\varepsilon := \begin{pmatrix}\varepsilon & 1\\1 & 1\end{pmatrix} \qquad\text{with}\qquad \varepsilon \in \left[0; \frac{1}{2}\right]\]
and compare the LU- and QR-decomposition, to point out in which cases QR is superior.

\paragraph{LU-decomposition} \(A_\varepsilon = L_\varepsilon \cdot U_\varepsilon\) where
\[L_\varepsilon := \begin{pmatrix}1 & 0\\l_{21} & 1\end{pmatrix} \qquad\text{and}\qquad U_\varepsilon := \begin{pmatrix}u_{11} & u_{12}\\0 & u_{22}\end{pmatrix}\]
so we see
\begin{equation*}
   \begin{split}
      & \begin{pmatrix}1 & 0\\l_{21} & 1\end{pmatrix} \cdot \begin{pmatrix}u_{11} & u_{12}\\0 & u_{22}\end{pmatrix} = \begin{pmatrix}u_{11} & u_{12}\\l_{21}u_{11} & l_{21}u_{12} + u_{22}\end{pmatrix} \overset{!}{=} \begin{pmatrix}\varepsilon & 1\\1 & 1\end{pmatrix}\\
      & \implies (u_{11} = \varepsilon,~u_{12}=1) \implies l_{21} = \frac{1}{\varepsilon} \implies u_{22} = 1 - \frac{1}{\varepsilon}
   \end{split}
\end{equation*}
So we have
\[U_\varepsilon = \begin{pmatrix}\varepsilon & 1\\0 & 1-\frac{1}{\varepsilon}\end{pmatrix} \qquad\text{and}\qquad L_\varepsilon = \begin{pmatrix}1 & 0\\ \frac{1}{3} & 1\end{pmatrix}\]

\paragraph{QR-decomposition} \(A_\varepsilon = Q_\varepsilon \cdot R_\varepsilon\) where
\[Q_\varepsilon Q_\varepsilon^T = I_2 \qquad\text{and}\qquad R_\varepsilon = \begin{pmatrix}r_{11} & r_{12}\\ 0 & r_{22}\end{pmatrix}\]
So we compute \(P\) as in \cref{lem:qr_permut_mat}
\[\|a_1\| = \sqrt{\varepsilon^2 + 1} \qquad\text{and}\qquad k = -\frac{\varepsilon}{\varepsilon} \|a_1\| = -\sqrt{\varepsilon^2 + 1}\]
\[a_1 - ke_1 = \begin{pmatrix}\varepsilon + \sqrt{\varepsilon^2 + 1}\\1\end{pmatrix} \qquad\text{and}\qquad \|a_1 - ke_1\| = \sqrt{(\varepsilon + \sqrt{\varepsilon^2 + 1})^2 + 1}\]
where we now have \(w = \frac{a_1 - ke_1}{\|a_1 - ke_1}\) which gives us
\[ww^T = \begin{pmatrix}
   \frac{(\varepsilon + \sqrt{\varepsilon^2 + 1})^2}{(\varepsilon + \sqrt{\varepsilon^2 + 1})^2 + 1} & \frac{\varepsilon + \sqrt{\varepsilon^2 + 1}}{(\varepsilon + \sqrt{\varepsilon^2 + 1})^2 + 1}\\
   \frac{\varepsilon + \sqrt{\varepsilon^2 + 1}}{(\varepsilon + \sqrt{\varepsilon^2 + 1})^2 + 1} & \frac{1}{(\varepsilon + \sqrt{\varepsilon^2 + 1})^2 + 1}
\end{pmatrix}\]
now we can compute \(P = I_2 - 2ww^T\) and so \(R = PA_\varepsilon\) and \(Q = P^T\)
\[Q_\varepsilon = \begin{pmatrix}
      \frac{2}{(\varepsilon + \sqrt{\varepsilon^2 + 1})^2 + 1}-1 & \frac{-2(\varepsilon + \sqrt{\varepsilon^2+1})}{(\varepsilon + \sqrt{\varepsilon^2 + 1})^2 + 1}\\
   \frac{-2(\varepsilon + \sqrt{\varepsilon^2+1})}{(\varepsilon + \sqrt{\varepsilon^2 + 1})^2 + 1} & 1- \frac{2}{(\varepsilon + \sqrt{\varepsilon^2 + 1})^2 + 1}
\end{pmatrix} \qquad R_\varepsilon = \begin{pmatrix}
   -\sqrt{\varepsilon^2 + 1} & \frac{-\varepsilon + 1}{\sqrt{\varepsilon^2 + 1}}\\
   0 & \frac{\varepsilon - 1}{\sqrt{\varepsilon^2 + 1}}
\end{pmatrix}\]

The \emph{condition} of a matrix indicates for a linear system \(G\varphi = r\) the influence of the perturbation of \(\widetilde{r} \approx r\) on the perturbed solution \(G\widetilde{\varphi} = \widetilde{r}\).
\begin{definition}[Matrix Condition]
   Given \(A \in \Mat_n(K)\)
   \[\mathcal{K}(A) := \|A\|_{\ast} \cdot \|A^{-1}\|_{\ast} \qquad\text{where}\qquad \|A\|_{\ast} := \max_{1 \leq i,j \leq n} |a_{ij}|\]
\end{definition}
\begin{remark}
   For \(A_\varepsilon\) we calculate its inverse
   \[\det(A_\varepsilon) = \varepsilon - 1 \quad\rightsquigarrow\quad A_\varepsilon^{-1} = \frac{1}{\det(A_\varepsilon)}\adj(A_\varepsilon) = \frac{1}{\varepsilon - 1}\begin{pmatrix}1&-1\\-1&\varepsilon\end{pmatrix} = \begin{pmatrix}\frac{1}{\varepsilon - 1}& -\frac{1}{\varepsilon - 1}\\-\frac{1}{\varepsilon - 1}& \frac{\varepsilon}{\varepsilon - 1}\end{pmatrix}\]
   and see that
   \[\mathcal{K}(A_\varepsilon) = \|A_\varepsilon\|_\ast \cdot \|A_\varepsilon^{-1}\|_\ast = 1 \cdot \left|\frac{1}{\varepsilon - 1}\right| = \frac{1}{1 - \varepsilon} \xrightarrow{\varepsilon \to 0} 1\]
   which means that the perturbation of representation and operations doesn't amplify.
\end{remark}

Now we want to examine the condition numbers of both decompositions and regard what happens when \(\varepsilon \to 0\).
\paragraph{LU-Condition}
\[\det(L_\varepsilon) = 1 \quad\rightsquigarrow\quad L_\varepsilon^{-1} = \begin{pmatrix}1&0\\-\frac{1}{\varepsilon}&1\end{pmatrix}\]
\[\mathcal{K}(L_\varepsilon) = \left|\frac{1}{\varepsilon}\right| \cdot \left|\frac{1}{\varepsilon}\right| = \frac{1}{\varepsilon} \xrightarrow{\varepsilon \to 0} \infty\]

\[\det(U_\varepsilon) = \varepsilon - 1 \quad\rightsquigarrow\quad U_\varepsilon^{-1} = \frac{1}{\varepsilon - 1} \begin{pmatrix}1-\frac{1}{\varepsilon}&-1\\0&\varepsilon\end{pmatrix} = \begin{pmatrix}\frac{1 - \frac{1}{\varepsilon}}{\varepsilon - 1} & -\frac{1}{\varepsilon - 1}\\0&\frac{\varepsilon}{\varepsilon - 1}\end{pmatrix}\]
\[\mathcal{K}(U_\varepsilon) = \underbrace{\left|1 - \frac{1}{\varepsilon}\right|}_{\xrightarrow{\varepsilon \to 0} \infty} \cdot \underbrace{\left|\frac{1 - \frac{1}{\varepsilon}}{\varepsilon - 1}\right|}_{\xrightarrow{\varepsilon \to 0} \infty} \xrightarrow{\varepsilon \to 0} \infty\]

\paragraph{QR-Condition}
\[\det(R_\varepsilon) = 1 - \varepsilon \quad\rightsquigarrow\quad R_\varepsilon^{-1} =
   \begin{pmatrix}
      -\frac{1}{\sqrt{\varepsilon^2 + 1}} & \frac{\varepsilon + 1}{\sqrt{\varepsilon^2 + 1}(1 - \varepsilon)}\\
      0 & -\frac{\sqrt{\varepsilon^2 + 1}}{1 - \varepsilon}
   \end{pmatrix}
\]
\[\mathcal{K}(R_\varepsilon) = |\sqrt{\varepsilon^2 + 1}| \cdot \left|\frac{\sqrt{\varepsilon^2 + 1}}{1 - \varepsilon}\right| = \left|\frac{\varepsilon^2 + 1}{1 - \varepsilon}\right| \xrightarrow{\varepsilon \to 0} 1\]

By definition of \(Q_\varepsilon\) holds \(Q_\varepsilon^T = Q_\varepsilon^{-1}\) and by symmetry \(Q_\varepsilon = Q_\varepsilon^T\), hence
\[\mathcal{K}(Q_\varepsilon) = \|Q_\varepsilon\|_\ast^2 \quad\rightsquigarrow\quad \lim_{\varepsilon \to 0} \mathcal{K}(Q_\varepsilon) = \lim_{\varepsilon \to 0}\left(_{i,j \in [0;n]} \frac{-2(\varepsilon + \sqrt{\varepsilon + 1})}{(\varepsilon + \sqrt{\varepsilon^2 + 1})^2 + 1}\right)^2\]
Now since
\[-2(\varepsilon + \sqrt{\varepsilon + 1}) \xrightarrow{\varepsilon \to 0} -2 \qquad\text{and}\qquad (\varepsilon + \sqrt{\varepsilon^2 + 1})^2 + 1 \xrightarrow{\varepsilon \to 0} 2\]
follows
\[\lim_{\varepsilon \to 0} \mathcal{K}(Q_\varepsilon) = |-1| \cdot |-1| = 1\]
In summary we see that even though \(A_\varepsilon\) is well-conditioned, it's LU-decomposition is not and thus is prone to error accumulation.
QR-decomposition on the other hand is not impacted by the condition of \(A_\varepsilon\) but is more complex.

\subsection{Using QR-Decomposition for Line Fitting}
% TODO: difference regression and curve fitting
Line (Curve) fitting is the process of constructing a curve, or mathematical function, that has the best fit to a series of data points.
Mathematically, this procedure can be viewed as a generalization for approximating functions through polynomials.

Let's say \(f: \mathbb{R}^k \to \mathbb{R}\) is perhaps a physical quantity which depends on \(k\) parameters.
Given are the following sampling points.
\[\Big(x^{(i)}\Big)_{i=1}^n \subset \mathbb{R}^k \quad\text{where}\quad x^{(i)} \in \Mat_{k,1}(R) \quad\text{with values}\quad (f_i)_{i=1}^n \subset \mathbb{R} \quad\text{i.e}\quad \forall i \in [1; n]: f_i = f\Big(x^{(i)}\Big)\]
Noisy sampling data causes difficulties for polynomial curve fitting, so in our scenario we assume that we have somewhat consistent sampling points.
This allows us to assume that we can construct an approximation \(\widetilde{f}\) which is \emph{affine}.

A linear function fixes the origin, whereas an affine function need not do so.
An affine function is the composition of a linear function with a translation, so while the linear part fixes the origin, the translation can map it somewhere else.
Linear functions between vector spaces preserve the vector space structure (so in particular they must fix the origin).
While affine functions don't preserve the origin, they do preserve some of the other geometry of the space, such as the collection of straight lines.
If you choose bases for vector spaces \(V\) and \(W\) of dimensions \(n\) and \(m\) respectively, and consider functions \(f: V \to W\), then \(f\) is linear if \(f(v) = Av\) for some \(A \in \Mat_{n,m}(K)\) and \(f\) is \emph{affine} if \(f(v) = Av + b\) for some matrix \(A\) and vector \(b\), where coordinate representations are used with respect to the bases chosen.

This means that the values of our approximation are given through
\[\widetilde{f}\Big(x^{(i)}\Big) = v_0 + \sum_{j=1}^k v_j \cdot x_j^{(i)} = (v_0~v_1~\cdots~v_k) \cdot \begin{pmatrix}1\\x_1^{(j)}\\x_2^{(j)}\\ \vdots \\ x_k^{(j)}\end{pmatrix}\]
where we see that in the best case scenario \(\forall i \in [1; n]: \widetilde{f}\big(x^{(i)}\big) = f_i\).
Where all sampling points lay on a single line.

From above we see matrix which corresponds to \(\widetilde{f}\) 
\[\widetilde{A} := \begin{pmatrix}
      1 & x_1^{(1)} & x_2^{(1)} & \cdots & x_k^{(1)}\\
      \vdots & x_1^{(2)} & \vdots & \vdots & \vdots \\
      \vdots & \vdots & \vdots & \vdots & \vdots & \\
      1 & x_1^{(n)} & x_2^{(n)} & \cdots & x_k^{(n)}\\
   \end{pmatrix}\]
Now we have to find the \emph{coefficient} vector \(v \in \mathbb{R}^{k+1}\) through the following system of linear equations
\[\widetilde{A} \cdot v = f\]
But for \(n > (k+1)\), this system has more equations than unknowns and has in general no solution.
So we restate our problem and say we want to find \(v\) such that \(\widetilde{A}v - b\) is minimal regarding the \emph{quadratic mean}.
This means we want to find \(v\) such that
\[F: \mathbb{R}^{k+1} \to \mathbb{R} \qquad\text{where}\qquad F(x) := \sum_{i=1}^n (\widetilde{A}x - f)_i^2\]
is minimal at \(x = v\) which implies that \(\forall x \in \mathbb{R}^{k+1}: F(v) \leq F(x)\) or differently put
\[\forall i \in [0;k]: \frac{\partial}{\partial v_i} F(v) = 0\]
The linear equations can be derived as follows.
\begin{equation*}
   \begin{split}
      \frac{\partial}{\partial v_i} F(v) & = \frac{\partial}{\partial v_i} \langle \widetilde{A}x - f, \widetilde{A}x - b\rangle = \frac{\partial}{\partial v_i} \big(\langle \widetilde{A}v, \widetilde{A}v \rangle - 2 \langle \widetilde{A}v, f\rangle + \|f\|^2\big) \overset{\text{(i)}}{=} \\
                                         & = \frac{\partial}{\partial v_i} \big(\langle Mv, v\rangle - 2 \langle \widetilde{A}v, f \rangle + \|f\|^2\big) = \frac{\partial}{\partial v_i} \left(\sum_{j,l = 0}^k M_{l,j} v_j v_l - 2 \sum_{j = 0}^k \sum_{l = 1}^n \widetilde{A}_{l,j} v_l f_l\right) \overset{\text{(ii)}}{=}\\
                                         & =  \sum_{j,l=0}^k M_{l,j} (\delta_{li} v_j + v_l \delta_{i,j}) - 2 \sum_{j=0}^k\sum_{l=1}^n \widetilde{A}_{j,l} \delta_{l,i} f_l = \sum_{j=0}^k M_{j,i} v_j + \sum_{l=0}^k M_{i,l} v_l - 2\sum_{j=1}^n \widetilde{A}_{j,i}f_j = \\
                                         & = (M^T v)_i + (Mv)_i - 2(\widetilde{A}^Tf)_i = 2 (A^TAv - A^Tf)_i \overset{!}{=} 0
   \end{split}
\end{equation*}
\begin{enumerate}[label=\roman*, align=Center]
   \item We set \(M := \widetilde{A}^T \widetilde{A} \implies M^T = \widetilde{A}^T \widetilde{A} = M\)
   \item We partially derive after \(v_i\) which lets all other entries become 0 since they are fixed as constants.
\end{enumerate}
This means \(v\) is characterized through
\[Mv = \widetilde{A}^T\widetilde{A}v = \widetilde{A}^T f\]
which is a system of linear equations in \emph{normal form}.

\begin{proposition}[Line Fitting Problem has a Solution]
   The line fitting problem --- finding \(v \in \mathbb{R}^{k+1}\) such that
   \[\|\widetilde{A}v - f\|^2 = \min_{x^{(i)} \in \mathbb{R}^{k+1}} \|\widetilde{A}x^{(i)} - f\|^2 =: \min_{x^{(i)} \in \mathbb{R}^{k+1}} F(x^{(i)})\]
   holds, has at least one solution.
\end{proposition}
\begin{proof}
   For an arbitrary \(f \in \mathbb{R}^n\) has the normal form a solution.
   We have
   \[\mathbb{R}^n = \im(A) \oplus \im(A)^\perp\]
   where \(\im(A) = \{Ax \mid x \in \mathbb{R}^{k+1}\}\) and
   \begin{equation*}
      \begin{split}
         \im(A)^\perp & = \{v \in \mathbb{R}^n \mid \forall w \in \im(A): \langle v, w \rangle = 0\} = \{v \in \mathbb{R}^n \mid \forall x \in \mathbb{R}^{k+1}: \langle v, Ax \rangle = 0\} = \\
                      & = \{v \in \mathbb{R}^n \mid \forall x \in \mathbb{R}^{k+1}: \langle A^Tv, x \rangle = 0\} = \{v \in \mathbb{R}^n \mid A^Tv = 0\}
      \end{split}
   \end{equation*}
   Now we show that any \(f\) can be uniquely decomposed into \(\im(A) \ni f^\parallel + f^\perp \in \im(A)^\perp\).
   \[f^\parallel \in \im(A) \implies \exists x^\parallel \in \mathbb{R}^{k+1}: f^\parallel = A \cdot x^\parallel\]
   \[f^\perp \in \im(A)^\perp \implies A^T \cdot f^\perp = 0\]
   so we have
   \[A^TAx^\parallel = A^T f^\parallel = A^T f^\parallel + A^T f^\perp = A^T(f^\parallel + f^\perp) = A^T f\]
\end{proof}

\begin{proposition}[Residual is Unique]
   The image of multiple solutions to the normal form are equal.
   This means that the residual \(r := f - Av\) of a solution is unique.
\end{proposition}
\begin{proof}
   From the residual we have
   \[r = f - Av \iff f = Av + r \iff f = Av + (b - Av)\]
   where \(Av \in \im(A)\) and \(\forall w \in \mathbb{R}^{k+1}\) holds
   \[\langle r, Aw \rangle = \langle f - Ax, Aw \rangle = \langle A^Tf - A^TAx, w \rangle = 0 \implies r \in \im(A)^\perp\]
   Since the decomposition of \(f\) is unique, \(Ax\) must be the same for all solutions with \(f^\parallel\).
\end{proof}

\begin{proposition}[Line Fitting \(\iff\) Normal Form]
   Solving the normal form \(A^TAv = A^T f\) is equivalent to solving a line fitting problem.
\end{proposition}
\begin{proof}
   First we how that a best line fit is a solution to the normal form.
   For any \(w \in \mathbb{R}^{k+1}\) holds
   \begin{equation*}
      \begin{split}
         \|f - Aw\|^2 & = \|f - Ax + Ax - Aw\|^2 = \|f - Ax\|^2 + 2 \langle r, A(x-w)\rangle + \|A(x-w)\|^2 = \\
                      & = \|f - Ax\|^2 + \|A(x-w)\|^2 \geq \|f - Ax\|^2
      \end{split}
   \end{equation*}
   this means that every solution \(x\) of the normal form minimizes the functional \(F\).

   Now we show that a solution of the normal form is a best line fit.
   \begin{equation*}
      \begin{split}
         \|Ax - f\|^2 & = \|Ax - f -Av + Av\| = \|Ax - Av\|^2 + \|Av - f\|^2 + 2 \langle A(x - v), Av - f\rangle \geq \\
                      & \geq \|Av - f\|^2 + 2 \langle x-v, A^TAv - A^Tf\rangle = \|Av-f\|^2
      \end{split}
   \end{equation*}
\end{proof}

% TODO: elaborate
\begin{example}
   Suppose we have \(f(x) = a_0 + a_1x + \frac{\log(x^{a_2})}{x} + a_3 \sin(x)\) with \((b_i)_{i=0}^{14}\) measurements.
   Then we can determine the unknown parameters \((a_i)_{i=0}^3\)
   \[\begin{pmatrix}
         1 & 1 & \frac{\log(1)}{1} & \sin(1)\\
         1 & 2 & \frac{\log(2)}{2} & \sin(2)\\
         \vdots & \vdots & \vdots & \vdots\\
         1 & 15 & \frac{\log(15)}{15} & \sin(15)\end{pmatrix} \cdot \begin{pmatrix}a_0\\a_1\\a_2\\a_3\end{pmatrix} = \begin{pmatrix}b_0\\b_1\\\vdots\\b_14\end{pmatrix}\]
\end{example}

Since the vector \(a\) is characterized by \(A^TAa = A^Tb\) we can use QR-Decomposition as follows
\begin{lstlisting}[language=Matlab, caption={Line Fitting with QR-Decomposition}]
   [R, y] = calc_qr_decomp(A.'*A, A.'*b, n)
   a = solve_upper_tria_sys(R, y);\end{lstlisting}

\section{Solving Systems of non-linear Equations}
We regard the system of equations more abstractly, meaning we regad non-linear equations as the problem of finding the roots of a function \(f: \mathbb{R}^d \to \mathbb{R}^d\), i.e. finding \(x\) such that \(f(x) = 0\).
We introduce this section using \(d = 1\) and \(f \in C^0(I)\).

Depending on whether we need just any root, a specific one or all of them, the method of computation varies.
Since even some of the easiest non-linear equations don't have rational solutions, we can't solve them exactly on a computer.
So to approximate \(x\) we construct sequences \((x_i)_{i \in \mathbb{N}_0}\) which converge to \(x\).
The construction of the squences is aborted after finitely many steps and an error estimation is used to determine the accuracy of the solution.

The efficiency of such a method is given through the convergence behaviour of the sequence.
\begin{definition}[Linear Convergence]
   Let \((\varepsilon_i)_{i=0}^n\) be a sequence for which holds that
   \[\lim_{i \to \infty} \frac{\varepsilon_{i+1}}{\varepsilon_i} = C \in (0; 1)\]
   A sequence \((x_i)_{i=0}^n\) of roots converges at least linear to \(x\) iff
   \[\forall i \in [0;n]: |x - x_i| \leq \varepsilon_i\]
\end{definition}
\begin{definition}[Convergence of Order \(p\)]\label{def:conv_ord_p}
   Let \(p > 1\) \((\varepsilon_i)_{i=0}^n\) be a sequence for which holds that
   \[\lim_{i \to \infty} \frac{\varepsilon_{i+1}}{\varepsilon_i^p} = C > 0\]
   A sequence \((x_i)_{i=0}^n\) of roots converges of order \(p\) iff
   \[\forall i \in [0;n]: |x - x_i| \leq \varepsilon_i\]
\end{definition}
\begin{remark}
   The definition doesn't differ for \(f: \mathbb{R}^d \to \mathbb{R}^d\), except that the absolute value is replaced with an appropriate norm.
\end{remark}
\begin{remark}
   If we instead require
   \[\forall i \in [0;n]: \frac{\varepsilon_{i+1}}{\varepsilon_i^p} \leq C\]
   for the sequence of error boundaries, we see inductively
   \[\varepsilon_i \leq C \cdot \varepsilon_{i-1} \leq C^2 \cdot \varepsilon_{i-2} \leq \ldots \leq C^i \cdot \varepsilon_0\]
   i.e.
   \[\varepsilon_i \leq C^{s_{i,p}} \cdot \varepsilon_0^{p^i} \qquad\text{where}\qquad s_{i,p} := \sum_{l = 0}^{i-1} p^i = \begin{cases}i & p = 1\\ \frac{p^i - 1}{p - 1} & p \neq 1\end{cases}\]
   if we set \(p = 1\).
   This means that the error boundaries \(\varepsilon_i\) form a zero-sequences for linear convergence.

   For \(p > 1\) we see
   \begin{equation*}
      \begin{split}
         \varepsilon_{i+1} \leq C \cdot \varepsilon_i^p \implies \varepsilon_i \leq C \varepsilon_{i-1}^p & \leq C(C \varepsilon_{i-2}^p)^p = C^{1+p} \varepsilon_{i-2}^{p^2} \leq C^{1+p}(C \varepsilon_{i-3}^p)^{p^2} = C^{1+p+p^2} \varepsilon_{i-3}^{p^3} = \overset{\ast}{\ldots} \\
                                                                                                          & \implies \varepsilon_i \leq C^{-\frac{1}{p-1}} \left(C^\frac{1}{p-1} \varepsilon_0\right)^{p^i}
      \end{split}
   \end{equation*}
   where we used \(\frac{p^i - 1}{p-1} = -\frac{1}{p-1} + \frac{p^i}{p-1}\) at \(\ast\).

   This way we assure that the error boundaries converge if the error of the starting value suffices
   \[0 \leq C^{-\frac{1}{p-1}} \cdot \left(C^{\frac{1}{p-1}} \cdot \varepsilon_0\right)^{p^i}\]
\end{remark}

\subsection{Bisection}
In the bisection method we start with an initial finite interval which contains at least one root.
So for \(f \in C^0(I)\) holds \(f(a) < 0 \land f(b) > 0\).

\begin{proposition}[Bisection Convergence]
   The convergence \(x_i \to x\) of the algorithm above is linear with \(C = \frac{1}{2}\).
\end{proposition}
\begin{proof}
   After \(n\) steps of this algorithm the root is contained in \((a_n, b_n)\).
   This means that the root can only be \(\frac{b_n - a_n}{2}\) away of the center \(x_n\).
   This leads us to the following error estimation
   \[|x - x_n| \leq \frac{b_n - a_n}{2} = 2^{-n}(b - a) = \frac{b-a}{2^n} =:\varepsilon_n\]
   where we see that
   \[\lim_{n \to \infty} \frac{\varepsilon_{n+1}}{\varepsilon_n} = \lim_{n \to \infty} \frac{2^{-n-1}}{2^{-n}} = \frac{1}{2}\]
\end{proof}
\begin{remark}
   If the root should be calculated up to a precision of \(\varepsilon > 0\) we can determine how many steps we need to make.
   \[|x - x_n| \leq \varepsilon \implies 2^{-n} (b-a) \leq \varepsilon \implies n = \left\lceil \log_2\left(\frac{b-a}{\varepsilon}\right)\right\rceil\]
\end{remark}

\lstinputlisting[language=Matlab, caption={Bisection}]{bisection.m}

\begin{example}
   We compute how many steps bisection needs to determine an intervall \([a; b] \subset \big[\frac{1}{2}; 1\big]\) such that \(b - a \leq \varepsilon\).

   Since bisection halfs the intervall length with every step, we know that
   \[b - a \leq \varepsilon \implies \frac{1}{2^n}\left(1 - \frac{1}{2}\right) \leq \varepsilon\]
   and so
   \[\frac{1}{2^n}\left(1 - \frac{1}{2}\right) \leq \varepsilon \implies \frac{1}{2^{n+1}} \leq \varepsilon \implies 2^{n+1} \leq \frac{1}{\varepsilon} \implies n+1 = \left\lceil \log_2\left(\frac{1}{\varepsilon}\right)\right\rceil \implies n = \left\lceil \log_2\left(\frac{1}{\varepsilon}\right)\right\rceil - 1\]
\end{example}

Conceptually bisection is limited to scalar functions, i.e. a function with a one-dimensional image.
Another drawback is that it is comparatively slow.
Also we only find one root and can't even tell which one.
Optimally we want a precedure where we can decide which root of \(f\) we want to approximate.

\subsection{Newton's Method}
In numerical analysis, Newton's method, is a root-finding algorithm which produces successively better approximations to the roots (or zeroes) of a real-valued function.
The idea is to start with an initial guess \(x_0\) which is reasonably close to the true root.
Then we approximate the function by its tangent line, and finally to compute the x-intercept of this tangent line.
This x-intercept will typically be a better approximation to the original function's root than the first guess, and the method can be iterated.

\begin{center}
   \input{drawings/newton_method.tex}
\end{center}

More formally, suppose \(f: I \to \mathbb{R}\) is differentiable and we have some current approximation \(x_n\).
Then we can derive the formula for a better approximation, \(x_{n+1}\) by referring to the diagram above.
The equation of the tangent line to the curve \(y = f(x)\) at \(x = x_n\) is
\[y = f'(x_n) (x - x_n) + f(x_n)\]
The x-intercept of this line (the value of \(x\) which makes \(y = 0\)) is then used as the next approximation to the root \(x_{n+1}\).
\[0 = f'(x_n) (x_{n+1} - x_n) + f(x_n) \implies x_{n+1} = x_n - \frac{f(x_n)}{f'(x_n)}\]
We start the process with some arbitrary initial value \(x_0\).
The closer to the zero, the better.
But, in the absence of any intuition about where the zero might lie, a ``guess and check''  method might narrow the possibilities to a reasonably small interval by appealing to the intermediate value theorem.

\lstinputlisting[language=Matlab, caption={Newton Method}]{newton_method.m}

The method will usually converge, provided this initial guess is close enough to the unknown zero, and that \(f'(x_0) \neq 0\).
Furthermore, for a zero of multiplicity 1, the convergence is at least quadratic in a neighbourhood of the zero, which intuitively means that the number of correct digits roughly doubles in every step.

If we regard Newton's method as local, linear approximation of \(f\), we can use it directly for solving systems of equations.
Suppose \(f: \mathbb{R}^n \to \mathbb{R}^n\) is differentiable.
We assume the \emph{Jacobi matrix} and it's inverse exist in every \(x\).
Then there holds for some \(x_n\) and \(x \in \mathcal{U}(x_n)\)
\[f(x) \approx f(x_n) + J(x_n)(x - x_n) \overset{!}{=} 0\]
To solve for \(x\) we transform the problem into a new iteration
\[x = x_n - J^{-1}(x_n) \cdot f(x_n)\]
and thus have an iteration-method for determining roots of non-linear vectorial equations.

\begin{example}
   To calculate \(\sqrt{a}\) of some \(a > 0\) we regard
   \[f(x) = x^2 - a\]
   and our goal is to solve the equation \(f(x) = 0\).
   So from above we see
   \[x_{n+1} = x_n - \frac{x_n^2 - a}{2x_n} = \frac{1}{2}\left(x_n + \frac{a}{x_n}\right) \qquad\text{for}~n = 0, 1, 2, \ldots\]
\end{example}

\begin{theorem}[Newton's Method Convergence]\label{thm:newt_meth_conv}
   Let \(x\) be a simple root of \(f \in C^2(I_\varepsilon)\), where \(I_\varepsilon := \{y \in \mathbb{R} \mid~|y - x| \leq \varepsilon\}\) is a neighbourhood around this root.
   If for \(\varepsilon > 0\) holds that
   \[2 \varepsilon \cdot M(\varepsilon) < 1 \qquad\text{where}\qquad M(\varepsilon) := \max_{s, t \in I_\varepsilon} \left|\frac{f''(s)}{2 \cdot f'(t)}\right|\]
   then is for any starting value \(x_0\) Newton's method well-defined and converges of order 2 to \(x\).
\end{theorem}
\begin{proof}
   We regard the taylor approximation of \(f\) in its root \(x_\ast\)
   \[f(x) = f(x_\ast) + f'(x_\ast)(x - x_\ast) + \frac{f''(\xi)}{2!}(x - x_\ast)^2 \quad\text{for some}~\xi \in [x_\ast; x]\]
   hence \(x \in I_\varepsilon \implies \xi \in I_\varepsilon\).

   First we show that \(x\) is the only root of \(f\) in \(I_\varepsilon\) by contradiction.
   So suppose \(x' \neq x\).
   From this an the fact that \(x\) is a simple root follows that
   \[f(x) = f'(x_\ast)(x - x_\ast)\underbrace{\left(1 + \frac{f''(\xi)}{2 \cdot f'(x_\ast)}(x - x_\ast)\right)}_{:= (i)}\]
   Now since
   \[\left|\frac{f''(\xi)}{2 \cdot f'(x_\ast)}(x - x_\ast)\right| \leq \varepsilon \cdot M(\varepsilon) \leq 2 \varepsilon M(\varepsilon) < 1 \implies (i) \neq 0\]
   hence \(f(x) \neq 0\), which is a contradiction.

   Now we prove, that all iterations are in \(I_\varepsilon\) and differ (otherwise the iteration would halt at some point).
   % TODO: finish
\end{proof}
\begin{remark}
   This theorem is of little practical value since the definition of the neighbourhood already requires the knowledge of the root.

   More commonly is the situation where you can determine an intervall
   \[I_\delta(a) := [a - \delta; a + \delta]\]
   which contains the root \(x\) for certain (see below for an example using bisection).
   Hence the goeal here is to compute \(\delta\) such that Newton's method converges for all values in \(I_\delta\).
\end{remark}

\begin{corollary}
   Let \(I_\delta(a) := [a - \delta; a + \delta]\) and suppose for \(\delta\) holds
   \[4\delta \cdot \max_{s,t \in I_{3\delta}(a)} \left|\frac{f''(s)}{2 \cdot f'(t)}\right| < 1\]
   Furthermore assume that \(x\) is the only root in \(I_{3\delta}(a)\), then converges Newton's method for any starting value \(x_0 \in I_\delta(a)\).
\end{corollary}
\begin{proof}
   Let \(x \in I_\delta(a)\) be the root and \(I_\varepsilon\) defined as in \cref{thm:newt_meth_conv}.
   It holds that
   \[I_\delta(a) \subset I_\varepsilon \subset I_{3\delta}(a) \qquad\text{where}~\varepsilon := 2\delta\]
   Then follows
   \[4\delta \cdot \max_{s,t \in I_{3\delta}(a)} \left|\frac{f''(s)}{2 \cdot f'(t)}\right| < 1 \implies 2 \varepsilon \cdot M(\varepsilon) < 1\]
   and the inclusion on the left-hand side above show the claim.
\end{proof}

\begin{example}
   Let \(f(x) := e^{x^2} - 2\) and \(I := \big[\frac{1}{2}; 1\big]\).
   We want to use the corollary in order to determine the intervall, such that the Newton method converges for any starting value.

   So first off, we compute
   \[f'(x) = \frac{d}{dx} \exp(x^2) - \frac{d}{dx} 2 = \exp(x^2) \cdot \frac{d}{dx}(x^2) = \exp(x^2) \cdot 2x\]
   \begin{equation*}
      \begin{split}
         f''(x) & = \frac{d}{dx}(\exp(x^2) \cdot 2x) = 2 \cdot \frac{d}{dx} (\exp(x^2) \cdot x) = 2 \left(\frac{d}{dx} (\exp(x^2)) \cdot x + \exp(x^2) \cdot \frac{d}{dx}(x)\right) = \\
                & = 2\big((\exp(x^2) \cdot 2x) \cdot x + \exp(x^2)\big) = 2\exp\big(x^2(2x^2 + 1)\big)
      \end{split}
   \end{equation*}

   \begin{equation*}
      \begin{split}
         f'''(x) & = \frac{d}{dx} \big(2\exp(x^2) \cdot (2x^2 + 1)\big) = 2 \frac{d}{dx}\big(\exp(x^2)\cdot (2x^2 + 1)\big) = \\
                 & = 2 \left(\frac{d}{dx}\big(\exp(x^2)\big)\cdot(2x^2 + 1) + \exp(x^2) \cdot \frac{d}{dx}(2x^2 + 1)\right) = \\
                 & = 2\big(\exp(x^2) \cdot 2x \cdot (2x^2 + 1) + \exp(x^2) \cdot 4x\big) = 2 \exp(x^2)(4x^3 + 6x) = 4\exp(x^2)(2x^3 + 3x)
      \end{split}
   \end{equation*}

   Now since \(\forall x \in I: f''(x) > 0\) follows that \(f'(x)\) is increasing and since \(f'\left(\frac{1}{2}\right) = \exp\left(\left(\frac{1}{2}\right)^2\right)\cdot 2 \cdot \frac{1}{2} = e^{\frac{1}{4}}\)
   we have the lower bound
   \[\forall x \in I: e^{\frac{1}{4}} \leq f'(x)\]

   Analogously, since \(\forall x \in I: f'''(x) > 0\) follows that \(f''(x)\) is increasing and since
   \[f''\left(\frac{1}{2}\right) = 2 \exp\left(\left(\frac{1}{2}\right))^2\right) \cdot \left(2 \left(\frac{1}{2}\right)^2 + 1\right) = 3 \exp\left(\frac{1}{4}\right)\]
   \[f''(1) = 2 \exp(1) \cdot (2 \cdot 1^2 + 1) = 6 \exp(1)\]
   we have to lower and upper bounds
   \[\forall x \in I: 3e^{\frac{1}{4}} \leq f''(x) \leq 6e\]

   Next we see
   \[\exp(x^2) - 2 = 0 \implies \exp(x^2) = 2 \implies x^2 = \log(2) \implies x = \pm \sqrt{\log(2)}\]
   but since we regard \(f\) over \(\big[\frac{1}{2}; 1\big]\), \(x = \sqrt{\log(2)}\) is the only root.

   From the corollary we know that the method converges \(\forall x_0 \in I_\delta(x)\) where \(I_\delta(x) := [x - \delta; x + \delta]\) iff for \(\delta\) holds that
   \[4\delta \cdot \max_{s,t \in I_{3\delta}(a)} \left|\frac{f''(s)}{2 \cdot f'(t)}\right| < 1\]
   Hence we can use the bounds we computed above in order to determine \(\delta\)
   \[4 \delta \cdot \frac{6e}{2e^\frac{1}{4}} < 1 \implies 4 \delta < \frac{2 e^\frac{1}{4}}{6e} \implies 4 \delta < \frac{e^\frac{5}{4}}{3} \implies \delta < \frac{e^\frac{5}{4}}{12}\]
   So by setting \(\delta = \frac{e}{12}\) and because we know from above that \(x = \sqrt{\log(2)}\) is the only root in \(I\) we now know that Newton's method converges for all starting values in \(\left[\sqrt{\log(2)} - \frac{e}{12}; \sqrt{\log(2)} + \frac{e}{12}\right]\).
\end{example}

\lstinputlisting[language=Matlab, caption={Determine Interval for Newton's Method}]{comp_start_intervall_newton_meth.m}

\subsubsection{Horner's Scheme}
Horner's scheme refers to a polynomial evaluation method expressed by
\[p(x) = \sum_{i=0}^{n} a_i x^i = a_0 + x\bigg(a_1 + x\Big(a_2 + x\big(a_3 + \ldots + x(a_{n-1} + xa_n) \ldots\big)\Big)\bigg)\]
This allows evaluation of a polynomial of degree \(n\) with only \(n\) multiplications and \(n\) additions.
This is optimal, since there are polynomials of degree \(n\) that cannot be evaluated with fewer arithmetic operations.

Suppose wish to evaluate the \(p\) at a specific value \(x_0\).
To accomplish this, we define a new sequence of constants as follows:
\[b_n := a_n \qquad b_{n-1} := a_{n-1} + b_nx_0 \qquad b_{n-2} := a_{n-2} + b_{n-1} x_0 \qquad \ldots \qquad b_0 := a_0 + b_1 x_0\]
Then is \(b_0\) the value of \(p(x_0)\).
To see why this works, regard the form above.
By iteratively substituting the \(b_i\) into the expression
\begin{equation*}
   \begin{split}
      p(x_0) & = a_0 + x_0\Big(a_1 + x_0\big(a_2 + \ldots + x_0(a_{n-1} + b_nx_0)\ldots\big)\Big) = \\
             & = a_0 + x_0\big(a_1 + x_0(a_2 + \ldots + x_0b_{n-1})\big) = \\
             & \vdots \\
             & = a_0 + x_0 b_1 = \\
             & = b_0
   \end{split}
\end{equation*}

Now if \(x_0\) is a root of \(p_n\), Horner's scheme defines the rule to compute the coefficients of \(p_{n-1} := \frac{p_n(x)}{(x-x_0)}\).
This way we can use Newton's method to utilize Horner's scheme to compoute the roots of \(p_n\) iteratively.
\begin{enumerate}
   \item Compute a root \(x_0\) of \(p_n\) with Newton's method.
   \item Compute \(p_{n-1}\) and repeat.
\end{enumerate}

\lstinputlisting[language=Matlab, caption={Horner's Scheme}]{horners_scheme.m}

\subsection{Fixed-Point Iteration}
Fixed-point iteration is a method of computing fixed points of iterated functions.
We look at this method because many non-linear problems can be written naturally as a fixed-point equation
\[\varphi(x) = x\]
where in general \(\varphi: \mathbb{R}^d \to \mathbb{R}^d\).

A \emph{fixed-point} of a map \(f: X \to Y\) is \(x \in X\) iff \(f(x) = x\).
An \emph{iterated function} \(X \to X\) is obtained by composing another function \(f: X \to X\) with itself a certain number of times.
The process of repeatedly applying a function to itself is called \emph{iteration}.

Now we use the recursive formula of the newton method to define such an iterated function
\[\varphi(x) := x - \frac{f(x)}{f'(x)}\]
where the fixpoints \(x = \varphi(x)\) will turn out as the solutions \(f(x) = 0\) given that \(f'(x) \neq 0\).
With \(\varphi\) we can define the newton-method as iteration
\[x_{n+1} = \varphi(x_n)\]
This gives us a sequence whose limit --- given it converges --- is a fixpoint, if \(\varphi\) is continuous
\[\alpha := \lim_{n \to \infty} x_n = \lim_{n \to \infty} \varphi(x_n) = \varphi\left(\lim_{n \to \infty} x_n\right) = \varphi(\alpha)\]

For a convergent fixpoint iteration we can determine it's order of convergence pretty easily.
Let \(x = \lim_{n \to \infty} x_n\).
We assume that \(\varphi\) is sufficiently differentiable around \(x\).
Now we define \(p \in \mathbb{Z}\) such that
\[\varphi^{(m)}(x) = 0 \quad\text{for}~m \in [1; p-1] \qquad\text{and}\qquad \varphi^{(p)}(x) \neq 0\]
With a Taylor expansion around the fixpoint holds
\begin{equation*}
   \begin{split}
      \varphi(x_n) & = \varphi(x) + \sum_{m=1}^{p-1} \frac{1}{m!} \varphi^{(m)}(x) (x_n - x)^m + \frac{(x_n - x)^p}{p!} \cdot \varphi^{(p)}(\xi_n)\\
                   & = \varphi(x) + \frac{(x_n - x)^p}{p!} \cdot \varphi^{(p)}(\xi_n)
   \end{split}
\end{equation*}
with \(\xi_n \in [x; x_n]\).
If we now set \(x_{n+1} = \varphi(x_n)\) and use that \(\varphi(x) = x\) follows
\[\frac{1}{p!} \varphi^{(p)}(\xi_n) = \frac{x_{n+1} - x}{(x_n - x)^p}\]
Here we assumed that \(x_{n+1}\) and \(x_n\) converge to \(x\).
Since \(\xi_n \in [x; x_n]\) follows \(\xi_n \xrightarrow{n \to \infty} x\).

Assuming \(\varphi^{(p)}\) is continuous in \(x\) and since \(\varphi^{(p)}(x) \neq 0\) follows
\[0 \neq \frac{1}{p!}\varphi^{(p)}(x) = \lim_{n \to \infty} \frac{x_{n+1} - x}{(x_n - x)^p}\]
which means that the convergence is of order \(p\) and for the asymptotic error constant \(C\) in \cref{def:conv_ord_p} holds
\[C = \frac{1}{p!} \cdot \varphi^{(p)}(x)\]
from which we get the following

\begin{theorem}[Fixpoint-Iteration Convergence]
   Let \(x\) be a fixpoint of \(\varphi \in C^p(I_\varepsilon)\), where \(I_\varepsilon := \{y \in \mathbb{R} \mid~|y - x| \leq \varepsilon\}\) is a neighbourhood and \(p\) such that \(\varphi^{(m)}(x) = 0\) for \(m \in [1; p-1]\).
   If
   \[M(\varepsilon) = \max_{t \in I_\varepsilon} |\varphi'(t)| < 1\]
   then converges the fixpoint-iteration to \(x\) for every starting value \(x_0 \in I_\varepsilon\).
   Also the convergence is of order \(p\) and for the asymptotic error constant \(C\) in \cref{def:conv_ord_p} holds
   \[C = \frac{1}{p!} \cdot \varphi^{(p)}(x)\]
\end{theorem}
\begin{proof}
   Left out.
\end{proof}

% TODO: reverse engineer fixpoint_iter
\lstinputlisting[language=Matlab, caption={Fixpoint Iteration}]{fixpoint_iteration.m}

\section{Iterative Methods for Systems of Equations}
In the context of systems of equations, iterative methods construct an initial value \(x^{(0)}\) (through solving an \emph{initial value problem}) and then iteratively compute a sequence of vectors \((x^{(i)})_{i=0}^n\) which converges to the unknown \(x\) of the system
\[Ax = b\]
In many cases it can be shown that the rate of convergence is (almost) independent of the starting value.
Furthermore, in many cases the choice \(x^{(0)} = 0\) results in satisfying outcomes, thus we will focus on the construction of iterative methods.

\subsection{Real-World Application}
Iterative methods are only applicable to a smaller set of matrices than direct methods.
As an example we regard Poisson's equation, which is used to model electrical fields and belongs to the class of \emph{boundary value problems}.
We will approximate the equation using a \emph{finite difference method}, which will result in a system of linear equations represented by an imensly big, sparse matrix.
Then we will introduce four methods for solving these kind of systems iteratively and finally we see how we can assure they actually converge.

\subsubsection{Boundary Value Problems}
A boundary value problem is a differential equation together with a set of additional constraints, called the \emph{boundary conditions}.
Boundary value problems are an important class of problems, especially in multiple branches of physics.
For example many time-independent physical problems can be described by \emph{Laplace's equation}, which is a second-order partial differential equation.
\begin{example}
   To describe the electric potential \(u\) in an infinitesimal thin conductor \(\Omega = (a, b)\) created by an outside electrical field \(f\) we can use Laplace's equation
   \[\forall x \in \Omega: -u''(x) = f(x)\]
   With the boundary conditions
   \[u(a) = g(a) \quad\text{and}\quad u(b) = g(b)\]
   where \(g: \Gamma \to \mathbb{R}\) is a function on the two endpoints --- the boundary --- \(\Gamma := \{a, b\}\) of \(\Omega\).
   If we find some \(u\) such that
   \[\big(\forall x \in \Omega: -u''(x) = f(x)\big) \land \big(\forall x \in \Gamma: u(x) = g(x)\big)\]
   then \(u\) describes the electric potential in the conductor \(\Omega\).
\end{example}
However in many cases the relevant physical quantity is not the potential but rather the produced field over (in our case) a two-dimensional conductor \(\Omega \subset \mathbb{R}^2\), in which case we have the second derivative over the position variables \((x_1, x_2)\).
Which brings us to the multidimensional generalization of Laplace's equation, namely \emph{Poisson's equation}
\[\forall x \in \Omega: -\Delta u(x) = f(x)\]
Where we have
\begin{definition}[Laplace Operator]
   Given \(C^2(I) \ni f: \mathbb{R}^n \to \mathbb{R}\) and \(x \in \mathbb{R}^n\)
   \[\Delta f(x) := \sum_{i = 1}^n \frac{\partial^2}{\partial x_i^2} f(x)\]
\end{definition}
as well as for the interval-edge \(\Gamma := \partial \Omega\) we have \(g: \Gamma \to \mathbb{R}\) to formulate the boundary conditions
\[\forall x \in \Gamma: u(x) = g(x)\]

In contrast to the one-dimensional problem in the example above, the solution to Poisson's equation can't be given exactly.
Hence we need to use a method of numerical discretisation to approximate a solution.

\subsubsection{Finite Difference Method}
% TODO: difference quotient of numerics to derivative definition in analysis
Finite-difference methods (FDM) are numerical methods for solving differential equations by approximating them with \emph{difference quotients}.
A difference quotient describes the ratio of how fast \(f(x)\) changes when \(x\) is changed and thus are used to define the derivative in analysis.
There are a couple of variants, but we will use the symmetric derivative as it provides a better numerical approximation than other definitions.
\begin{definition}[Symmetric Derivative]
   We can write the derivative of \(f \in C^1(I)\) as
   \[\lim_{h \to 0} \frac{f(x + h) - f(x - h)}{2h}\]
\end{definition}
So FDMs are discretization methods, which convert an ordinary, or partial differential equation into a system of linear equations, which can then be solved by matrix algebra techniques.

In the next step we do exactly this by first using the symmetric difference to approximate the derivative of our unknown function \(u\) (the electric field) in \(x\).
\[u'(x) \approx \frac{u(x + h) - u(x - h)}{2h} = \frac{u\big(x + \frac{h}{2}\big) - u\big(x - \frac{h}{2}\big)}{h}\]
As we've seen in \cref{sec:num_diff}, we can approximate higher order derivatives through successive application of the first order approximation.
\begin{equation*}
   \begin{split}
      u''(x) & \approx \frac{u'\left(x + \frac{h}{2}\right) - u'\left(x - \frac{h}{2}\right)}{h} \approx \frac{u(x + h) - u(x)}{h^2} - \frac{u(x) - u(x - h)}{h^2} = \\
             & = \frac{1}{h^2}u(x + h) - \frac{2}{h^2}u(x) + \frac{1}{h^2}u(x - h)
   \end{split}
\end{equation*}
Now we apply this approximation to the Laplace operator.
\begin{equation*}
   \begin{split}
      -\Delta u(x) \approx & -\left(\frac{1}{h^2}u(x + he_1) - \frac{2}{h^2}u(x) + \frac{1}{h^2} u(x - he_1)\right) - \\
                           & - \left(\frac{1}{h^2} u(x + he_2) - \frac{2}{h^2} u(x) + \frac{1}{h^2}u(x-he_2) \right) = \\
                           & = \frac{1}{h^2} (-1)u(x+he_1) + (-1)u(x-he_1) + 4u(x) + (-1)u(x+he_2) + (-1)u(x+he_2)
   \end{split}
\end{equation*}

Since this linear combination is really clunky to write, we introduce a notation from which we can also tell how the exact term should look like.
We envision the pattern of the inputs of \(u\) on a cartesian coordinate system with \(x\) naturally at the center.
\[\begin{pmatrix} & x + he_2 & \\ x - he_1 & x & x + he_1 \\ & x - he_2 & \\\end{pmatrix}\]
Now we write the coefficients of the five points \(u(x)\), \(u(x \pm he_1)\) and \(u(x \pm he_2)\) of the linear combination above into a matrix with each input of \(u\) corresponding to the image above.
This way we get the following \emph{5-point-star notation}
\[L_x(u) := \frac{1}{h^2} \begin{pmatrix} & -1 & \\-1 & 4 & -1\\ & -1 & \end{pmatrix} u(x)\]

Now we can substitute the Lagrange operator \(\Delta u\) in Poisson's equation with our approximation \(L_x(u)\).

\subsubsection{Approximating Poisson's Equation}
Since we can't possibly determine the whole electric field \(u\) we will approximate it by only determining it in the points of a lattice.
In every lattice point we approximate Poisson's equation using \(L_x(u)\) to approximate Laplace's operator.
In the end we will write down the resulting equations exemplarily for a few key lattice points which gives enough insight to see how the matrix form of the resulting linear equations comes into place.

To keep things simple we regard our conductor as the unit square \(\Omega := [0; 1]^2\), since the following methods and results can be directly applied to arbitrary rectangular delimited areas.
We define a lattice of \(n^2\) points
\[\Theta := \{x_{ij} := (ih, jh)^T \mid i, j \in [0;n+1]\}\]
through the increment \(h := \frac{1}{n+1}\).

\begin{center}
   \input{drawings/poisson-lattice.tex}
\end{center}

The value of \(u\) in the lattice points on the boundary \(\Gamma\) (in red) are given from the boundary problem.
\hfsetfillcolor{red}
\hfsetbordercolor{red}
\begin{equation*}
   \tikzmarkin{a} \forall x \in \Gamma: u(x) = g(x) \tikzmarkend{a}
\end{equation*}
hence we only need to approximate \(u\) in the blue points without the boundary \(\Theta_0 := \Theta \setminus \Gamma\), so this gives us the following approximated Poisson equation.
\[\forall x \in \Theta_0: L_x(u) = f(x)\]
But due to the definition of the 5-point-star notation through the symmetric derivative, \(L_x(u)\) also includes some lattice points on the boundary \(\Gamma\).
This is because the points on the dashed line use the values of their cartesian neighbours in the linear combination of \(L_x(u)\).
Imagine the 5-point-star over the purple point \((h, h)\) for example.
We see that it overlaps with \(g(x-he_1)\) and \(g(x-he_2)\).

\begin{center}
   \input{drawings/poisson-2ndborder.tex}
\end{center}

To remove those duplicated terms from our desired equations, we add them to the right side of our system of linear equations.
This gives us the following cases of equations
For the center of the lattice in blue we have
\hfsetfillcolor{blue}
\hfsetbordercolor{blue}
\begin{equation*}
   \tikzmarkin{b1} (0.1,-0.8)(-0.1,1) M_x(u) := \frac{1}{h^2} \begin{pmatrix} & -1 & \\ -1 & 4 & -1\\ & -1 & \end{pmatrix} u(x) \tikzmarkend{b1} \qquad\text{and}\qquad \tikzmarkin{b2} r(x) := f(x) \tikzmarkend{b2}
\end{equation*}

For the corner point \(x = (h, h)^T\) of the second boundary we have
\hfsetfillcolor{purple}
\hfsetbordercolor{purple}
\begin{equation*}
   \tikzmarkin{c1} (0.1,-0.8)(-0.1,1) M_x(u) := \frac{1}{h^2} \begin{pmatrix}0 & -1 & 0\\0 & 4 & -1\\0 & 0 & 0\end{pmatrix} u(x) \tikzmarkend{c1} \qquad\text{and}\qquad \tikzmarkin{c2} (0.1,-0.3)(-0.1,0.55) r(x) := f(x) + \frac{1}{h^2}g(h, 0)^T + \frac{1}{h^2}g(0, h)^T \tikzmarkend{c2}
\end{equation*}
which is analogous for the remaining corner points.
For lattice points \(x = (x_1, x_2)^T\) on lower edge in orange i.e. \(x_1 \in [2h; 1-2h]\) and \(x_2 = h\) we set
\hfsetfillcolor{orange}
\hfsetbordercolor{orange}
\begin{equation*}
   \tikzmarkin{d1} (0.1,-0.8)(-0.1,1) M_x(u) := \frac{1}{h^2} \begin{pmatrix}0 & -1 & 0\\-1 & 4 & -1\\0 & 0 & 0\end{pmatrix} u(x) \tikzmarkend{d1} \qquad\text{and}\qquad \tikzmarkin{d2} (0.1,-0.3)(-0.1,0.55) r(x) := f(x) + \frac{1}{h^2}g(x_1, 0)^T \tikzmarkend{d2}
\end{equation*}
where we proceed analogously for the other edges.
Putting it all together we have the system of linear equations
\[M_x(u) = r(x)\]
In order to write this system in matrix form we have to enumerated the lattice points properly.
We use lexicographic numbering from the bottom left such that \(x_{ij} = (ih, jh)^T\) has the index \(k = (i-1)n + j\).
To illustrate:
\begin{center}
   \input{drawings/lexicographic_ordering.tex}
\end{center}

With this enumeration we get the equation for a blue center point \(x_k\) of the blue center.
\[M_{x_k}(u) = \frac{1}{h^2}\big((-1)u_{k-n} + (-1)u_{k+n} + 4u(x_k) + (-1)u(x_{k-1}) + (-1)u(x_{k+1})\big) = f(x_k)\]
In the other equations (in the two boundaries) we leave out the coefficients where the index of \(x\) is not in \(\{1, 2, \ldots, n^2\}\), as is intended by the equations with the 5-point-star notation above.
This way we can represent all \(n^2\) equations with the matrix \(M \in \mathbb{R}^{n^2 \times n^2}\) which we can write as \(n \times n\) blockmatrix
\[M := \frac{1}{h^2} \begin{pmatrix}
      T & -I_n & 0_n & \cdots & 0_n\\
      -I_n & T & -I_n & \ddots & \vdots\\
      0_n & -I_n & \ddots & \ddots & 0_n\\
      \vdots & \ddots & \ddots & \ddots & -I_n\\
      0_n & \cdots & 0_n & -I_n & T
   \end{pmatrix} \quad\text{where}\quad T := \begin{pmatrix}
      4 & -1 & 0 & \cdots & 0\\
      -1 & 4 & -1 & \ddots & \vdots\\
      0 & -1 & \ddots & \ddots & 0\\
      \vdots & \ddots & \ddots & \ddots & -1\\
      0 & \cdots & 0 & -1 & 4
\end{pmatrix} \in \mathbb{R}^{n \times n}\]
with the right side of the system \(r \in \mathbb{R}^{n^2}\) given through \(r_k := r(x_{i,j})\), hence we can write
\[M\widetilde{u} = r\]
where \(\widetilde{u}_k\) is the approxiamation of \(u\) in \(x_k\).
We see \(M\) is sparse as per row only 5 entries are different from zero.
Therefor we use this (the Poisson Model) to introduce iterative methods.

To summarize; we have seen, that the numerical discretisation of partial differential equations leads to immensely big systems of linear equations, with sparse matrices.
For such matrices, direct elimination methods are unsuitable, since their computational effort grows cubic with the matrix dimension.
This is why we introduce iterative methods for solving such systems.

\subsubsection{Properties of Poisson-Model}
In order to analyze the rate of convergence for those methods we need some additional theory about \(M\) generally called a \emph{difference operator}.
The term \emph{linear operator} is defined in functional analysis and is a synonym to the term linear map of linear algebra.
We will see that the rate of convergence depends on the eigenvalues of \(M\).

\begin{definition}[Linear Operator]
   The linear map \(T: X \to Y\) between \(\mathbb{R}\)-vector spaces is a linear operator iff
   \begin{enumerate}[label=\roman*, align=Center]
      \item \(\forall \lambda \in \mathbb{R}, x \in X: T(\lambda x) = \lambda T(x)\)
      \item \(\forall x, y \in X: T(x + y) = T(x) + T(y)\)
   \end{enumerate}
\end{definition}
\begin{example}
   For \(A \in \Mat_{m,n}(\mathbb{R})\) we have
   \[A: \mathbb{R}^m \to \mathbb{R}^n \qquad\text{where}\qquad x \mapsto Ax\]
\end{example}

\begin{definition}[Positive Definite Matrix]
   A matrix \(A\) iff
   \[\forall x \in \mathbb{R}^n \setminus \{0\}: x^TAx > 0\]
\end{definition}

\begin{lemma}[Eigenpairs of Poisson Model]
   \(M\) has \(n^2\) eigen-vectors and -values.
   The eigenvector written as eigenfunctions \(e_{ij}: \Theta_0 \to \mathbb{R}\) are defined through
   \[e_{ij}(x) := \frac{h}{2} \sin(i \pi y_1) \cdot \sin(j \pi y_2)\]
   with the corresponding eigenvalues
   \[\lambda_{ij} := \frac{4}{h^2} \left[\sin\left(\frac{\pi}{2} ih\right)^2 + \sin\left(\frac{\pi}{2} jh\right)^2\right]\]
   The eigenfunction form an orthonormal basis of \(\mathbb{R}^{\Theta_0}\) with the scalar product
   \[\langle u, v \rangle := \sum_{x \in \Theta_0} u(x) \cdot v(x)\]
\end{lemma}
\begin{proof}
   Left out.
\end{proof}

\begin{corollary}
   \(M\) is a positive definite operator, i.e.
   \[\forall v \in \mathbb{R}^{\theta_0} \setminus \{0\}: \langle v, Mv \rangle > 0\]
\end{corollary}
\begin{proof}
   \(M\) is symmetric and all its Eigenvalues are positive.
\end{proof}

\begin{corollary}
   The multiplicity of the Eigenvalues is the number of index pairs for which holds
   \[\lambda_{ij} = \lambda_{i'j'}\]
   The minimal and maximal Eigenvalues are given by
   \[\lambda_{\min} := \lambda_{11} = \frac{8}{h^2} \sin\left(\frac{\pi}{2} h\right)^2 \quad\text{and}\quad \lambda_{\max} := \lambda_{nn} = \frac{8}{h^2} \cos\left(\frac{\pi}{2} h\right)^2\]
\end{corollary}

\begin{definition}[Operator Norm]
   Given a linear operator \(M\) over a normed vector space \(V\)
   \[\|M\| := \sup\left\{\frac{\|Mv\|}{\|v\|} \mid v \in V: v \neq 0\right\}\]
\end{definition}

\begin{proposition}
   Given the operator \(M\) of the Poisson-model
   \begin{enumerate}[label=\roman*, align=Center]
      \item \(\|M\| \leq \lambda_{\max}\)
      \item \(\|M^{-1}\| \leq \lambda_{\min}^{-1}\)
   \end{enumerate}
\end{proposition}
\begin{proof}
   For the operator norm holds
   \[\|M\| = \sup_{v \in \mathbb{R}^{\theta_0} \setminus \{0\}} \frac{\|Mv\|}{\|v\|}\]
   where \(\|\ldots\|\) is the norm of the scalar product above.
   The expansion of \(v\) through eigenfunctions gives us
   \[v = \sum_{i,j = 1}^n \alpha_{ij}e_{ij}\]
   and so
   \[\|Mv\|^2 = v^T M^T M v = \sum_{i,j,k,l}^n \alpha_{ij}e_{ij}^T \lambda_{ij} \lambda_{kl} e_{kl} \alpha_{kl} = \sum_{i,j = 1}^n \lambda_{ij}^2 \alpha_{ij}^2 \leq \lambda_{}^2 \sum_{i,j = 1}^n \alpha_{ij}^2\]
   \[\|v\|^2 = \sum_{i,j = 1}^n \alpha_{ij}^2\]
   from which follows that
   \[\|M\| \leq \lambda_{}\]

   Now if we set \(v = e_{}\) where \(e_{\max}\) is the Gitterfunktion (?) for the eigenvalue \(\lambda_{\max}\) we get
   \[\|M\| \geq \lambda_{}\]
   and from that the estimation for the operator norm.
   To estimate the norm of the inverses we use
   \[\|M^{-1}\| = \sup_{v \in \mathbb{R}^{\theta_0} \setminus \{0\}} \frac{\|M^{-1}v\|}{\|v\|} \overset{v = Mw}{=} \sup_{v \in \mathbb{R}^{\Theta_0} \setminus \{0\}} \frac{\|w\|}{\|Mw\|} = \frac{1}{\inf \frac{\|Mw\|}{\|w\|}}\]
\end{proof}

\subsection{Introduction to Iterative Methods}
We start with an abstract construction of an iterative method to introduce the basic terminology.
Constructing iterative methods is based on decomposing the matrix \(A\) of the system \(Ax = b\).

So for instance let \(B \in \GL_n(\mathbb{R})\) be an invertible matrix, we split \(A\) as follows
\[A = B - (B - A)\]
Now we can translate the system of linear equations into a fixed-point equation
\begin{equation*}
   \begin{split}
      Ax = b & \iff \big(B - (B - A)\big)x = b \iff B^{-1}\big(B - (B - A)\big)x = B^{-1}b \iff \\
             & \iff \big(I_n + B^{-1}(B - A)\big)x = B^{-1}b \iff x = B^{-1}b - B^{-1}(B - A)x \iff \\
             & \iff x = (I_n - B^{-1}A)x + B^{-1}b \iff x = x - B^{-1}(Ax - b)
   \end{split}
\end{equation*}
From this fixpoint equation we can formulate a fixpoint iteration
\begin{equation}\label{eq:iter_smooth}
   x^{(k+1)} = x^{(k)} - N(Ax^{(k)} - b)
\end{equation}
where \(N := B^{-1}\) charactarizes the iterative method.
This form resembles a \emph{smoothing} or \emph{correction} process

\(Ax^{(k)} - b\) describes the ``distance'' between the given solution vector and the solution \(Ax^{(k)}\) of the current iteration.
This residual is then subtracted from the current iteration to form the new (hopefully improved) one.

Equivalently we can also write it in the form
\begin{equation}\label{eq:iter_mat}
   x^{(k+1)} = Kx^{(k)} + Nb
\end{equation}
where we call \(K := I_n - NA\) the \emph{iteration-matrix} of the method.
Following we introduce to general terminology and notation in order to show the convergence and rate of convergence of iterative methods.

\subsubsection{Rational Exponents for Matrices}
\begin{definition}[Energy Norm]
   \[\|v\|_B := \sqrt{\langle v, B \cdot v\rangle}\]
\end{definition}
\begin{remark}
   The associated matrix norm is also denoted as
   \[\|A\|_B := \sqrt{\langle A, B \cdot A\rangle}\]
\end{remark}

Furthermore we define the notation for two matrices \(A, B\) that
\[A > B :\iff A - B~\text{is positive definite}\]
\[A \geq B :\iff A - B~\text{is positive semi-definite}\]

We know from linear algebra that for a positive definite matrix \(B\) exists a unitary transformation \(Q\) such that \(B = Q^HDQ\) with a diagonal matrix \(D\) which has the positive eigenvalues of \(B\) on its diagonal.
From those two sidenotes we define
\[B^s := Q^HD^sQ \quad\text{for}~s \in \mathbb{R} \quad\text{where}\quad D^s := \begin{cases} d_{ii}^s & i = j\\ 0 & \text{else}\end{cases}\]
This way we can define square roots for positive definite matrices.

Since \(Q\) is unitary it holds that
\[\sqrt{B}\sqrt{B} = Q^HD^\frac{1}{2}QQ^HD^\frac{1}{2}Q = Q^HD^\frac{1}{2}D^\frac{1}{2}Q = Q^HDQ = B\]
We see that \(B^s\) is again positive definite.
Since both \(B\) and \(B^\frac{1}{2}\) are both positive definite and hence hermitian it holds that
\[\|v\|_B = \sqrt{\langle v, Bv \rangle} = \sqrt{\langle v, B^\frac{1}{2}B^\frac{1}{2}v \rangle} = \sqrt{\left\langle \big(B^\frac{1}{2}\big)^Hv, B^\frac{1}{2}v \right\rangle} = \sqrt{\langle B^\frac{1}{2}v, B^\frac{1}{2}v\rangle} = \|B^\frac{1}{2}v\|\]
\[\|A\|_B = \sup_{v \in \mathbb{R}^n\setminus\{0\}} \frac{\|Av\|_B}{\|v\|_B} = \sup_{v \in \mathbb{R}^n\setminus\{0\}} \frac{\|B^\frac{1}{2}Av\|}{\|B^\frac{1}{2}v\|} \overset{v := B^{-\frac{1}{2}}w}{=} \sup_{w \in \mathbb{R}^n\setminus\{0\}} \frac{\|B^\frac{1}{2}AB^{-\frac{1}{2}}w\|}{\|w\|} = \|B^\frac{1}{2}AB^{-\frac{1}{2}}\|\]
with the euclidean scalar product and the euclidean norm.

\subsubsection{Spectralradius}
The Spectralradius of the iteration matrix of an iterative method will be introduced as an indication if the method converges and it also describes the rate of convergence.
\begin{definition}[Spectrum]
   Given a matrix \(A\), its spectrum \(\sigma(A)\) is the set of its eigenvalues.
\end{definition}

\begin{lemma}\label{lem:sim_eq_spect}
   Two similar matrices \(A\) and \(B\) have the same spectrum.
\end{lemma}
\begin{proof}
   Let \(A\) be similar to \(B\) i.e. \(A = T^{-1}BT\) for some \(T \in \GL_n(K)\) and let \((e, \lambda)\) be an arbitrary eigenpair of \(B\) i.e. \(Be = \lambda e\), for which holds that \(e = Te'\).
   \[Be = \lambda e \iff T^{-1}Be = \lambda T^{-1}e \iff T^{-1}B(Te') = \lambda T^{-1}(Te') \iff Ae' = \lambda e'\]
\end{proof}

\begin{definition}[Spectral Radius of Matrix]
   Given \(A \in \Mat_n(\mathbb{R})\).
   \[\rho(A) := \max\{|\lambda| \mid \lambda \in \sigma(A)\}\]
\end{definition}
\begin{remark}
   Differently put it is the supremum among the absolute values of the elements in its spectrum.
\end{remark}

\begin{lemma}
   Let \(\|\ldots\|\) be a norm on \(\mathbb{R}^n\) and \(e^{(k)} := x^{(k)} - x\) the error, then
   \[\|e^{(k)}\| \leq \|K\|^k \cdot \|e^{(0)}\|\]
\end{lemma}
\begin{proof}
   Left out.
\end{proof}

\begin{lemma}\label{lem:spec_rad_smaller_than_norm}
   Let \(\|\ldots\|\) be a norm on \(\mathbb{R}^n\).
   \begin{enumerate}[label=\roman*, align=Center]
      \item For all eigenvalues \(\lambda\) of \(A \in \Mat_n(\mathbb{R})\) holds \(|\lambda| \leq \|A\|\).
      \item For all matrices \(A\) holds \(\rho(A) \leq \|A\|\).
   \end{enumerate}
\end{lemma}
\begin{proof}
   First off we prove (i), so let \(e\) be the normed eigenvector of the eigenvalue \(\lambda\).
   \[|\lambda| = \|\lambda e\| = \|Ae\| \leq \|A\| \|e\| = \|A\|\]
   Since the absolute value of any eigenvalue is less or equal to the matrix norm, it follows directly that also the eigenvalue with maximal absolute value is less or equal to the norm, i.e. (ii) follows directly from (i).
\end{proof}

\begin{lemma}\label{lem:eps_norm}
   For every \(A \in \Mat_n(\mathbb{R})\) and \(\varepsilon \in \mathbb{R}\) exists a norm \(\|\ldots\|\) on \(\mathbb{R}^n\) such that
   \[\rho(A) \leq \|A\| \leq \rho(A) + \varepsilon\]
\end{lemma}
\begin{remark}
   This lemma states, that for any \(\varepsilon > 0\) we can find a norm on \(\mathbb{R}^n\) such that the norm of the iteration matrix only differs by \(\varepsilon\) from the spectral radius.
\end{remark}
\begin{proof}
   J. Stoer, R. Bulirsch: Numerische Mathematik II, Satz 6.9.2
\end{proof}

\subsubsection{Convergence of Iterative Methods}
\begin{definition}[Regular Iterative Method]
   An iterative method in the form of \cref{eq:iter_smooth} where \(N \in \GL_n(K)\).
\end{definition}
\begin{remark}
   This is to say \(N\) is \emph{regular}.
\end{remark}

\begin{lemma}\label{lem:spec_rad_norm}
   Let \(A \in \GL_n(\mathbb{R})\), \(\|\ldots\|\) a norm on \(\mathbb{R}^n\) and \(\|\ldots\|_M\) an associated matrix norm.
   It holds that
   \[\rho(A) = \lim_{k \to \infty} \|A^k\|_M^\frac{1}{k}\]
\end{lemma}
\begin{proof}
   First we note \(A \in \GL_n(\mathbb{R}) \implies \rho(A) > 0\).
   Now we define the matrix \(B := \frac{1}{\rho(A)} A\) so we can rewrite the statement as
   \[\lim_{k \to \infty} \|B^k\|_M^\frac{1}{k} = 1\]
   We choose a norm \(\|\ldots\|_{B, \varepsilon}\) according to \cref{lem:eps_norm}, then holds
   \[1 = \rho(B) \overset{\ast}{=} \rho(B^k)^\frac{1}{k} \leq \|B^k\|_{B, \varepsilon}^\frac{1}{k} \leq \|B\|_{B, \varepsilon} \leq \rho(B) + \varepsilon = 1 + \varepsilon\]
   Where we used for \(\ast\) that \(\rho\big(P(A)\big) = P\big(\rho(A)\big)\) which can be shown using the \emph{Schur decomposition} which allows to write an arbitrary matrix as unitarily equivalent to an upper triangular matrix whose diagonal elements are the eigenvalues of the original matrix.
   This inequality holds for any \(k\) and so
   \[\limsup_{k \to \infty} \|B^k\|_{B, \varepsilon}^\frac{1}{k} \leq 1 + \varepsilon\]
   If we can show that the \(\limsup\) is independent of \(\varepsilon\), the statement follows if we let \(\varepsilon \to 0\).
   Let \(\varepsilon_0\) be a value taken by \(\varepsilon\).
   We define
   \[\|B\|_1 := \|B\|_{B, \varepsilon_0} \quad\text{and}\quad \|B\|_2 := \|B\|_{B, \varepsilon}\]
   According to the equivalence of norms on \(\mathbb{R}^n\) it holds that
   \[\exists c \in (0; 1): c\|B\|_1 \leq \|B\|_2 \leq \frac{1}{c} \|B\|_1\]
   Thus we can rewrite
   \begin{equation*}
      \begin{split}
         \limsup_{k \to \infty} \|B^k\|_1^\frac{1}{k} & \overset{c^\frac{1}{k} \to 1}{=} \limsup_{k \to \infty} \left(c \|B^k\|_1\right)^\frac{1}{k} \leq \limsup_{k \to \infty}\left(\|B^k\|_2\right)^\frac{1}{k} \\
                                                              & \leq \limsup_{k \to \infty}\left(\frac{1}{c} \|B^k\|_1\right)^\frac{1}{k} = \limsup_{k \to \infty} \|B^k\|_1^\frac{1}{k}
      \end{split}
   \end{equation*}
   Which means that all above actually equal.
   Therefore does \(\limsup\) not depend on \(\varepsilon\) and thus the statement is proven.
\end{proof}

\begin{theorem}\label{thm:spec_rad_conv}
   An iterative method as in \cref{eq:iter_smooth}
   \begin{enumerate}[label=\roman*, align=Center]
      \item is convergent iff \(\rho(K) < 1\).
      \item converges to the exact solution \(x\) if the method is regular.
   \end{enumerate}
\end{theorem}
\begin{proof}
   First we prove (i), so suppose the method converges.

   Let \(b = 0\), then the iteration is given as
   \[x^{(k)} = K^k x^{(0)}\]
   \[x^{(0)} = 0 \implies \lim_{k \to \infty} x^{(k)} = \lim_{k \to \infty} K^k x^{(0)} = 0\]
   which must hold for any \(x^{(0)}\), according to \cref{def:conv_iter_meth}, since the iteration is convergent.

   Now let \(x^{(0)} = e\) be the eigenvector of \(\lambda_{}\) of \(K\).
   \[\lim_{k \to \infty} K^k e = \lim_{k \to \infty} \lambda_{}^k e = \left(\lim_{k \to \infty} \lambda_{\max}^k \right) e \overset{!}{=} 0\]
   \[e \neq 0 \implies \lim_{k \to \infty} \lambda_{}^k = 0 \implies |\lambda_{\max}| = \rho(K) < 1\]

   Now we prove the converse, so suppose \(\rho(K) < 1\) and let \(\rho' \in (\rho(K); 1)\).
   According to \cref{lem:spec_rad_norm} for a sufficiently large \(m_0\) holds that
   \[\forall m \geq m_0: \|K^k\|^\frac{1}{k} \leq \rho' \implies \|K^k\| \leq \rho'^k\]
   but since \(\rho' < 1\) follows
   \[\lim_{k \to \infty} \rho'^k = 0 \implies \lim_{k \to \infty} \|K^k\| = 0 \implies \lim_{k \to \infty} \|K^k x^{(0)}\| = 0 \implies \lim_{k \to \infty} K^kx^{(0)} = 0\]
   for any starting value \(x^{(0)}\).
   Thus we have proven (i) according to \cref{def:conv_iter_meth}.

   In order to prove (ii) we write the iteration method in the form form
   \[x^{(k+1)} = Kx^{(k)} + Nb\]
   and suppose that \(N\) is invertible i.e. the method is regular.
   Recursively we get
   \begin{equation*}
      \begin{split}
         x^{(k)} & = Kx^{(k-1)} + Nb = K\big(Kx^{(k-2)} + Nb\big) + Nb = K^2x^{(k-2)} + KNb + Nb = \\
                 & = K^3x^{(k-3)} + K^2Nb + Nb = \ldots = K^kx^{(0)} + \left(\sum_{i=0}^{k-1} K^i\right) Nb
      \end{split}
   \end{equation*}
   For the sum can be shown that
   \[\left(\sum_{i=0}^{k-1} K^i\right) (I_n - K)= I_n - K^k\]
   Since \(\rho(K) < 1\) implies that 1 is not an eigenvalue of \(K\), follows that \(I_n - K\) is invertible, hence
   \[\sum_{i=0}^{k-1} K^i = (I_n - K^k)(I_n - K)^{-1}\]
   \[K^k \xrightarrow{k \to \infty} 0 \implies \lim_{k \to \infty} \left(\sum_{i=0}^{k-1} K^i\right) = (I_n - K)^{-1}\]
   So for any given solution \(b\) we receive
   \[\lim_{k \to \infty} x^{(k)} = \lim_{k \to \infty} K^kx^{(0)} + \left(\sum_{i=0}^{k-1} K^i\right) Nb = (I_n - K)^{-1}Nb\]
   which means that the method is convergent according to \cref{def:conv_iter_meth}.
   Now we still need to show that it converges to the exact solution \(x\), so we note that
   \[K = I_n - NA \iff NA = (I_n - K) \iff (I_n - K)^{-1} = A^{-1}N^{-1}\]
   \[K = A^{-1}N^{-1} \implies (I_n - K)^{-1}Nb = A^{-1}N^{-1}Nb = A^{-1}b = x \implies \lim_{k \to \infty} x^{(k)} = x\]
\end{proof}

From the proof of the theorem above we derive the following
\begin{definition}[Convergent Iterative Method]\label{def:conv_iter_meth}
   Given \(A \cdot x = b\), an iterative method as in \cref{eq:iter_smooth} is convergent iff
   \[\forall b \in \mathbb{R}^n: \exists \lim_{k \to \infty} x^{(k)} = x~\text{independent of}~x^{(0)}\]
\end{definition}
% TODO: write down example
\begin{remark}
   To determine the number of iterations which are required to approximate the solution to an accuracy of \(\varepsilon\) we can use
   \[\rho(K)^n = \varepsilon \iff n = \log_{\rho(K)}(\varepsilon) \iff n = \frac{\log(\varepsilon)}{\log\big(\rho(K)\big)}\]
\end{remark}

The following theorem is used to prove the convergence of the Gauss-Seidel method which will be introduced later on.
\begin{theorem}\label{thm:abstr_gauss_seidel_conv}
   Given an iterative method with the iteration matrix
   \[K = I_n - W^{-1}A \quad\text{where}\quad W + W^H > A > 0\]
   The method converges and it holds that \(\|K\|_A < 1\).
\end{theorem}
\begin{proof}
   It is sufficient to prove \(\|K\|_A < 1\) since we proved that \(\rho(K) \leq \|K\|\) in \cref{lem:spec_rad_smaller_than_norm}.
   Furthermore we proved in \cref{thm:jacobi_conv} that
   \[\|K\|_A = \|A^\frac{1}{2}KA^{-\frac{1}{2}}\|\]

   Now we define the matrix \(\hat{K} := A^\frac{1}{2}KA^{-\frac{1}{2}}\) for which holds that \(\hat{K} = I_n - A^\frac{1}{2}K^{-1}A^\frac{1}{2}\).
   We also prove that \(\hat{K}^H \hat{K} = I_n\), for that we set \(W := (D-L)\).
   \begin{equation*}
      \begin{split}
         \hat{K}^H \hat{K} & = I_n - A^\frac{1}{2}(W^{-H} + W^{-1})A^\frac{1}{2} + A^\frac{1}{2}W^{-H}AW^{-1}A^\frac{1}{2} = \\
                           & = I_n - A^\frac{1}{2}W^{-H}(W + W^H)W^{-1}A^\frac{1}{2} + A^\frac{1}{2}W^{-H}AW^{-1}A^\frac{1}{2} = \\
                           & < I_n - A^\frac{1}{2}W^{-H}AW^{-1}A^\frac{1}{2} + A^\frac{1}{2}W^{-H}AW^{-1}A^\frac{1}{2} = I_n
      \end{split}
   \end{equation*}
   It can be shown that for every matrix \(B \in \Mat_n(K)\) holds \(\|B\| = \sqrt{\rho(B^HB)}\).
   As well as for positive definite matrices \(B, C: C > B\) holds \(\rho(B) < \rho(C)\).
   From this follows
   \[\|K\|_A = \|\hat{K}\| = \sqrt{\rho(\hat{K}^H\hat{K})} < 1\]
\end{proof}

\subsubsection{The Rate of Convergence}
\begin{lemma}\label{lem:spectr}
   Let \(B\) be a hermitian matrix where \(\alpha_1 I_n < B < \alpha_2 I_n\), then holds
   \[\sigma(B) \subset (\alpha_1, \alpha_2)\]
\end{lemma}
\begin{remark}
   With the following convention
   \[\alpha_1 I_n \leq B \leq \alpha_2 I_n \implies \sigma(B) \subset [\alpha_1, \alpha_2]\]
\end{remark}
\begin{proof}
   \(B\) being hermitian implies that it can be diagonalized through a unitary matrix.
   \[B = Q^HDQ\]

   Thus the statement above is equivalent to \(\alpha_1 I_n < D < \alpha_2 I_n\).

   Now, since \(D\) has the eigenvalues of \(B\) on its diagnoal, the statement ``\(D - \alpha_1 I_n\) is positive definite'' means that all eigenvalues of \(D\) must be larger than \(\alpha_1\).
   The upper estimation follows analogously.
\end{proof}

The following abstract theorem will be used to prove the rate of convergence for the following Jacobi method.
\begin{theorem}\label{thm:jacobi_conv}
   An iterative method as in \cref{eq:iter_smooth} converges if \(0 < A < 2N^{-1}\).
   Regarded as in \cref{eq:iter_mat} it converges with a rate of
   \[\rho(K) = \|K\|_A = \|K\|_{N^{-1}} \leq \max\{1 - \lambda, \Lambda - 1\}\]
   for some \(\lambda, \Lambda \in \mathbb{R}: 0 < \lambda \leq \Lambda\).
\end{theorem}
\begin{proof}
   For this proof we use that
   \[0 < \lambda N^{-1} \leq A \leq \Lambda N^{-1} \iff \sigma(M) \subset [1-\Lambda; 1-\lambda]\]
   which we will show cyclically.

   First we prove that \(\rho(M) < 1\).
   For that we define
   \[M' := \|M\|_A = A^\frac{1}{2}MA^{-\frac{1}{2}} = I - A^\frac{1}{2}NA^\frac{1}{2} \quad\text{and}\quad M'' := \|M\|_{N^{-1}} = N^{-\frac{1}{2}}MN^\frac{1}{2} = I - N^\frac{1}{2}AN^\frac{1}{2}\]
   since both are similar to \(M\), they all have the same spectrum according to \cref{lem:sim_eq_spect} hence
   \[\sigma(M) = \sigma(M') = \sigma(M'') \implies \rho(M) = \rho(M') = \rho(M'')\]
   and because \(M'\) and \(M''\) are hermitian holds
   \[\rho(M') = \|M'\| = \|M\|_A \quad\text{and}\quad \rho(M'') = \|M''\| = \|M\|_{N^{-1}}\]
   from which follows that \(\rho(M) = \|M\|_A = \|M\|_{N^{-1}}\).

   Now let \(A' := N^\frac{1}{2}AN^\frac{1}{2}\) and we rewrite according to \cref{lem:spectr}.
   \[0 < A < 2N^{-1}\implies 0 < A' < 2I_n \implies \sigma(A') \subset (0, 2)\]
   \[\lambda N^{-1} \leq A \leq \Lambda N^{-1} \implies \lambda I_n \leq A' \leq \Lambda I_n \implies \sigma(A') \subset [\lambda; \Lambda]\]
   then follows from \(M'' = I - A'\) that
   \[\sigma(M'') \subset (-1; 1) \implies \rho(M) = \rho(M'') < 1\]
   \[\sigma(M'') \subset [1-\Lambda; 1-\lambda] \implies \sigma(M) \subset [1-\Lambda; 1-\lambda]\]

   Now we show the rate of convergence.
   \[\rho(M) = \rho(M'') = \{|\lambda| \mid \lambda~\text{is ev of}~M''\} \leq \max\{|\xi| \mid \xi \in [1-\Lambda; 1-\lambda]\} = \max\{|1-\Lambda|, |1-\lambda|\}\]
   where we have that \(0 < \lambda \leq \Lambda \implies \{|1-\Lambda|, |1-\lambda|\} = \max\{1-\Lambda, 1-\lambda\}\).

   Finally we bring this proof full circle by proving our assumption from the beginning.
   Under the condition that \(N\) is positive definite and \(A\) is hermitian we have that \(\sigma(M'')\) is real, hence we have a minimal and maximal eigenvalue \(\rho_{\min}\), \(\rho_{}\) of \(M''\).
   \[\sigma(M) = \sigma(M'') \subset [\rho_{\min}; \rho_{}]\]
   Since \(A' = I - M''\) it follows that
   \[\lambda := 1 - \rho_{} \quad\text{and}\quad \Lambda := 1 - \rho_{\min}\]
   are the minimal, respectively maximal eigenvalue of \(A'\).
   Now since \(\sigma(A') \subset [\lambda; \Lambda]\) and \(0 < \lambda \leq \Lambda\) it follows that \(A'\) is positive definite.
   This means that
   \[\lambda I_n \leq A' \leq \Lambda I_n \implies \lambda I_n \leq N^\frac{1}{2}AN^\frac{1}{2} \leq \Lambda I_n \implies \lambda N^{-1} \leq A \leq \Lambda N^{-1}\]
\end{proof}

From the prove above we derive the following
\begin{definition}[Rate of Convergence]
   The rate of convergence of an iterative method in the form \cref{eq:iter_mat} is the spectral radius \(\rho(K)\) of its iteration matrix.
\end{definition}

\subsection{Classic Iterative Methods}
In the introduction we have seen that the convergence of iterative solvers for linear systems is determined by the spectral radius of the iteration matrix \(\rho(K)\).
This is what characterizes the historic iterative methods.

\subsubsection{Richardson}
We regard the decomposition
\[A = I_n + (A - I_n)\]
and rewrite the system as follows
\[Ax = b \iff \big(I_n + (A - I_n)\big)x = b \iff x = b - (A - I_n)x \iff x = (I_n - A)x + b\]
and finally as fixpoint iteration
\begin{definition}[Richardson Method]
   \[x^{(k+1)} = Kx^{(k)} + b \qquad\text{where}\qquad K := (I_n - A)\]
   \[x^{(k+1)} = x^{(k)} - N(Ax^{(k)} - b) \qquad\text{where}\qquad N := I_n\]
\end{definition}

\subsubsection{Gauss-Seidel}
We regard the decomposition
\[A = D - L - U\]
with a proper lower triangular matrix \(L\), a diagonal matrix \(D\) and a proper upper triangular matrix \(U\).
With this we rewrite the system as
\begin{equation*}
   \begin{split}
      Ax = b & \iff (D - L - U)x = b \iff (D - L)x = b + Ux \iff x = (D - L)^{-1}Ux + (D - L)^{-1}b \iff \\
             & \iff x = (D - L)^{-1}((D - L - A)x + (D - L)^{-1}b \iff x = (I_n - (D - L)^{-1}A)x + (D - L)^{-1}b
   \end{split}
\end{equation*}
From which we formulate the fixpoint iteration
\begin{definition}[Gauss-Seidel Method]
   \[x^{(k+1)} = Kx^{(k)} + (D - L)^{-1}b \quad\text{where}\quad K := I_n - (D - L)^{-1}A\]
   \[x^{(k+1)} = x^{(k)} - N(Ax^{(k)} - b) \quad\text{where}\quad N := (D - L)^{-1}\]
\end{definition}

However we can derive a formula, which doesn't depend on the seperate matrices.
We regard the intermediate step
\[(D + L)x = b - Ux\]
The Gauss–Seidel method now solves the left hand side of this expression for \(x\), using previous value for \(x\) on the right hand side.
So by taking advantage of the triangular form of \(D+L\), the elements of \(x^{(k+1)}\) can be computed sequentially using backward substitution.
\[
   \begin{pmatrix}
      d_{11}  & 0     & 0      & 0 \\
      l_{21} & d_{22} & 0      & 0 \\
      l_{31} & l_{32} & d_{33} & 0 \\
      l_{41} & l_{42} & l_{43} & d_{44}
   \end{pmatrix} \begin{pmatrix} x_1^{(k+1)}\\x_2^{(k+1)}\\x_3^{(k+1)}\\x_4^{(k+1)}\end{pmatrix} =
   \begin{pmatrix}b_1\\b_2\\b_3\\b_4\end{pmatrix} -
   \begin{pmatrix}
      0 & u_{12} & u_{13} & u_{14} \\
      0 & 0      & u_{23} & u_{24} \\
      0 & 0      & 0      & u_{34} \\
      0 & 0      & 0      & 0
   \end{pmatrix} \begin{pmatrix}x_1^{(k)}\\x_2^{(k)}\\x_3^{(k)}\\x_4^{(k)}\end{pmatrix}
\]
This means the \(i\)-th equation is solved after the \(i\)-th variable \(x_i^{(k+1)}\), whereby the previously calculated values \(x_1^{(k+1)}, \ldots, x_{i-1}^{(k+1)}\) of the current iteration step are also used.
\[x_2^{(k+1)} = \frac{1}{d_{22}}\bigg(b_2 - l_{21}x_1^{(k+1)} - \Big(u_{23}x_3^{(k)} + u_{24}x_4^{(k)}\Big)\bigg)\]
Which brings us to the recursive formula
\[x_i^{(k+1)} = \frac{1}{a_{ii}}\left(b_i - \sum_{j=1}^{i-1}a_{ij}x_j^{(k+1)} - \sum_{j=i+1}^n a_{ij}x_j^{(k)}\right)\]
which we can rewrite in our correction form
\begin{definition}[Gauss-Seidel Method Formula]
   \[x_i^{(k+1)} = x_i^{(k)} - \frac{1}{a_{ii}} \left(\sum_{j=1}^{i-1} a_{ij} x_j^{(k+1)} + \sum_{j=i}^n a_{ij} x_j^{(k)} - b_i\right)\]
\end{definition}

\lstinputlisting[language=Matlab, caption={Gauss-Seidel}]{gauss_seidel.m}

The Gauss-Seidel Method is one of the classic iterative methods for which convergence can be proven using only the spectral radius.
\begin{theorem}[Gauss-Seidel Convergence]
   The Gauss-Seidel Method converges for positive definite matrices \(A\) and it holds that \(\|K\|_A < 1\).
\end{theorem}
\begin{proof}
   Since \(A\) is positive definite, \(D\) has only positive entries.
   The iteration matrix is given through \(K = I_n - (D-L)^{-1}A\) and suffices
   \[(D - L) + (D - L)^H = 2D - L - U = A + D > A\]
   Then the statement follows from \cref{thm:abstr_gauss_seidel_conv} for \(W := D - L\).
\end{proof}

\subsubsection{Jacobi}
We regard the same decomposition from before
\[A = D - L - U\]
which we write as fixpoint equation
\begin{equation*}
   \begin{split}
      Ax = b & \iff (D - L - U)x = b \iff Dx = b + Lx + Ux \implies \\
             & \implies Dx = b + (L + U)x \implies x = (I_n - D^{-1}A)x + D^{-1}b
   \end{split}
\end{equation*}
\begin{definition}[Jacobi Method]
   \[x^{(k+1)} = Kx^{(k)} + D^{-1}b \qquad\text{where}\qquad K := (I_n - D^{-1}A)\]
   \[x^{(k+1)} = x^{(k)} - N(Ax^{(k)} - b) \qquad\text{where}\qquad N := D^{-1}\]
\end{definition}
\begin{remark}
   In contrast to Gauss-Seidel we only use values of the previous iteration.
\end{remark}
As with Gauss-Seidel we regard
\[Dx = b - (L + U)x\]
and realise that we can solve the equations for \(x\) on the left by backward substitution which gives us the formula
\[x_i^{(k+1)} = \frac{1}{a_{ii}} \left(b_i - \sum_{j \neq i} a_{ij} x_j^{(k)}\right)\]
Rewriting gives us the
\begin{definition}[Jacobi Method Formula]
   \[x_i^{(k+1)} = x_i^{(k)} - \frac{1}{a_{ii}} \left(\sum_{j=1}^n a_{ij} x_j^{(k)} - b_i\right)\]
\end{definition}
\lstinputlisting[language=Matlab, caption={Jacobi Method}]{jacobi.m}

The following theorem shows the rate of convergence of the Jacobi Method.
\begin{theorem}[Jacobi Convergence]\label{cor:jacobi_conv}
   Let \(A\) be a matrix with diagonal \(D\) such that
   \[0 < A < 2D - A\]
   Then holds that
   \[\rho(K) = \|K\|_A = \|K\|_D < 1\]
\end{theorem}
\begin{proof}
   Follows from \cref{thm:jacobi_conv} for \(N = D^{-1}\).
\end{proof}

\subsection{Relaxation Methods}
We have seen that the convergence of these classical iterative solvers for linear systems is determined by the spectral radius of the iteration matrix, \(\rho(K)\).
However this ultimately means that the rate of convergence depends on the properties of \(A\), hence on the given problem.
So to still be able to ensure the convergence we need a way to analyze convergence depending on \(A\).

This is the reason we look at a certain category of iterative solvers, namely \emph{relaxation methods}, which employ a \emph{relaxation}-parameter \(\omega\).
This way we can prove the rate of convergence for some value of \(\omega\) which depends on the coefficient matrix \(A\).
For a general linear system, it is difficult to determine an optimal (or even good) parameter due to the difficulty in determining the spectral radius of the iteration matrix (because its difficult to compute the largest eigenvalue.)

\subsubsection{Damped Richardson}
\begin{definition}[Damped Richardson Method]
   \[x^{(k+1)} = Kx^{(k)} + \omega b \qquad\text{where}\qquad K := (I_n - \omega A)\]
   \[x^{(k+1)} = x^{(k)} - \omega N(Ax^{(k)} - b) \qquad\text{where}\qquad N := I_n\]
\end{definition}

% TODO: comment
\lstinputlisting[language=Matlab, caption={Richardson Method}]{richardson.m}

\begin{lemma}[Damped Richardson Rate of Convergence]\label{lem:spec_rad_rich}
   Let \(A\) be positive definite with smallest and largest eigenvalues \(\lambda_{\min}\), \(\lambda_{\max}\), then holds
   \[\rho(K) = \max\{|1 - \omega \lambda_{\min}|, |1 - \omega \lambda_{\max}|\}\]
\end{lemma}
\begin{proof}
   The eigenvalues of \(K\) are given as \(1 - \omega \lambda_i\) with the eigenvalue \(\lambda_i\) of \(A\).
   Since the function \(p(x) := |1 - \omega x|\) has no local maximum, it takes the maximum in either \(\lambda_{\min}\) or \(\lambda_{}\).
\end{proof}

\begin{theorem}[Damped Richardson Convergence]
   Given \(A\) has only positive eigenvalues, then the relaxed Richardson method converges iff
   \[\omega \in \left(0; \frac{2}{\lambda_{\max}}\right)\]
\end{theorem}
\begin{proof}
   First we prove that \(\omega \in \left(0; \frac{2}{\lambda_{}}\right)\) implies convergence.
   \[\omega \in \left(0; \frac{2}{\lambda_{}}\right) \implies -1 < 1 - \omega\lambda_{\max} \leq 1 - \omega\lambda_{\min} < 1\]
   From \cref{lem:spec_rad_rich} follows \(\rho(K) < 1\), so according to \cref{thm:spec_rad_conv} converges the method.

   Now we prove the converse so suppose the method converges i.e. \(\rho(K) < 1\).
   \[1 > \rho(K) \geq |1 - \omega\lambda_{}| \geq 1 - \omega\lambda_{\max} \implies \omega > 0\]
   \[-1 < - \rho(K) \leq - |1 - \omega\lambda_{}| \leq 1 - \omega\lambda_{\max} \implies \omega\lambda_{\max} < 2\]
\end{proof}

\begin{theorem}[Damped Richardson Optimal Rate of Convergence]\label{thm:optimal_omega}
   Suppose \(A\) has only positive eigenvalues with smallest and largest \(\lambda_{\min}\), \(\lambda_{\max}\).
   For
   \[\omega := \frac{2}{\lambda_{\max} + \lambda_{\min}}\]
   the rate of convergence is optimal with
   \[\rho(K) := \frac{\lambda_{\max} - \lambda_{\min}}{\lambda_{\max} + \lambda_{\min}}\]
\end{theorem}
\begin{proof}
   Follows directly from the representation of the spectral radius in \cref{lem:spec_rad_rich}.
\end{proof}

\begin{corollary}[Damped Richardson Rate of Convergence for Poisson]
   The rate of convergence of the relaxed Richardson method with \(\omega\) as in \cref{thm:optimal_omega} concerning the Poisson Model is
   \[\rho(M) = 2 \cos\left(\frac{\pi}{2}h\right) - 1 = 1 - \frac{\pi^2 h^2}{2} + \mathcal{O}(h^4)\]
\end{corollary}
\begin{proof}
   % TODO
\end{proof}

What eigenvalues \(A\) (and hence the iteration matrix \(K\)) has is typically a significantly more complicated question than actually solving the system of equations.
This makes it necessary to estimate the smallest/largest eigenvalue.
In contrast to calculating the largest eigenvalue can the maximum norm of a sparse matrix be easily calculated.
\begin{corollary}[Damped Richardson sufficient Convergence]
   Sufficient for the convergence of the Richardson method is the condition
   \[\omega \in \left(0; \frac{2}{\|A\|_\infty}\right) \quad\text{with}\quad \|A\|_\infty := \sup_{v \in \mathbb{R}^n \setminus \{0\}} \frac{\|Av\|_\infty}{\|v\|_\infty} = \max_{i \in [1;n]}\left(\sum_{j = 1}^n |A_{ij}|\right)\]
\end{corollary}
\begin{proof}
   % TODO: exercise
\end{proof}

\subsubsection{Successive Over-Relaxation (SOR)}
\begin{definition}[SOR Method]
   \[x^{(k+1)} = (I_n - \omega E)^{-1} \big((1 - \omega)I_n + \omega F)x^{(k)} + \omega Nb\]
   where
   \[N := (D - \omega L)^{-1} \qquad E := D^{-1}L \qquad F := D^{-1}U\]
   and we have the iteration matrix
   \[K := I_n - \left(\frac{1}{\omega}D - L\right)^{-1}A\]
\end{definition}

\begin{definition}[SOR Method Formula]
   \[x_i^{(k+1)} = x_i^{(k)} - \frac{\omega}{a_{ii}} \left(\sum_{j=1}^{i-1} a_{ij} x_j^{(k+1)} + \sum_{j=i}^n a_{ij} x_j^{(k)} -b_i\right)\]
\end{definition}
\begin{remark}
   For \(\omega = 1\) we receive the regular Gauss-Seidel method.
\end{remark}

\begin{lemma}
   For the SOR method with the iteration matrix above holds
   \[\forall \omega \in \mathbb{C}: \rho(M) \geq |\omega - 1|\]
\end{lemma}
\begin{proof}
   Let \(n := \dim(A)\).
   Since \(I_n - \omega L\) and \((1 - \omega)I_n + \omega U\) are lower or upper triangular matrices with constant diagonal entries holds
   \[\det(I_n - \omega L) = 1 \qquad\text{and}\qquad \det((1 - \omega)I_n + \omega U) = (1 - \omega)^n\]
   through the properties of the determinant follows
   \[\det(M) = \frac{1}{\det(I_n - \omega L)} \det((1 - \omega)I_n + \omega U) = (1 - \omega)^n\]
   Since we are interested in the largest eigenvalue of \(M\) we use the identity
   \[\det(\lambda I_n - M) = \prod_{i = 1}^n(\lambda - \lambda_i)\]
   with the eigenvalues \(\lambda_i\) of \(M\).
   Setting \(\lambda = 0\) in this equation gives us
   \[(-1)^n \det(M) = \det(-M) = (-1)^n \prod_{i = 1}^n \lambda_i\]
   where we get
   \[\det(-M) = (-1)^n \prod_{i = 1}^n \lambda_i \implies (1 - \omega)^n = \prod_{i = 1}^n \lambda_i \implies |1 - \omega|^n = \prod_{i=1}^n |\lambda_i|\]
   This implies that at least one eigenvalue with \(|\lambda_i| \geq |1 - \omega|\) exists.
\end{proof}

\begin{theorem}[SOR Convergence]
   Given \(A\) positive definite and \(\omega \in (0; 2)\), the SOR method converges with a rate of
   \[\rho(K) \leq \|K\|_A < 1\]
\end{theorem}
\begin{proof}
   Let \(W := \frac{1}{\omega} D - E\).
   Since
   \[\omega \in (0; 2) \implies \frac{2}{\omega} - 1 > 0\]
   follows
   \[W + W^H = \frac{2}{\omega}D - E - F = A + \left(\frac{2}{\omega} - 1\right)D > A > 0\]
   But in order for the method to be defined we need to show that \(W\) is invertible.
   So let \(Wx = 0\), it follows that
   \[0 = \langle x, Wx \rangle + \langle Wx, x \rangle = \langle x, W + W^H x \rangle\]
   and since \(W + W^H > 0\) follows \(x = 0\).
\end{proof}

Practical experience shows that SOR converges substantially faster than Gauss-Seidel for a suitable choice of \(\omega\).
Choosing \(\omega\) as in the following lemma we can optimize the rate of convergence.
\begin{lemma}\label{lem:sor_iter_mat_norm}
   Let \(A\) be positive definite and \(\omega \in (0; 2)\), then holds
   \[\|K\|_A = \sqrt{1 - \frac{\frac{2}{\omega} - 1}{\|A^{-\frac{1}{2}}WD^{-\frac{1}{2}}\|_2^2}} \quad\text{with}\quad W = (\omega N)^{-1} = \frac{1}{\omega}D - E\]
   If for some \(c \in \mathbb{R}^+\) holds \(WD^{-1}W^H \leq cA\), then follows
   \[\|A^{-\frac{1}{2}}WD^{-\frac{1}{2}}\|^2 \leq c \qquad\text{and}\qquad \|k\|_A \leq \sqrt{1 - \frac{\frac{2}{\omega} - 1}{c}}\]
\end{lemma}
\begin{proof}
   We use
   \[\|M\|_A = A^\frac{1}{2}MA^{-\frac{1}{2}} = I_n - \omega A^\frac{1}{2} N A^\frac{1}{2}\]
   From
   \[A - W - W^HJ = D - E - E^H - \frac{1}{\omega}D + E - \frac{1}{\omega}D + E^H = \left(1 - \frac{2}{\omega}\right)D\]
   follows
   \begin{equation*}
      \begin{split}
         \|M\|_A^H\|M\| & = (I_n - \omega A^\frac{1}{2}N^HA^\frac{1}{2})(I_n - \omega A^\frac{1}{2}NA^\frac{1}{2}) = \\
                        & = I_n - A^\frac{1}{2}(\omega N^H + \omega N)A^\frac{1}{2} + A^\frac{1}{2} \omega N^H A \omega N A^\frac{1}{2} = \\
                        & = I_n - A^\frac{1}{2}(\omega N^HW\omega N + \omega N^HW^H\omega N)A^\frac{1}{2} + A^\frac{1}{2}\omega N^HA\omega NA^\frac{1}{2} = \\
                        & = I_n + A^\frac{1}{2} \omega N^H(A - W - W^H) \omega NA^\frac{1}{2} = \\
                        & = I_n + \left(1 - \frac{2}{\omega}\right)A^\frac{1}{2} \omega N^HD\omega NA^\frac{1}{2} = I_n - \left(\frac{2}{\omega} - 1\right)(XX^H)^{-1}
      \end{split}
   \end{equation*}
   with the matrix
   \[X := A^{-\frac{1}{2}}(\omega N)^{-1}D^{-\frac{1}{2}} = A^{-\frac{1}{2}}WD^{-\frac{1}{2}}\]
   Now we need to estimate the smallest eigenvalue of \((XX^H)^{-1}\) below
   \[\frac{1}{\rho(XX^H)} = \frac{1}{\|X\|^2}\]
   and so
   \[\|M\|^2_A = \rho(\|M\|^H\|M\|) = 1 - \left(\frac{2}{\omega} - 1\right) \frac{1}{\|X\|_2^2}\]
   which is the upper boundary for the rate of convergence.
   Now we still need to show the estimate of the norm
   \[XX^H = A^{-\frac{1}{2}}WD^{-1}W^HA^{-\frac{1}{2}} \leq cA^{-\frac{1}{2}}AA^{-\frac{1}{2}} = cI_n\]
   so it follows that \(\sigma(XX^H) \subset [0; c]\) and so \(\|X\|^2 = \rho(XX^H) \leq c\).
\end{proof}

\begin{theorem}[SOR Optimal Rate of Convergence]
   Let \(\gamma, \Gamma \in \mathbb{R}^+\) such that
   \[0 < \gamma D \leq A \qquad\text{and}\qquad \left(\frac{1}{2}D - L\right)D^{-1}\left(\frac{1}{2}D - L^T\right) \leq \frac{\Gamma}{4}A\]
   then suffices
   \[c := \frac{\Omega^2}{\gamma} + \Omega + \frac{\Gamma}{4} \qquad\text{where}\qquad \Omega := \frac{2 - \omega}{2\omega} = \frac{1}{\omega} - \frac{1}{2}\]
   the condition \(WD^{-1}W^H \leq cA\) of \cref{lem:sor_iter_mat_norm} and we get the estimation
   \[\|K\|_A \leq \sqrt{1 - \frac{2\Omega}{\frac{\Omega^2}{\gamma} + \Omega + \frac{\Gamma}{4}}}\]
   For the optimal relaxation parameter \(\omega_{opt} := \frac{2}{1 + \sqrt{\gamma\Gamma}}\) is the right side a minimum, namely
   \[\|K\|_A \leq \sqrt{\frac{\sqrt{\Gamma} - \sqrt{\gamma}}{\sqrt{\Gamma} + \sqrt{\gamma}}}\]
\end{theorem}
\begin{proof}
   We write the matrix \(W\) as
   \[W = \frac{1}{\omega}D - L = \left(\frac{1}{\omega} - \frac{1}{2}\right)D + \frac{1}{2}D- L = \Omega D + \frac{1}{2}D - L\]
   Then follows
   \begin{equation*}
      \begin{split}
         WD^{-1}W^T & = \left[\Omega D + \left(\frac{1}{2}D - L\right)\right]D^{-1} \left[\Omega D + \left(\frac{1}{2}D - L^T\right)\right] = \\
                    & = \Omega^2 D + \Omega\left(\frac{1}{2}D - L + \frac{1}{2}D - L^T\right) + \left(\frac{1}{2}D - L\right)D^{-1} \left(\frac{1}{2}D - L^T\right) \leq\\
                    & \leq \frac{\Omega^2}{\gamma} A \Omega A + \frac{\Gamma}{4}A = \left(\frac{\Omega^2}{\gamma} + \Omega + \frac{\Gamma}{4}\right)A = cA
      \end{split}
   \end{equation*}
   In order to determine \(\omega_{opt}\) we have to maximize
   \[f(\omega) := \frac{2 \Omega}{\frac{\Omega^2}{\gamma} + \Omega + \frac{\Gamma}{4}}\]
   We see that for \(\Omega_{opt} := \frac{\sqrt{\gamma\Gamma}}{2}\)
   \[f(\Omega_{opt}) = \frac{2 \sqrt{\gamma\Gamma}}{\Gamma + \sqrt{\gamma\Gamma}}\]
\end{proof}

\subsubsection{Damped Jacobi}
For a suitable \(\omega\) the condition \(0 < 2D - A\) in \cref{cor:jacobi_conv} becomes obsolete.

\begin{definition}[Damped Jacobi Method]
   \[x^{(k+1)} = Kx^{(k)} + D^{-1}b \qquad\text{where}\qquad K := (I_n - \omega D^{-1}A)\]
   \[x^{(k+1)} = x^{(k)} - N(Ax^{(k)} - b) \qquad\text{where}\qquad N := \omega D^{-1}\]
\end{definition}

\begin{theorem}[Damped Jacobi Convergence]
   Given \(A\) positive definite, the damped Jacobi method converges for
   \[\omega \in \left(0; \frac{2}{\Lambda}\right) \qquad\text{with}\qquad \Lambda := \|D^{-\frac{1}{2}}AD^{-\frac{1}{2}}\| = \rho(D^{-1}A)\]
\end{theorem}
\begin{remark}
   The condition above can equivalently formulated as
   \[0 < \omega A < 2D\]
\end{remark}
\begin{proof}
   The damped Jacobi iteration is given through
   \[x^{(k+1)} = x^{(k)} - \omega D^{-1}(Ax^{(k)} - b)\]
   With the notation of the proof above we see \(N = \omega D^{-1}\), then is the condition from before
   \begin{equation*}
      \begin{split}
         & 0 < A < 2N^{-1} \iff 0 < A < \frac{2}{\omega}D \iff 0 < \omega A < 2D \iff 0 < \omega D^{-\frac{1}{2}}AD^{-\frac{1}{2}} < 2I_n \iff \\
         & \iff 0 < \omega \rho\Big(D^{-\frac{1}{2}}AD^{-\frac{1}{2}}\Big) < 2 \iff 0 < \omega \|D^{-\frac{1}{2}}AD^{-\frac{1}{2}}\| < 2 \iff 0 < \omega \rho(D^{-1}A) < 2
      \end{split}
   \end{equation*}
\end{proof}

\section{Matlab Cheatsheet}
% TODO: matlab cheatsheet
Print defaul: \lstinline|format short|

Print long: \lstinline|format long|

Print fractions: \lstinline|format rat|

Inverse of a matrix: \lstinline|inv(A)|

Calculate Derivatives: \lstinline|syms x| and \lstinline|diff(sin(5*x), 3)|

Calculate Integral: \lstinline|syms x| and \lstinline|int(x^2, x, [0 1])|

Solve equation: \lstinline|syms x| and \lstinline|solve(a*x^2 + b*x + c == 0, x)|

Maximum of function: \lstinline|max(arrayfun(@(x)f(x), linspace(-1, 1, 10000)))|
