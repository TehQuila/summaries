\section{Matrices}
\subsection{Definition \& Terminology}
\begin{definition}[Matrix]
   Given \(m, n \in \mathbb{N}_{>0}\). An \(m \times n\) \emph{matrix} consists of \(m\) rows and \(n\) columns, thus \(m \cdot n\) numbers, arranged in a rectangle:
   \[
   A_{m,n} = (a_{ij}) =
   \bordermatrix{
      ~ & & & & j & \cr
        & a_{11} & a_{12} & \cdots & \cdots & a_{1n} \cr
        & a_{21} & a_{22} & \cdots & \cdots & a_{1n} \cr
      i & \vdots & \vdots & \ddots & a_{ij} & \vdots \cr
        & \vdots & \vdots & \vdots & \ddots & \vdots \cr
        & a_{m1} & \cdots & \cdots & \cdots & a_{mn} \cr
   }_{\substack{1 \leq i \leq m \\ 1 \leq j \leq n}}
   \]
   The matrix-entries are written as \(a_{ij}\) where \(i\) is the row index with \(1 \leq i \leq m\) and \(j\) is the column index with \(1 \leq j \leq n\).
\end{definition}
\begin{remark}[Notation]
   Given a field \(K\) we say \(\Mat_{m,n}(K)\) is the set of all matrices whose coefficients are in \(K\).
   \[A = (a_{ij}) \in \Mat_{m,n}(K): a_{ij} \in K\]
\end{remark}

\subsubsection{Matrix Shapes}
\begin{definition}[Quadratic Matrix]
   An \(m \times n\) matrix where \(m = n\).
\end{definition}

\begin{definition}[Diagonal Matrix]
   A quadratic matrix where all entries \(a_{ij} \in K\) with \(i \neq j\) are \(0\)
\end{definition}

\begin{definition}[Triangular Matrix]\label{def:triangular_matrix}
   Given \(A_{m,n} = (a_{ij})_{\substack{1 \leq i \leq m \\ 1 \leq j \leq n}}\) we say

   \begin{enumerate}[label=\roman*, align=Center]
      \item \(A\) is an \emph{upper} triangular matrix if \(\forall i > j: a_{ij} = 0\)
      \item \(A\) is an \emph{lower} triangular matrix if \(\forall i < j: a_{ij} = 0\)
   \end{enumerate}
\end{definition}

\begin{proposition}[Triangular Matrix Operations]
   Let \(A\) and \(B\) be upper triangular matrices, then
   \begin{enumerate}[label=\roman*, align=Center]
      \item \(A + B\) is upper triangular.
      \item \(A \cdot B\) is upper triangular.
   \end{enumerate}
\end{proposition}

\subsubsection{Important Matrices}
\begin{definition}[Zero Matrix]
   An \(m \times n \) matrix whose entries are all \(0\).
\end{definition}

\begin{definition}[Identity Matrix]
   A diagonal matrix whose entries are 1.
   \[I_m := (\delta_{ij})\]
\end{definition}
\begin{remark}
   \(\delta_{ij}\) is the Kronecker Delta
   \[\delta_{ij} := \begin{cases} 1: & i=j\\ 0: & i \neq j\end{cases}\]
\end{remark}

\subsection{Matrix Operations}
\begin{definition}[Matrix Addition]
   Given \(A, B \in \Mat_{m,n}(K)\)
   \[A + B := (a_{ij}) + (b_{Ã­j}) = (a_{ij} + b_{ij})\]
   \[A - B := A + (-1) \cdot B = (a_{ij}) + (-1)(b_{ij}) = (a_{ij} - b_{ij})\]
\end{definition}
\begin{remark}
   The addition is only defined for equally sized matrices.
\end{remark}

\begin{proposition}[Addition Rules]
   Let \(A, B, C \in \Mat_{m,n}(K)\)
   \begin{enumerate}[label=\roman*, align=Center]
      \item \(0_{m,n} + A = A\)
      \item \(A - A = 0_{m,n}\)
      \item \(A + (B + C) = (A + B) + C\)
   \end{enumerate}
\end{proposition}

\begin{definition}[Scalar Multiplication]
   Given \(A \in \Mat_{m,n}(K)\) and \(\lambda \in K\)
   \[\lambda \cdot A := \lambda (a_{ij}) = (\lambda a_{ij})\]
\end{definition}

\begin{proposition}[Scalar Rules]
   Let \(\lambda, \mu \in K\) and \(A, B \in \Mat_{m,n}(K)\), then
   \begin{enumerate}[label=\roman*, align=Center]
      \item \(\lambda (A + B) = \lambda A + \lambda B\)
      \item \((\lambda + \mu) A = \lambda A + \mu A\)
      \item \(\lambda (\mu A) = (\lambda \cdot \mu) A\)
   \end{enumerate}
\end{proposition}

\begin{definition}[Matrix Multiplication]\label{def:matrix_mult}
   Given \(A \in \Mat_{m,n}(K)\) and \(B \in \Mat_{n,l}(K)\)
   \[A_{m,n} \cdot B_{n,l} := \left(\sum_{j=1}^n (a_{ij} \cdot b_{jk})\right)_{\substack{1 \leq i \leq m \\ 1 \leq k \leq l}}\]
\end{definition}
\begin{remark}
   This means \(A\) and \(B\) can only be multiplied if \(A\) has \(n\) columns and \(B\) has \(n\) rows:
   \[A \in \Mat_{m,n}, B \in \Mat_{n,l} \implies A \cdot B \in \Mat_{m,l}(K)\]
\end{remark}

\begin{proposition}[Multiplication Rules]
   Let \(A, B, C \in \Mat_{m,n}(K)\)
   \begin{enumerate}[label=\roman*, align=Center]
      \item \(I_m \cdot A = A \cdot I_m = A\)
      \item \(0_{m,n} \cdot A = 0_{m,n}\)
      \item \(A \cdot (B \cdot C) = (A \cdot B) \cdot C\)
   \end{enumerate}
\end{proposition}

\subsection{Invertible Matrices}
\begin{definition}[Invertible Matrix]\label{def:invertible_mat}
   \(A \in \Mat_n(K)\) where
   \[\exists B \in \Mat_n(K): A \cdot B = B \cdot A = I_n\]
\end{definition}
\begin{remark}
   For an invertible matrix \(A\) holds \(\det(A) \neq 0\).
\end{remark}

\begin{proposition}[Invertible Matrix Properties]\label{pro:invertible_mat}
   Given \(A \in \Mat_n(K)\), the following statements are equivalent.
   \begin{enumerate}[label=\roman*, align=Center]
      \item \(A \in \GL_n(K)\)
      \item \(A\) is invertible.
      \item \(\rk(A) = n\)
      \item \(A\) is equivalent to \(I_n\).
      \item \(A\) is the product of finite elementary matrices.
   \end{enumerate}
\end{proposition}

From \Cref{pro:invertible_mat} (v) follows
\begin{proposition}
   Let \(A, B\) be invertible with inverse \(A'\)
   \begin{enumerate}[label=\roman*, align=Center]
      \item \(A^{-1} := A'\) is unique.
      \item \((A^{-1})^{-1} = A\)
      \item \(A \cdot B \in \GL_n(K)\) and \(B \cdot A \in \GL_n(K)\)
      \item \((A \cdot B)^{-1} = B^{-1} \cdot A^{-1}\)
   \end{enumerate}
\end{proposition}
\begin{remark}
   This characterizes the \emph{general linear group}, c.f. \cref{sssec:gen_lin_grp}.
\end{remark}

\begin{remark}[Inverse of \(2 \times 2\) Matrix]
   \[A^{-1} = \frac{1}{\det(A)} \cdot \adj(A)\]
\end{remark}

\begin{remark}[Gauss-Jordan-Algorithm]
   The inverse of a matrix can be computd as follows.
   \[\left(\begin{array}{ccc|ccc}
      1   & -1 & -2 & 1 & 0 & 0 \\
      -2  & -1 & 1  & 0 & 1 & 0 \\
      1   & 3  & 1  & 0 & 0 & 1
      \end{array}\right) \rightsquigarrow
      \left(\begin{array}{ccc|ccc}
         1 & -1 & -2 & 1 & 0 & 0 \\
         0 & -3 & -3 & 2 & 1 & 0 \\
         0 & 4  & 3  & -1 & 0 & 1
      \end{array}\right) \rightsquigarrow\]
   \[\rightsquigarrow \left(\begin{array}{ccc|ccc}
         1 & -1 & -2 & 1 & 0 & 0 \\
         0 & -3 & -3 & 2 & 1 & 0 \\
         0 & 0  & -1  & \frac{5}{3} & \frac{4}{3} & 1
      \end{array}\right) \rightsquigarrow
      \left(\begin{array}{ccc|ccc}
         1 & -1 & -2 & 1 & 0 & 0 \\
         0 & 1  & 1  & -\frac{2}{3} & -\frac{1}{3} & 0 \\
         0 & 0  & 1  &  -\frac{5}{3} & -\frac{4}{3} & -1
      \end{array}\right) \rightsquigarrow\]
   \[\rightsquigarrow \left(\begin{array}{ccc|ccc}
         1 & 0 & -1  & \frac{1}{3} & -\frac{1}{3} & 0 \\
         0 & 1  & 1  & -\frac{2}{3} & -\frac{1}{3} & 0 \\
         0 & 0  & 1  &  -\frac{5}{3} & -\frac{4}{3} & -1
      \end{array}\right) \rightsquigarrow
      \left(\begin{array}{ccc|ccc}
         1 & 0 & 0 & -\frac{4}{3} & -\frac{5}{3} & -1 \\
         0 & 1 & 0 & 1            & 1            & 1 \\
         0 & 0 & 1 & -\frac{5}{3} & -\frac{4}{3} & -1
      \end{array}\right)\]

   \[\begin{pmatrix}
      1   & -1 & -2 \\
      -2  & -1 & 1  \\
      1   & 3  & 1
   \end{pmatrix}^{-1} = \begin{pmatrix}
      -\frac{4}{3} & -\frac{5}{3} & -1 \\
      1            & 1            & 1 \\
      -\frac{5}{3} & -\frac{4}{3} & -1
   \end{pmatrix}\]
\end{remark}

\subsection{Transposed Matrices}
\begin{definition}[Transposed Matrix]
   Given \(A \in \Mat_{m,n}: A = (a_{ij})_{\substack{1 \leq i \leq m \\ 1 \leq j \leq n}}\)
   \[A^T := (a_{ji})_{\substack{1 \leq j \leq m \\ 1 \leq i \leq n}}\]
\end{definition}
\begin{remark}
   Transposing a matrix means rewriting its rows as columns, therefore is
   \[0_{mn}^T = 0_{nm}\]
   \[I_{m}^T = I_{m}\]
\end{remark}

\begin{proposition}[Calculation Rules]
   Let \(A,B \in \Mat_{m,n}(K)\)
   \begin{enumerate}[label=\roman*, align=Center]
      \item \((A^T)^T = A\)
      \item \((A \cdot B)^T = B^T \cdot A^T\)
      \item \((A + B)^T = A^T + B^T\)
      \item \((\lambda \cdot A)^T = \lambda \cdot A^T\)
      \item \((A^T)^{-1} = (A^{-1})^T\)
   \end{enumerate}
\end{proposition}

\begin{definition}[Hermitian Transpose]
   Given \(A \in \GL_n(\mathbb{C})\)
   \[A^H := \overline{A^T}\]
\end{definition}

\begin{definition}[Symmetric Matrix]
   Given \(A \in \Mat_n(K)\) where
   \[A = A^T\]
\end{definition}

\begin{definition}[Skew-Symmetric Matrix]
   Given \(A \in \Mat_n(K)\) where
   \[A^T = -A\]
\end{definition}

\begin{definition}[Orthogonal Matrix]
   A matrix \(A \in \GL_n(\mathbb{R})\) for which holds
   \[A^T \cdot A = A \cdot A^T = I_n\]
\end{definition}
\begin{remark}[Intuition]
   \(A^{-1} = A^T\).
\end{remark}
\begin{remark}
   Since \(A \in \OG(n)\) represents a distance preserving transformation we know that \(\det(A) \in \{\pm 1\}\).
\end{remark}

\begin{definition}[Unitary Matrix]
   A matrix \(A \in \GL_n(\mathbb{C})\) where
   \[A^H \cdot A = A \cdot A^H = I_n\]
\end{definition}

\subsection{Matrix Relations}
\begin{definition}[Similar Matrices]\label{def:sim_mat}
   \(A, B \in \Mat_m(K)\) are similar if
   \[\exists S \in \GL_n(K): S \cdot A \cdot S^{-1} = B\]
\end{definition}
\begin{remark}
   As with the equivalence of matrices ''\(A\) is similar to \(B\)`` is also an equivalence relation.
\end{remark}

\begin{definition}[Equivalent Matrices]\label{def:equiv_mat}
   Two matrices \(A, B \in \Mat_{m,n}(K)\) are equivalent if
   \[\exists S \in \GL_m(K), T \in \GL_n(K): S \cdot A \cdot T = B\]
\end{definition}
\begin{remark}
   We can use this definition as an equivalence relation.
   \[A \sim B :\iff SAT = B\]
      %TODO: f is a linear map of K-vector spaces
   This means that two to \(f\) corresponding matrices are equivalent and the set of all to \(f\) corresponding matrices is an equivalence class of matrices.
\end{remark}

\begin{definition}[Congruent Matrices]\label{def:congr_mat}
   \(A, B \in \Mat_n(K)\) are congruent if
   \[\exists U \in \GL_n(K): A = U^T\cdot B\cdot U\]
\end{definition}

\subsection{Row \& Column Space}
\begin{definition}[Row Space]
   Given \(A \in \Mat_{m,n}(K)\) with rows \(r_1, \ldots, r_n\)
   \[\ZR(A) := \spanv_K(r_1, \ldots, r_n)\]
\end{definition}

\begin{definition}[Column Space]\label{def:col_space}
   Given \(A \in \Mat_{m,n}(K)\) with columns \(c_1, \ldots, c_n\)
   \[\SR(A) := \spanv_K(c_1, \ldots, c_n)\]
\end{definition}
\begin{remark}
   We would show that the column rank is constructed exactly as the row rank since we can transpose \(A\) and use it's rows.
\end{remark}

\begin{definition}[Row/Column Rank]\label{def:row_rank}
   Given \(A \in \Mat_{m,n}(K)\)
   \[\rk_{row}(A) := \dim\big(ZR(A)\big) \qquad \rk_{col}(A) := \dim\big(SR(A)\big)\]
\end{definition}

\begin{proposition}[\(S \cdot A \cdot T\) Matrix Rank]\label{pro:SAT}
   Given \(A \in \Mat_{m,n}(K)\) with \(r = \rk(A)\)
   \begin{enumerate}[label=\roman*, align=Center]
      \item \(\exists S \in \GL_m(K), T \in \GL_n(K): S \cdot A \cdot T = \begin{pmatrix}I_r & 0\\ 0 & 0\end{pmatrix}\)
      \item \(\rk_{row}(A) = \rk_{col}(A)\)
   \end{enumerate}
\end{proposition}
\begin{remark}
   Since \(\rk_{row}(A) = \rk_{col}(A)\) we can also write
   \[\rk(A) := \rk_{row}(A)\]
   which is compatible with \cref{def:mat_rank} and the view that \(\rk(A)\) is the number of linear independent columns of \(A\).
   This can further be illustrated through \cref{def:col_space} and \cref{def:row_rank}.
\end{remark}

\begin{corollary}[Matrix Kernel Dimension]\label{cor:dim_ker}
   Given \(A \in \Mat_{m,n}(K)\)
   \[\dim(\ker(A)) = n - \rk(A)\]
\end{corollary}

\subsection{Diagonalization}
\begin{definition}[Diagonalizable Matrix]
   \(A \in \Mat_m(K)\) is diagonizable if it is similar to a diagonal matrix.
\end{definition}

\paragraph{Diagonalization Procedure}
Let \(A \in \Mat_n(K)\) be the matrix that you want to diagonalize (if possible).
\begin{enumerate}
   \item Find the characteristic polynomial \(P_A\) of \(A\).
   \item Find eigenvalues \(\lambda\) of \(A\) and their algebraic multiplicities from \(P_A\).
   \item For each \(\lambda\) of \(A\), find a basis of the eigenspace \(\Eig(A, \lambda)\).\\
      If there is a \(\lambda\) such that the geometric multiplicity of \(\lambda\), is less than the algebraic multiplicity, then \(A\) is not diagonalizable.
   \item If we combine all basis vectors for all eigenspaces, we obtained \(n\) linearly independent eigenvectors \(v_1, v_2, \ldots, v_n\).
   \item Define the nonsingular matrix \(S = [v_1~v_2~\ldots~v_n]\).
   \item Define the diagonal matrix \(D\), whose entry \(D_{i,i}\) is the eigenvalue \(\lambda_i\) such that the i-th column vector \(v_i\) of \(S\) is in \(\Eig(A, \lambda_i)\).
   \item Then \(A\) is diagonalized as \(S^{-1} \cdot A \cdot S = D\).
\end{enumerate}

\begin{example}
   Let us consider \(A = \begin{pmatrix}4 & -3 & -3\\3 & -2 & -3\\-1 & 1 & 2\end{pmatrix}\).
\end{example}

\begin{definition}[Triangularizable Matrix]
   \(A \in \Mat_m(K)\) is triangularizable if it is similar (\ref{def:sim_mat}) to a triangular matrix.
\end{definition}

\subsection{Determinant}
Geometrically, the determinant can be viewed as how much the area/volume was scaled by a linear transformation described by the matrix.
Looking at the trivial linear transformation and the transformation  which scales space by a factor of 2 we have
\[\det(I_n) = 1 \quad\text{and}\quad \det\begin{pmatrix}2&0\\0&2\end{pmatrix} = 4\]

\begin{figure}[h]
   \centering
   \input{drawings/det_scale_id.tex}
   \qquad
   \input{drawings/det_scale_2.tex}
\end{figure}

\subsubsection{Determinant as normalized alternating map}
\begin{definition}[Normalized Multilinear Map]
   \(f: \Mat_n(K) \to K\) is normalized if
   \[f(I_n) = 1\]
\end{definition}

\begin{theorem}[Permutation Determinant]\label{thm:det}
   Let \(K\) be a field and \(n \in \mathbb{N}\), then there exists a unique, normalized, alternating multilinear map
   \[\det: \Mat_n(K) \to K\]
   and for \(A \in \Mat_n(K)\) holds
   \[\det(A) = \sum_{\sigma \in S_n} \sign(\sigma) \cdot a_{1, \sigma(1)} \cdot a_{2, \sigma(2)} \cdot \ldots \cdot a_{n, \sigma(n)}\]
\end{theorem}
\begin{remark}
   For an arbitrary \(k \in K\) maps \(k \cdot \det: I_n \to k\).
\end{remark}
\begin{remark}
   It follows that \(\det(0) = 1\) and \(\det(k) = k\) for some \(k \in K\).
   If \(n = 2\) we have
   \[\det\begin{pmatrix}a&b\\c & d\end{pmatrix} = ad - bc\]
   For \(n = 3\) we get
   \[\det\begin{pmatrix}
      a & b & c\\
      d & e & f\\
      g & h & i
   \end{pmatrix} = aej + bfg + cdh - ceg - afh - bdj\]
   For \(n \geq 4\) this formular is not viable anymore so we use the following proposition.
\end{remark}

\begin{proposition}[Determinant Calculation Rules]\label{pro:determinant_rules}
   For \(A, B \in \Mat_n(K)\) holds
   \begin{enumerate}[label=\roman*, align=Center]
      \item \(\det(A) = \det(A^T)\)
      \item \(\det(A \cdot B) = \det(A) \cdot \det(B)\)
      \item If \(A\) is a diagnoal, upper- or lower triangular matrix \(\det(A) = a_{11} \cdot \ldots \cdot a_{nn}\)
      \item If \(A\) is a diagonal, upper- or lower triangular block matrix \(\det(A) = \det(A_{11}) \cdot \ldots \cdot \det(A_{nn})\)
      \item Given a homomorphism \(\varphi: K \to K'\) and \(A' \in \Mat_n(K')\) is \(\det(A') = \varphi(\det(A))\).
      \item Is \(A'\) created through \(Z_1\) (\ref{def:elementary_row_transformation}) on \(A\), then is \(\det(A') = \lambda \cdot \det(A)\)
      \item Is \(A'\) created through \(Z_2\) (\ref{def:elementary_row_transformation}) on \(A\), then is \(\det(A') = (-1) \cdot \det(A)\)
      \item \(\forall k \in K: \det(k \cdot A) = k^n \cdot \det(A)\)
      \item \(\forall \sigma \in S_n: \det(P_\sigma) = \sign(\sigma)\)
   \end{enumerate}
\end{proposition}
\begin{remark}
   It also holds that
   \[A~\text{is invertible} \iff \det(A) \in K^\times\]
   Further holds
   \[\det(A^{-1}) = \big(\det(A)\big)^{-1} = \frac{1}{\det(A)}\]
   since \(\det(A \cdot A^{-1}) = \det(I_n) = 1\) and \(\det(A \cdot A^{-1}) = \det(A) \cdot \det(A^{-1}) = 1\)
\end{remark}

\begin{corollary}
   Let \(A \in \Mat_n(K)\) and \(B \in \GL_n(K)\), then is
   \[\det(B \cdot A \cdot B^{-1}) = \det(A)\]
\end{corollary}

\begin{definition}[Determinant of Endomorphism]
   Given \(f \in \End_K(V)\) of a finite-dimensional K-vector space \(V\) where \(\dim(V) = n\).
   \[\det(f) := \det(A)\]
\end{definition}
\begin{remark}
   Given a basis of \(V\) we can choose a different basis which is related to the original through \(S \in GL_n(K)\).
   The corresponding matrix of \(f\) in the new basis is \(SAS^{-1}\).
   From the previous corollary follows that \(\det(f)\) is well-defined and so
   \[\ker(f) = 0 \iff \im(f) = V \iff f \in \text{GL}(V) \iff \det(f) \neq 0\]
   where \(GL(V)\) denotes the group of all endomorphisms of \(V\).
\end{remark}

\begin{definition}[Minor]
   Given \(A \in \Mat_{m,n}(K)\), the determinants of matrices \((a_{ij})_{i \in I, j \in J}\) where \(I \subset \{1, \ldots, m\}\) and \(J \subset \{1, \ldots, n\}\) are the minors of order \(k = |I| = |J|\).
\end{definition}
\begin{remark}
   In other words, a minor of a matrix \(A\) is the determinant of some smaller square matrix, cut down from \(A\) by removing one or more of its rows and columns.
   The number of rows/columns of the respective submatrix is the \textit{order} of the minor.
\end{remark}
\begin{example}
   Let \(A \in \Mat_n(K)\), and \(r, c \in \{1, \ldots, n\}\) we define
   \begin{equation}\label{eq:sliced_mat}
      A'_{rc} := (a_{ij})_{i \in I, j \in J} \quad\text{for}\quad I := \{1, \ldots, n\} \setminus \{r\} \quad\text{and}\quad J := \{1, \ldots, n\} \setminus \{c\}
   \end{equation}
   Now let \(\), the minors of the \((n-1)\)th order of \(A\) is \(\det(A'_{rc})~\forall r, c \in [1;n]\).
   \[A = \begin{pmatrix}a&b&c\\d&e&f\\g&h&i\end{pmatrix} \quad\rightsquigarrow\quad A_{11}' = \begin{pmatrix}e&f\\h&i\end{pmatrix} \qquad A_{22}' = \begin{pmatrix}a&c\\g&i\end{pmatrix} \qquad A_{23}' = \begin{pmatrix}a&b\\g&h\end{pmatrix}\]
\end{example}

\begin{proposition}[Laplace's Formula]\label{pro:laplace}
   Let \(A \in \Mat_n(K)\) and \(A'_{rc}\) as in \cref{eq:sliced_mat} for \(r, c \in \{1, \ldots, n\}\).
   \[\det(A) = \sum_{i=1}^n (-1)^{r+i} \cdot a_{ri} \cdot \det(A'_{ri}) \quad\text{expansion of row}~r\]
   \[\det(A) = \sum_{i=1}^n (-1)^{i+c} \cdot a_{ic} \cdot \det(A'_{ic}) \quad\text{expansion of column}~c\]
\end{proposition}
\begin{example}
   Let \(A = \begin{pmatrix}2&3&4&5\\0&1&2&3\\2&5&7&9\\0&0&1&1\end{pmatrix}\).
   We do laplace expansion over row \(c = 4\) since it has the most 0s.
   \begin{equation*}
      \begin{split}
         \det(A) & = (-1)^{4+1} \cdot a_{41} \cdot \det(A_{41}') + (-1)^{4+2} \cdot a_{42} \cdot \det(A_{42}')\\
                 & + (-1)^{4+3} \cdot a_{43} \cdot \det(A_{43}') + (-1)^{4+4} \cdot a_{44} \cdot \det(A_{44}')\\
                 & = (-1) \cdot 0 \cdot \det(A_{41}') + 1 \cdot 0 \cdot \det(A_{42}') + (-1) \cdot 1 \det(A_{43}') + 1 \cdot 1 \cdot \det(A_{44}')\\
                 & = (-1) \cdot \det(A_{43}') + \det(A_{44}') = (-1) \cdot \det\begin{pmatrix}2&3&5\\0&1&3\\2&5&9\end{pmatrix} + \det\begin{pmatrix}2&3&4\\0&1&2\\2&5&7\end{pmatrix}\\
                 & = (-1)(-4) + (-2) = 2
      \end{split}
   \end{equation*}
\end{example}

\begin{definition}[Adjugate Matrix]
   Given \(A \in \Mat_n(K)\)
   \[\adj(A) := \Big((-1)^{i+j} \cdot \det(A_{ji}')\Big)_{1 \leq i, j \leq n}\]
\end{definition}
\begin{example}
   Let \(A = \begin{pmatrix}a&b\\c&d\end{pmatrix}\), then is
   \[\adj(A) = \begin{pmatrix}d&-b\\-c&a\end{pmatrix}\]
\end{example}

\begin{corollary}[Adjugate Calculation Rule]\label{cor:adj}
   Let \(A \in \Mat_n(K)\) then is
   \[\adj(A) \cdot A = A \cdot \adj(A) = \det(A) \cdot I_n\]
\end{corollary}

\begin{corollary}[Adjugate Properties]
   Let \(A \in \Mat_n(K)\), the following statements are equivalent
   \begin{enumerate}[label=\roman*, align=Center]
      \item \(A \in \GL_n(K)\)
      \item \(\exists B \in \Mat_n(K): B \cdot A = I_n\)
      \item \(\exists C \in \Mat_n(K): A \cdot C = I_n\)
      \item \(\det(A) \in K^\times \implies (\det(A))^{-1} \cdot \adj(A) \cdot A = A \cdot (\det(A))^{-1} \cdot \adj(A) = I_n\)
   \end{enumerate}
\end{corollary}

\begin{proposition}[Vandermonde Determinant]
   Let \(x_1, x_2, \ldots, x_n \in K\), then is
   \[\det\begin{pmatrix}
         1 & x_1 & (x_1)^2 & \cdots & (x_1)^{n-1}\\
         1 & x_2 & (x_2)^2 & \cdots & (x_2)^{n-1}\\
         \vdots & \vdots & \vdots & \vdots & \vdots\\
         1 & x_n & (x_n)^2 & \cdots & (x_n)^{n-1}
   \end{pmatrix} = \prod_{1 \leq i < j \leq n} (x_j - x_i)\]
\end{proposition}

\subsubsection{Orientation}
The determinant is positive or negative according to whether the linear mapping preserves or reverses the \emph{orientation} of space.
So if we look at the trivial linear transformation and the transformation which ''flips`` space along he \(y\)-axis we have
\[\det(I_n) = 1 \quad\text{and}\quad \det\begin{pmatrix}-1&0\\0&1\end{pmatrix} = -1\]

\begin{figure}[h]
   \centering
   \input{drawings/det_orient_id.tex}
   \qquad
   \input{drawings/det_orient_flip.tex}
\end{figure}

\begin{definition}[Path]
   Given \(I = [0;1]\) and \(S \subset \mathbb{R}^n\).
   \[f: I \to S~\text{continuous}\]
   where the \textit{initial point} of the path is \(f(0)\) and the \textit{terminal point} is \(f(1)\).
\end{definition}

\begin{definition}[Path-Component]
   Given an equivalence relation on \(\mathbb{R}^n\)
   \[x \sim y :\iff \exists~\text{path from}~x~\text{to}~y\]
   An equivalence class is a \textit{path-component}.
\end{definition}
\begin{remark}
   The space \(\mathbb{R}^n\) is said to be \textit{path-connected} if there is exactly one path-component, i.e. if there is a path joining any two points in \(\mathbb{R}^n\).
\end{remark}

\begin{proposition}[General Linear Group Partition]
   \(\forall n \in \mathbb{N}_{>0}\) consists \(\GL_n(\mathbb{R})\) of two path-components.
   Specifically, two matrices in \(\GL_n(\mathbb{R})\) belong to the same path-component iff their determinants have the same algebraic sign.
\end{proposition}
\begin{remark}
   \(\det: \GL_n(\mathbb{R}) \to \mathbb{R}^\times\) is continuous hence there doesn't exist a path from a matrix with positive determinant to one with a negative determinant.
\end{remark}

\begin{definition}[Orientation]
   Given a finite-dimensional K-vectorspace \(V\), an \emph{orientation} on \(V\) is a path-component of the bases of \(V\).
\end{definition}

\subsection{Eigenvalues \& Eigenvectors}
An \textit{eigenvector} of a linear transformation \(f\) is a non-zero vector that changes by only a scalar factor (namely the \textit{eigenvalue}) when that linear transformation is applied to it.
Differently put, it is a vector which still resides within its original span after a linear transformation.

If the linear transformation is a rotation then is the eigenvector of that transformation actually the axis of rotation (since in a rotation nothing is stretched its eigenvalue is 1).

\begin{definition}[Eigenvector/Eigenvalue]
   Given a K-vector space \(V\) and an endomorphism \(f: V \to V\).
   \(v \in V: v \neq 0\) is an \textit{eigenvector} if
   \[f(v) = \lambda \cdot v\]
   where \(\lambda\) is the \textit{eigenvalue}.
\end{definition}
\begin{remark}
   If \(V\) is finite dimesional we can represent \(f\) with a matrix \(A\) which results in \(A \cdot v = \lambda \cdot v\).
   The first thing we note is that we can write
   \[\lambda \cdot v = (\lambda \cdot I_n) \cdot v\]
   so we can simplify
   \[A \cdot v = \lambda I_n \cdot v \implies A \cdot v - \lambda I_n \cdot v = 0 \implies (A - \lambda I_n) \cdot v = 0\]
   Since we want a non-zero vector \(v\), the only way this is possible is if \(\det(A - \lambda I_n) = 0\).
\end{remark}

\begin{definition}[Eigenspace]
   Given a K-vector space \(V\), an endomorphism \(f: V \to V\) and an eigenvalue \(\lambda \in K\).
   \[\Eig(f, \lambda) := \ker(f - \lambda \cdot \id_V) = \{v \in V \mid f(v) = \lambda \cdot v\}\]
\end{definition}
\begin{remark}
   It is trivial to see that
   \[(\forall \lambda, \mu \in K: \lambda \neq \mu): \Eig(f, \lambda) \cap \Eig(f, \mu) = 0\]
   Since \(v \in \Eig(f, \lambda) \cap \Eig(f, \mu) \implies \lambda v = f(v) = \mu v\).
\end{remark}

\begin{proposition}[Diagonalizable Endomorphism]
   Let \(V\) be a finite dimensional K-vector space and \(f: V \to V\) an endomorphism, then are the following statements equivalent
   \begin{enumerate}[label=\roman*, align=Center]
      \item \(f\) is diagonizable.
      \item There exists a basis of \(V\) consisting of eigenvectors.
      \item \(V = \sum_{\lambda} \Eig(f, \lambda)\)
      \item \(V = \bigoplus_{\lambda} \Eig(f, \lambda)\)
   \end{enumerate}
\end{proposition}

\subsection{Characteristic Polynomial}
\begin{definition}[Characteristic Polynomial]
   Given a finite dimensional K-vector space \(V\) and an endomorphism \(f: V \to V\).
   \[P_f := \det(f - x \cdot \id_V) \in K[x]\]
\end{definition}
\begin{remark}
   Regarding \(f\) as \(A \in \Mat_n(K)\) we define analogously
   \[P_A := \det(A - x \cdot I_n) \in K[x]\]
   since \(A - x I_n \in \Mat_n(K[x]) \implies \det(A - x I_n) \in K[x]\).
\end{remark}

\begin{definition}[Trace of a Matrix]
   Given \(A \in \Mat_n(K)\)
   \[\tr(A) := \sum_{i=1}^n a_{ii}\]
\end{definition}
\begin{remark}
   With the remark above follows that
   \[P_A = x^n - \tr(A)x^{n-1} + \ldots\]
\end{remark}

\begin{proposition}[Endomorphism Eigenvalues]
   Let \(V\) be a finite dimensional K-vector space and \(f: V \to V\) an endomorphism, then the eigenvalues of \(f\) are the roots of the characteristic polynomial \(P_f\).
\end{proposition}
\begin{example}
   Let \(V = \mathbb{R}^2\) and \(f: V \to V\) be a shear represented by \(A = \begin{pmatrix}3&1\\0&2\end{pmatrix}\)
   We want to find its eigenvalues and vectors.
   \[v~\text{is eigenvector} \implies v \neq 0 \land f(v) = \lambda v\]
   \[f(v) = \lambda v \iff Av = \lambda v \iff Av = (\lambda I_2)v \iff A v - (\lambda I_2) v = 0 \iff (A - \lambda I_2) v = 0\]
   \[A - \lambda I_2 = \begin{pmatrix}3&1\\0&2\end{pmatrix} - \begin{pmatrix}\lambda&0\\0&\lambda\end{pmatrix} = \begin{pmatrix}3 - \lambda&1\\0&2 - \lambda\end{pmatrix}\]
   Now we calculate the characteristiv polynomial
   \[P_f = \det\begin{pmatrix}3 - \lambda&1\\0&2 - \lambda\end{pmatrix} = 1 \cdot 0 - (3 -\lambda)(2- \lambda) = (\lambda -3)(2 - \lambda)\]
   the eigenvalues (roots of \(P_f\)) are \(\lambda_1 = 3\) and \(\lambda_2 = 2\).

   Now \(v \in \Eig(f, \lambda) \implies v \in \ker(f - \lambda\id_V) \implies (A - \lambda I_2)v = 0\), so we calculate
   \[\begin{pmatrix}3 - 2&1\\0&2 - 2\end{pmatrix} \begin{pmatrix}v_1\\v_2\end{pmatrix} = 0 \implies v_1 + v_2 = 0 \implies \begin{pmatrix}1\\-1\end{pmatrix} \in \Eig(f, 2)\]
   \[\begin{pmatrix}3 - 3&1\\0&2 - 3\end{pmatrix} \begin{pmatrix}v_1\\v_2\end{pmatrix} = 0 \implies v_1~\text{arbirary}, v_2 = 0 \implies \begin{pmatrix}1\\0\end{pmatrix} \in \Eig(f, 3)\]
\end{example}

\begin{definition}[Algebraic Multiplicity]
   Given an eigenvalue \(\lambda\) and a characteristic polynomial \(P_f\).
   The largest \(k \in \mathbb{N}: (x - \lambda)^k \mid P_f\).
\end{definition}
\begin{definition}[Geometric Multiplicity]
   \[\dim(\Eig(f, \lambda))\]
\end{definition}
\begin{example}
   Let \(A = \begin{pmatrix}1&1\\0&1\end{pmatrix}\), then is the eigenvalue 1 and
   \[P_A = (x - 1)^2 \implies \text{algebraic multiplicity}~2\]
   \[\Eig(A, 1) = \spanv\begin{pmatrix}1\\0\end{pmatrix}\]
\end{example}

\begin{proposition}[\(k \geq \dim(\Eig(f, \lambda))\)]
   Let \(f: V \to V\) be an endomporphism and \(\lambda\) an eigenvalue.

   The geometric multiplicity of \(\lambda\) is at most equal to the algebraic multiplicity.
\end{proposition}

\begin{proposition}[Triagonizable Endomorphism]
   Let \(V\) be a finite dimensional K-vector space.
   \[f: V \to V~\text{is triagonizable} \iff P_f~\text{is reducible in}~K[x]\]
\end{proposition}

\begin{proposition}[Trivial Characteristic Polynomial]
   Let \(A \in \Mat_n(K)\), the following statements are equivalent
   \begin{enumerate}[label=\roman*, align=Center]
      \item \(A\) is similar to a proper upper triangular matrix.
      \item \(P_A = x^n\)
      \item \(\exists m \in \mathbb{N}: A^m = 0\)
      \item \(A^n = 0\)
   \end{enumerate}
\end{proposition}
\begin{remark}
   A proper upper triangular matrix' diagonal is also 0.
   We call a matrix \(A\) for which \(A^m = 0\) \textit{nilpotent}.
\end{remark}

\begin{theorem}[Cayley-Hamilton Theorem]
   Let \(A \in \Mat_n(K)\) with the characteristic polynomial \(P_A\), then holds
   \[P_A(A) = 0\]
\end{theorem}
\begin{remark}
   This theorem states that every square matrix over a commutative ring (such as the real or complex field) satisfies its own characteristic equation.
\end{remark}

\subsection{Commutating Blockmatrices}
For \(A \in \Mat_p(R)\) form elements such as
\[a_dA^d + \ldots + a_1A + a_0I_p\]
for \(d \in \mathbb{N}\) and \(a_0, \ldots, a_d \in R\) a commutative ring \(R[A] \subset \Mat_p(R)\).
We can generalize this to block matrices

\begin{lemma}
   Let \(\Omega \subset \Mat_n(R)\) over a commutative unitary ring \(R\) where
   \[\forall A, B \in \Omega: A \cdot B = B \cdot A\]
   then is the set of all \(R\)-linear combinations \(A_1 \cdot \ldots \cdot A_d\) of \(A_1, \ldots, A_d \in \Omega\)
   \[R[\Omega] \subset \Mat_p(R)\]
    a commutative ring with identity element \(I_n\).
\end{lemma}

\begin{proposition}[Block Determinant]
   Given a commutative unitary ring \(R\), let \(\Omega := \{A_{ij} \in \Mat_p(R)\}\) be a set of pairwise commutating matrices and \(A \in \Mat_k(R[\Omega]) \implies A \in \Mat_{kp}(R)\), then is
   \[\det(A) = \det(\det_b(A))\]
   where \(\det_b: \Mat_k(R[\Omega]) \to \Mat_p(R)\) is the block determinant of \(A\).
\end{proposition}
\begin{example}
   Suppose \(A = \begin{pmatrix}A_{11} & A_{12}\\A_{21} & A_{22}\end{pmatrix}\) then is
   \[\det(A) = \det(\det_b(A)) = \det(A_{11} \cdot A_{22} - A_{12} \cdot A_{21})\]
\end{example}

\subsection{Normalforms}
\subsubsection{Smith-Normalform}
\begin{lemma}[\(a_{11}\) divides Matrix]\label{lem:a_div_mat}
   Let \(A \in \Mat_{m,n}(R)\) over a Euclidean domain \(R\).

   \(A\) can be transformed into
   \[\widetilde{A} := \left(\begin{array}{c|ccc}
      d      & 0 & \cdots &  0 \\
      \hline
      0      &   &        & \\
      \vdots &   & \ast   & \\
      0       &   &        & \\
   \end{array}\right)\]
   where \(d \in R \setminus \{0\}\) divides every entry of the \(\ast\) block.
\end{lemma}

\begin{theorem}[Smith-Normalform]\label{thm:smith-normalform}
   Let \(A \in \Mat_{m, n}(R)\) over a Euclidean domain \(R\), the following holds
   \begin{enumerate}[label=\roman*, align=Center]
      \item \(\exists r \in \mathbb{N}, S \in \GL_m(R), T \in \GL_n(R)\) such that
         \[SAT = \left(\begin{array}{c|c} A_r & 0\\ \hline 0 & 0 \end{array}\right) \quad\text{where}\quad
            A_r := \begin{pmatrix}
               a_1 & 0      & 0 \\
               0   & \ddots & 0 \\
               0   & 0      & a_r \\
         \end{pmatrix}\]
         with \(a_1, \ldots, a_r \in R \setminus \{0\}\) such that \(\forall i: a_i | a_{i+1}\).
      \item \(r \in \mathbb{N}\) is unique and \(a_1, \ldots, a_r\) are unique except for multiplication with units.
   \end{enumerate}
\end{theorem}

We still need some background knowledge to be able to appreciate the conciseness of the following proof.
First of all we need to imagine \(S\) and \(T\) as a sequence of row/column transformations.
We use the knowledge of \cref{lem:a_div_mat} to transform \(A\) as follows
\[A \rightsquigarrow \left(\begin{array}{c|ccc}
         d      & 0 & \cdots & 0 \\
   \hline
         0      &   &        & \\
         \vdots &   & A'     & \\
         \vdots &   &        & \\
\end{array}\right) \rightsquigarrow \left(\begin{array}{cc|ccc}
         a_1    & 0      & \cdots & \cdots & 0 \\
         0      & d      & 0      & \cdots & 0 \\
         \hline
         \vdots & 0      &        &     & \\
         \vdots & \vdots &        & A'' & \\
         0      & 0      &        &     & \\
\end{array}\right) \rightsquigarrow \left(\begin{array}{ccc|cc}
         a_1    & 0      & \cdots & \cdots & 0 \\
         0      & a_2    & 0      & \cdots & 0 \\
         \vdots & 0      & d      & 0  & 0 \\
         \hline
         \vdots & \vdots & 0      &    & \\
         0      & 0      & 0      &    & A''' \\
\end{array}\right) \rightsquigarrow \left(\begin{array}{c|c} A_r & 0\\ \hline 0 & 0 \end{array}\right)\]
Now we describe this algorithmically in the following

\begin{remark}
   The inductive nature of the prove above hints at the \emph{Smith-Algorithm} to determine \(S\) respectively \(T\).
\end{remark}

\begin{example}
   \begin{equation*}
      \begin{split}
         \begin{pmatrix}T & T^2\\ T^3 & T^4+T+1\end{pmatrix} & \rightsquigarrow
         \begin{pmatrix}T & T^2\\ 0 & T+1\end{pmatrix} \rightsquigarrow
         \begin{pmatrix}T & 0 \\ 0 & T+1\end{pmatrix} \rightsquigarrow
         \begin{pmatrix}T & 0 \\ T+1 & T+1\end{pmatrix} \rightsquigarrow
         \begin{pmatrix}T & 0 \\ 1 & T+1\end{pmatrix} \rightsquigarrow \\
         & \rightsquigarrow \begin{pmatrix}1 & T+1 \\ T & 0\end{pmatrix} \rightsquigarrow
         \begin{pmatrix}1 & 0 \\ T & -T^2-T\end{pmatrix} \rightsquigarrow
         \begin{pmatrix}1 & 0 \\ 0 & -T^2-T\end{pmatrix}
      \end{split}
   \end{equation*}
\end{example}

\begin{corollary}
   Every \(A \in \GL_n(R)\) over a Euclidean domain \(R\) is a product of elementary matrices.
\end{corollary}

\subsubsection{Frobenius-Normalform}
\begin{definition}[Companion Matrix]
   Given a normed polynomial \(p(x) = x^n + a_{n-1}x^{n-1} + \ldots + a_1 x_1 + a_0\)
   \[C(p) := \begin{pmatrix}
         0      & 0      & \cdots & 0      & -a_0\\
         1      & 0      & \cdots & 0      & -a_1\\
         0      & 1      & \cdots & 0      & -a_2\\
         \vdots & \vdots & \ddots & \vdots & \vdots\\
         0      & 0      & \cdots & 1      & -a_{n-1}
   \end{pmatrix}\]
\end{definition}
\begin{remark}
   \(p(x)\) is in this case the characteristic polynomial of \(C(p)\).
\end{remark}

\begin{corollary}[Frobenius-Normalform]\label{cor:frobenius}
   Let \(A \in \Mat_n(K)\), then there exists a unique \(r \in \mathbb{N}\) and unique normed polynomials \((p_i)_{i \leq r} \subset K[x]: p_i | p_{i+1} \forall i\) such that \(A\) is similar to
   \[\begin{pmatrix}C(p_1) & & 0\\ & \ddots & \\ 0 & & C(p_r)\end{pmatrix}\]
\end{corollary}

\subsubsection{Jordan-Normalform}
\begin{definition}[Jordan-Block]
   For \(\lambda \in \mathbb{C}\),
   \[J_k(\lambda) := \begin{pmatrix}
         \lambda & 1       & 0      & \cdots  & 0       \\
         0       & \ddots & \ddots &  \ddots & \vdots  \\
         \vdots  & \ddots  & \ddots & \ddots  & 0       \\
         \vdots  &         & \ddots & \ddots & 1       \\
         0       & \cdots  & \cdots & 0       & \lambda \\
   \end{pmatrix} \in \Mat_k(\mathbb{C})\]
\end{definition}

\begin{corollary}[Jordan-Normalform]
   Let \(A \in \Mat_n(K)\) be triagonalizable, then is \(A\) similar to a \emph{Jordan-block}-diagonal-matrix
   \[\begin{pmatrix}J_{n_1}(\lambda_1) & & 0\\ & \ddots & \\ 0 & & J_{n_k}(\lambda_k)\end{pmatrix}\]
   where \(n_1 + \ldots + n_k = n\).
\end{corollary}

\subsection{System of Linear Equations}
\begin{definition}[System of Linear Equations]\label{def:sys_lin_eq}
   Consists of \(m\) equations and \(n\) variables \((x_1, \ldots, x_n)\).
   \[a_{11} x_1 + a_{12} x_2 + \ldots + a_{1n} x_n = b_1\]
   \[a_{21} x_1 + a_{22} x_2 + \ldots + a_{2n} x_n = b_2\]
   \[\vdots\]
   \[a_{m1} x_1 + a_{m2} x_2 + \ldots + a_{mn} x_n = b_m\]
\end{definition}

\begin{definition}[Homogenous System of Linear Equations]
   A system of linear equations where
   \[b_i = 0\]
\end{definition}

\subsubsection{Matrix Representation}
The \textit{i}-th equation of a system of linear equations can be written as
\[\left(\displaystyle\sum_{j=1}^n a_{ij} x_j = b_{ij}\right)_{1 \leq i \leq m}\]
which reveals the structure of a matrix multiplication (\ref{def:matrix_mult})
\[A_{m,n} \cdot x_{1,n} = b_{1,m}\]
\[\begin{pmatrix} a_{11} x_1 + & \ldots & + a_{1n} x_n \\ \vdots & \ddots & \vdots \\ a_{m1} x_1 + & \ldots & + a_{mn} x_n\end{pmatrix} = \begin{pmatrix} b_1 \\ \vdots \\ b_m \end{pmatrix}\]

\begin{definition}[Coefficient Matrix]
   A matrix consisting of the coefficients of a system equations
   \[A_{m,n} := \begin{pmatrix} a_{11} & \cdots & a_{1n} \\ \vdots & \ddots & \vdots \\ a_{m1} & \cdots & a_{mn}\end{pmatrix}\]
\end{definition}

\begin{definition}[Extended Coefficient Matrix]\label{def:extended_coefficient_matrix}
   A coefficient matrix with the column vector of the solutions
   \[(A, b) := \begin{pmatrix} a_{11} & \cdots & a_{1n} & b_1 \\ \vdots & \ddots & \vdots & \vdots \\ a_{m1} & \cdots & a_{mn} & b_m\end{pmatrix}\]
\end{definition}

\begin{definition}[Solution Set]
   The solutions of a system of linear equations (\ref{def:sys_lin_eq}) is the set
   \[L(A, b) := \{x \in K^n \mid A \cdot x = b\} \subset K^n\]
\end{definition}
\begin{remark}
   Remind you that \(\ker(A) = \ker(f)\).
   In a homogenous system of linear equations, the solution set is the set of all vectors, which multiplied with \(A\) result in the zero vector
   This set is equivalent to all the elements which \(f\) maps mapped to 0.
   This means that the \(L(A, b) = \ker(A) = \ker(f)\).

   If \(L(A, b) = \emptyset \implies b \not\in \im(f)\).
   \[L(A, b) \neq \emptyset \implies \exists x \in L(A, b) \implies L(A, b) = x + \ker(A)\]

   Also important is that \(\ker(A)\) is again a vector subspace.
   \[A \cdot x = b \implies A \cdot (x - x') = A \cdot x - A \cdot x' = b - b = 0 \implies x - x' \in \ker(A) \implies A \cdot x = A \cdot x' = b\]
   because both \(x\) and \(x'\) are solutions to the same equation system are both contained in the solution set, which is a vector subspace.
\end{remark}

\subsubsection{Elementary Row Transformations}
\begin{definition}[Single-Entry Matrix]
   A quadratic matrix whose entry in the \textit{i}-th row and the \textit{j}-th column is 1 and all other entries are 0.
   \[E_{ij} := (\delta_{ii'} \cdot \delta_{jj'})_{\substack{1 \leq i' \leq n \\ 1 \leq j' \leq m}}\]
\end{definition}
\begin{remark}
   This means that the entry \(a_{i'j'}\) of the matrix \(E_{ij}\) is given by \(\delta_{ii'} \cdot \delta_{jj'}\) so we see that this product is 1 if and only if \(i = i' \land j = j'\) otherwise it is 0.
   This means that exactly one entry of \(E_{ij}\) is nonzero, namely the one at row \(i\) and column \(j\).
\end{remark}

\begin{definition}[Elementary Row Transformations]\label{def:elementary_row_transformation}
   Given \(A \in \Mat_{m, n}(K)\) and \(\lambda \in K^*\), there are three transformations

   \(Z_1\): Add the \(\lambda\) multiple of \(A_{j,n}\) to the \textit{i}-th row of \(A\).

   \(Z_2\): Multiply the \textit{i}-th row of \(A\) with \(\lambda\).

   \(Z_3\): Switch the \textit{i}-th and the \textit{j}-th row of \(A\).
\end{definition}

\begin{definition}[Elementary Matrix]\label{def:elementary_matrix}
   A matrix \(\in \GL_m(K)\) which differs from the identity matrix by one single elementary row operation
   \[\text{For }Z_1: Q_{ij}(\lambda) := I_m + \lambda E_{ij} = \bordermatrix{
      ~ &   &   &   &   &   &   \cr
        & 1 & 0 & 0 & 0 & 0 & 0 \cr
        & 0 & \ddots & 0 & 0 & 0 & 0 \cr
      i & 0 & 0 & 1 & 0 & \lambda & 0 \cr
        & 0 & 0 & 0 & 1 & 0 & 0 \cr
      j & 0 & 0 & 0 & 0 & 1 & 0 \cr
        & 0 & 0 & 0 & 0 & 0 & 1
   }\]
   \[\text{For }Z_2: S_i(\lambda) := I_m + (\lambda - 1) E_{ii} = \bordermatrix{
      ~ &   &   &   &   &   &   \cr
        & 1 & 0 & 0 & 0 & 0 & 0 \cr
        & 0 & \ddots & 0 & 0 & 0 & 0 \cr
        & 0 & 0 & 1 & 0 & 0 & 0 \cr
      i & 0 & 0 & 0 & \lambda & 0 & 0 \cr
        & 0 & 0 & 0 & 0 & 1 & 0 \cr
        & 0 & 0 & 0 & 0 & 0 & 1
   }\]
   \[\text{For }Z_3: P_{ji} := \bordermatrix{
      ~ &   &   &   &   &   &   \cr
        & 1 & 0 & 0 & 0 & 0 & 0 \cr
        & 0 & \ddots & 0 & 0 & 0 & 0 \cr
      i & 0 & 0 & 0 & 0 & 1 & 0 \cr
        & 0 & 0 & 0 & 1 & 0 & 0 \cr
      j & 0 & 0 & 1 & 0 & 0 & 0 \cr
        & 0 & 0 & 0 & 0 & 0 & 1
   }\]
\end{definition}
\begin{remark}
   To add the \(\lambda\) multiple of the \textit{j}-th row of a matrix \(A\) to its \textit{i}-th row for example we would
   \[A' = Q_{ij}(\lambda) \cdot A\]
   Which is exactly the same as \(Z_1\) of \cref{def:elementary_row_transformation}.
   If we would swap the matrix multiplication as follows
   \[A' = A \cdot Q_{ij}(\lambda)\]
   we didn't do a row- but rather a \textit{column} transformation

   In other words: lefthand multiplication results in a row transformation whereas righthand multiplication with an elementary matrix results a column transformation. (Keep in mind that for a righthand multiplication with \(P_{ji}\) its indices are swapped!)
\end{remark}
\begin{example}
   Let
   \[A = \begin{pmatrix}
         1 & 1 &  2 \\
         0 & 2 & -1 \\
         1 & 5 & 0 \\
      \end{pmatrix}
   \]
   we would bring \(A\) in row echelon as follows
   \[\begin{pmatrix}
         1 & 1 &  2 \\
         0 & 2 & -1 \\
         1 & 5 & 0 \\
      \end{pmatrix} \rightsquigarrow \begin{pmatrix}
         1 & 1 &  2 \\
         0 & 2 & -1 \\
         0 & 4 & -2 \\
      \end{pmatrix} \rightsquigarrow \begin{pmatrix}
         1 & 1 &  2 \\
         0 & 2 & -1 \\
         0 & 0 & 0 \\
      \end{pmatrix}
   \]
   The first step was \(r_3 - r_1\) so we can construct an elementary matrix where \(a_{1,3} = -1\)
   \[\begin{pmatrix}
         1  & 0 & 0 \\
         0  & 1 & 0 \\
         -1 & 0 & 1 \\
      \end{pmatrix} \cdot \begin{pmatrix}
         1 & 1 &  2 \\
         0 & 2 & -1 \\
         1 & 5 & 0 \\
      \end{pmatrix} = \begin{pmatrix}
         1 & 1 &  2 \\
         0 & 2 & -1 \\
         0 & 4 & -2 \\
      \end{pmatrix}
   \]
   then we do \(r_3 - 2r_2\) so we can construct an elementary matrix where \(a_{2,3} = -2\)
   \[\begin{pmatrix}
         1 & 0  & 0 \\
         0 & 1  & 0 \\
         0 & -2 & 1 \\
     \end{pmatrix} \cdot
         \begin{pmatrix}
         1 & 1 &  2 \\
         0 & 2 & -1 \\
         0 & 4 & -2 \\
      \end{pmatrix} = \begin{pmatrix}
         1 & 1 &  2 \\
         0 & 2 & -1 \\
         0 & 0 & 0 \\
      \end{pmatrix}
   \]
   and so we see that
   \[\begin{pmatrix}
         1  & 0 & 0 \\
         0  & 1 & 0 \\
         -1 & 0 & 1 \\
      \end{pmatrix} \cdot \begin{pmatrix}
         1 & 0  & 0 \\
         0 & 1  & 0 \\
         0 & -2 & 1 \\
     \end{pmatrix}  \cdot \begin{pmatrix}
         1 & 1 &  2 \\
         0 & 2 & -1 \\
         1 & 5 & 0 \\
      \end{pmatrix} = \begin{pmatrix}
         1 & 1 &  2 \\
         0 & 2 & -1 \\
         0 & 0 & 0 \\
      \end{pmatrix}
   \]
   now we that \(A\) is in row echelon form we can also determine its kernel when we regard it as homogenous system of linear equations
   \[0z = 0\]
   \[2y - z = 0 \implies y = \frac{z}{2}\]
   \[x + y + 2z = 0 \implies x + \frac{z}{2} + 2z = 0 \implies x = -\frac{5z}{2}\]

   \[\ker(A) = \left\{\left(-\frac{5z}{2}, \frac{z}{2}, z\right): z \in \mathbb{Q}\right\}\]
\end{example}

\subsubsection{Gaussian Elimination}
\begin{definition}[Row Echelon Form]\label{def:row_echelon}
   \(A \in \Mat_{m,n}(K)\) is in row-echelon form if,

   \begin{enumerate}[label=\roman*, align=Center]
      \item \(\exists r \in \mathbb{N}: 0 \leq r \leq m\) where the rows 1 to \(r\) are non-zero and \(r+1\) to \(m\) are.
      \item \(\forall 1 \leq i \leq r\) we regard the column index \(j_i := \min\{j \mid a_{ij} \neq 0\}\), it has to holds that
         \[1 \leq j_1 < j_2 < \ldots < j_r \leq n\]
   \end{enumerate}
\end{definition}
\begin{remark}
   We call the column indices \textit{pivots}.
\end{remark}
\begin{remark}
   We can also define a measurement of how far \(A\) is in row echelon form.
   \[s(A) := s \in \mathbb{N}_{\leq m}\]
   The first \(s\) rows of \(A\) are in row echelon form.
   If \(1 \leq s < m \implies r = s\) and \(\forall s < i \leq m, 1 \leq j \leq p_s: a_{ij} = 0\).
\end{remark}

\begin{definition}[Rank of a Matrix]\label{def:mat_rank}
   Let \(A \in \Mat_{m,n}(K)\) be a matrix in row echelon form with \(r\) pivots.
   \[\rk(A) := r\]
\end{definition}
\begin{remark}
   Differently put is the rank of a matrix its number of linear independent columns.
\end{remark}

\begin{theorem}[Gauss Elimination]
   Every matrix \(A\) can be transformed through finite many elementary row transformations into a matrix \(\tilde{A}\) in row echelon form.
\end{theorem}
% TODO
\begin{example}
   Given the following system of linear equations
   \[x - y - 2z = 1\]
   \[-2x - y + z = 0\]
   \[x + 2y + z = -1\]
   we have
   \[A = \begin{pmatrix}
         1  & -1 & -2 \\
         -2 & -1 &  1 \\
         1  &  2 &  1 \\
      \end{pmatrix}
      b = \begin{pmatrix} 1 \\ 0 \\ -1\end{pmatrix} \in \Mat_{m,n}(\mathbb{Q})
   \]
   we write it in the extended coefficient matrix and bring it in row echelon form with the gauss elimination
   \[\begin{pmatrix}
         1  & -1 & -2 &  1 \\
         -2 & -1 &  1 &  0 \\
         1  &  2 &  1 & -1 \\
   \end{pmatrix} \rightsquigarrow \begin{pmatrix}
         1  & -1 & -2 &  1 \\
         0 & -3 &  -3 &  2 \\
         1  &  2 &  1 & -1 \\
   \end{pmatrix} \rightsquigarrow \begin{pmatrix}
         1 & -1 & -2 &  1 \\
         0 & -3 & -3 &  2 \\
         0 &  3 &  3 & -2 \\
   \end{pmatrix} \rightsquigarrow \begin{pmatrix}
         1 & -1 & -2 & 1 \\
         0 & -3 & -3 & 2 \\
         0 &  0 &  0 & 0 \\
   \end{pmatrix}\]
   First \(r_2 + 2 r_1\), then \(r_3 - r_1\) and last \(r_3 + r_2\).
   Now we rewrite this row echelon form back to a system of equations
   \[x - y - 2z = 1\]
   \[-3y - 3z = 2\]
   \[0x + 0y + 0z = 0\]
   if there is a 0 row there are two possible cases
   \[\forall i: b_i = 0 \implies L(A, b) = K^n\]
   \[\exists i > r: b_i \neq 0 \implies L(A, b) = \emptyset\]
   we see that the last equation is true for any value of any of the variables.
   We use this to express the solution set of those equations regarding an arbitrary \(z\)
   \[-3y - 3z = 2 \implies y = -\frac{2 + 3z}{3}\]
   \[x - \left(-\frac{2 + 3z}{3}\right) - 2z = 1 \implies x + \frac{2 + 3z - 6z}{3} = 1 \implies x = 1 - \frac{2 - 3z}{3}\]

   \[L(A, b) = \left\{\left(1 - \frac{2 - 3z}{3}, -\frac{2 + 3z}{3}, z\right): z \in \mathbb{Q}\right\}\]
\end{example}

\begin{definition}[Reduced Row Echelon Form]
   A matrix \(A \in \Mat_{m,n}\) in row echelon form is in \textit{reduced} row echelon form if
   \[\forall 1 \leq i \leq r: a_{i, j_i} = 1\]
   \[\forall 1 \leq i < j \leq r: a_{i, j_j} = 0\]
\end{definition}
\begin{example}
   \[\begin{pmatrix}
         1 & 1 &  2 \\
         0 & 2 & -1 \\
         0 & 0 & 0 \\
      \end{pmatrix} \rightsquigarrow \begin{pmatrix}
         1 & 1 &  2 \\
         0 & 1 & -\frac{1}{2} \\
         0 & 0 & 0 \\
      \end{pmatrix} \rightsquigarrow \begin{pmatrix}
         1 & 0 &  \frac{5}{2} \\
         0 & 1 & -\frac{1}{2} \\
         0 & 0 & 0 \\
      \end{pmatrix}
   \]
   \[x = \frac{5}{2}z\]
   \[y = -\frac{1}{2}z\]
\end{example}

\newpage

\section{Polynomials}
\begin{definition}[Polynomial]
   Given a field \(K\) and a family of coefficients \((a_i)_{i \in \mathbb{N}} \in K\), where \(a_n \neq 0\) and \(\forall i > n: a_i = 0\).

   A polynomial \(p(x)\) of degree \(n \in \mathbb{N}\) is
   \[p(x) := \sum_{k=0}^n a_k \cdot x^k = a_nx^n + a_{n-1}x^{n-1} + \ldots + a_1x + a_0\]
\end{definition}
\begin{remark}
   A polynomial of degree \(d\) is \textit{normalized} if \(a_d = 1\).
\end{remark}

\begin{definition}[Rational Function]
   Given two polynomials \(p(x), q(x)\)
   \[f(x) := \frac{p(x)}{q(x)}\]
\end{definition}

\subsection{Symmetric Polynomials}
A \emph{symmetric polynomial} is a polynomial \(p(x_1, x_2, \ldots, x_n)\) in \(n\) variables, such that if any of the variables are interchanged, one obtains the same polynomial.
Formally,
\begin{definition}[Symmetric Polynomial]
   Given \(\sigma \in S_n\), \(p\) is a symmetric polynomial iff
   \[p(x_1, x_2, \ldots, x_n) = p(x_{\sigma(1)}, x_{\sigma(2)}, \ldots, x_{\sigma(n)})\]
\end{definition}
\begin{example}
   \[x_1^3 + x_2^3-7\]
   \[4x_1^2x_2^2 + x_1^3x_2 + x_1x_2^3 + (x_1 + x_2)^4\]
   \[x_1x_2x_3 - 2x_1x_2 - 2x_1x_3 - 2x_2x_3\]
\end{example}
\emph{Elementary symmetric polynomials} are one type of basic building block for symmetric polynomials, in the sense that any symmetric polynomial can be expressed as a polynomial in elementary symmetric polynomials.
That is, any symmetric polynomial is given by an expression involving only additions and multiplication of constants and elementary symmetric polynomials.

\begin{definition}[Elementary Symmetric Polynomial]
   Let \(I \subset \{1, \ldots, n\}\) such that \(\lvert I\rvert = k\), then
   \[e_k(x_1, \ldots, x_n) := \sum\left(\prod_{i \in I} x_i\right)\]
\end{definition}
\begin{example}
   In every case, \(e_0(x_1, \ldots, x_n) = 1\) and \(e_k = 0\) for \(k > n\).

   For \(n = 1\):
   \[e_1(x_1) = x_1\]

   For \(n = 2\):
   \begin{equation*}
      \begin{split}
         e_1(x_1,x_2) &= x_1 + x_2\\
         e_2(x_1,x_2) &= x_1 x_2
      \end{split}
   \end{equation*}

   For \(n = 3\):
   \begin{equation*}
      \begin{split}
         e_1(x_1, x_2, x_3) &= x_1 + x_2 + x_3\\
         e_2(x_1, x_2, x_3) &= x_1x_2 + x_1x_3 + x_2x_3\\
         e_3(x_1, x_2, x_3) &= x_1x_2x_3
      \end{split}
   \end{equation*}

   For \(n = 4\):
   \begin{equation*}
      \begin{split}
         e_1(x_1, x_2, x_3, x_4) &= x_1 + x_2 + x_3 + x_4\\
         e_2(x_1, x_2, x_3, x_4) &= x_1x_2 + x_1x_3 + x_1x_4 + x_2x_3 + x_2x_4 + x_3x_4\\
         e_3(x_1, x_2, x_3, x_4) &= x_1x_2x_3 + x_1x_2x_4 + x_1x_3x_4 + x_2x_3x_4\\
         e_4(x_1, x_2, x_3, x_4) &= x_1x_2x_3x_4
      \end{split}
   \end{equation*}
\end{example}

\begin{proposition}
   Let \(R\) be a commutative ring.
   The \(R\)-algebra-homomorphism
   \[\iota: R[E_1, \ldots, E_n] \to R[X_1, \ldots, X_n] \qquad\text{where}\qquad \iota(E_k) := e_k(X_1, \ldots, X_n)\]
   is an isomorphism.
\end{proposition}
% TODO: concept of the proof?

As an application, we describe polynomials, whose roots are sums respectively powers of the roots of a given monic polynomial.
\begin{proposition}
   Let \(R\) be a commutative ring.
   \[\mathscr{P}_q(f) := \mathscr{P}_{q,n}(T, b_1, \ldots, b_n) \in R[T, E_1, \ldots, E_n] \quad\text{for}~q \in R[U]\]
   \[\Sigma_{\lambda}(f) := \Sigma_{\lambda, n}(T, b_1, \ldots, b_n) \in R[T, E_1, \ldots, E_n] \quad\text{for}~\lambda=(\lambda_1, \ldots, \lambda_n) \in R^n\]
   factor as
   \[\mathscr{P}_q(f) = \prod_{j=1}^n\big(T - q(\alpha_j)\big) \qquad\text{and}\qquad \Sigma_{\lambda}(f) \prod_{\substack{r_1, \ldots, r_n \in R\\\exists \sigma \in S_n: \forall j: r_{\sigma(j)} = \lambda_j}} \left(T - \sum_{j=1}^n r_j\alpha_j\right)\]
   if
   \[f := T^n + b_1T^{n-1} + \ldots + b_n \qquad\text{where}~b_i\in S\]
   factors in \(S[T]\) as \(f = \prod_{j=1}^n (T - \alpha_j)\) where \(\alpha_i \in S\) for every commutative \(R\)-algebra \(S\).
\end{proposition}

\newpage

\section{Groups}
\subsection{Definition \& Terminology}
\begin{definition}[Group]
   A set \(G\) with an operation
   \[\ast: G \times G \to G \quad\text{where}\quad (a, b) \mapsto a \ast b\]
   if the following holds:
   \begin{enumerate}[label=\roman*, align=Center]
      \item Closure: \(\forall a, b \in G: a \ast b \in G\)
      \item Associativity: \(\forall a, b, c \in G: (a \ast b) \ast c = a \ast (b \ast c)\)
      \item Neutral Element: \(\exists e \in G: (\forall a \in G: e \ast a = a = a \ast e)\)
      \item Inverse Element: \(\forall a \in G~\exists a^{-1} \in G: a \ast a^{-1} = a^{-1} \ast a = e\)
   \end{enumerate}
\end{definition}
\begin{remark}[Notation]
   As we only have a single operation, we omit it all together.
\end{remark}

% TODO: foundations, generalized associative law --> abc don't have to be bracketed.
\begin{proposition}
   Given a group \(G\),
   \begin{enumerate}[label=\roman*, align=Center]
      \item The identity element \(e\) is unique.
      \item The inverse \(a^{-1}\) of each \(a \in G\) is unique.
      \item \(\forall a \in G: (a^{-1})^{-1} = a\)
      \item \(\forall a, b \in G: (ab)^{-1} = b^{-1}a^{-1}\)
      \item The value of \(a_1a_2\ldots a_n\) is independent of how the expression is bracketed.
   \end{enumerate}
\end{proposition}
\begin{remark}[Notation]
   Since the expression \(\underbrace{a a \ldots a}_{n-times}\) does not depend on how it is bracketd, we shall denote it by \(x^n\) and similarly \(a^{-n} := a^{-1} a^{-1} \ldots a^{-1}\).
   Furthermore let \(x^0 := e\).

   When we are dealing with specific groups, we shal use the natural given operation.
   For example, when the operation is \(+\), the identity will be denoted by 0 and inverses \(a^{-1}\) by \(-a\).
\end{remark}

\begin{definition}[Group Table]
   Given a group \(G = \{g_1 := e, \ldots, g_n\}\) its \emph{group table} is the \(n \times n\) matrix whose \(i,j\) entry is the group element \(g_ig_j\).
\end{definition}
\begin{example}[\(S_3\) Group Table]
   \[\begin{matrix}
         \id  & (12)  & (13)  & (23)  & (123) & (132) \\
         (12) & \id   & (132) & (123) & (23)  & (13)  \\
         (13) & (123) & \id   & (132) & (12)  & (23)  \\
         (23) & (132) & (123) & \id   & (13)  & (12)  \\
         (123) & (13) & (23) & (12)   & (132) & \id   \\
         (132) & (23) & (12) & (13)   & \id   & (123) \\
   \end{matrix}\]
\end{example}

\begin{definition}[Order of Groups]
   The cardinality of the set \(G\).
\end{definition}

\begin{definition}[Finite Group]
   A group \(G\) where \(|G| = n\).
\end{definition}

\begin{definition}[Abelian Group]\label{def:abel_group}
   A group whose operation is commutative: \(\forall a, b \in G: ab = ba\).
\end{definition}

% TODO: place correctly
\begin{theorem}[Fundamental Theorem for Abelian Groups]
   Let \(G\) be a finitely generated abelian group.

   There exists unique \(r, s \in \mathbb{N}\) and unique \((a_i)_{i \leq r} \subset \mathbb{Z}\) where \(\forall i \leq r: a_i | a_{i+1}\) such that
   \[G \cong \bigoplus_{i = 1}^r \mathbb{Z}_{/a_i\mathbb{Z}} \oplus \mathbb{Z}^s\]
\end{theorem}

\subsection{Important Examples}
\subsubsection{Whole Numbers}
\begin{proposition}[Abelian Group \(\mathbb{Z}\)]\label{pro:z_abelian}
   \(\mathbb{Z}\) with
   \[\dotplus: \mathbb{Z} \times \mathbb{Z} \to \mathbb{Z} \quad\text{where}\quad \big([(m, n)], [(m', n')]\big) \mapsto [(m + m', n + n')]\]
   is an abelian group.
\end{proposition}
\begin{proof}
   Let \(x := [(x_1, x_2)]\), \(y := [(y_1, y_2)]\) and \(z := [(z_1, z_2)]\) where \(x_1, x_2, y_1, y_2, z_1, z_2 \in \mathbb{N}\).

   \(\dotplus\) is associative and commutative because they are induced from \(+: \mathbb{N} \times \mathbb{N} \to \mathbb{N}\).
   \begin{equation*}
      \begin{split}
         x \dotplus (y \dotplus z) & = [(x_1, x_2)] \dotplus \big([(y_1, y_2)] \dotplus [(z_1, z_2)]\big) = [(x_1, x_2)] \dotplus [(y_1 + z_1, y_2 + z_2)]\\
                                   & = [(x_1 + (y_1 + z_1), x_2 + (y_2 + z_2))] = [((x_1 + y_1) + z_1, (x_2 + y_2) + z_2)]\\
                                   & = [(x_1 + y_1, x_2 + y_2)] \dotplus [(z_1, z_2)] = \big([(x_1, x_2)] \dotplus [(y_1, y_2)]\big) \dotplus [(z_1, z_2)] = (x \dotplus y) \dotplus z
      \end{split}
   \end{equation*}

   \begin{equation*}
      \begin{split}
         x \dotplus y & = [(x_1, x_2)] \dotplus [(y_1, y_2)] = [(x_1 + y_1, x_2 + y_2)]\\
                      & = [(y_1 + x_1, y_2 + x_2)] = [(y_1, y_2)] \dotplus [(x_1, x_2)] = y \dotplus x
      \end{split}
   \end{equation*}

   \(0 := [(0, 0)]\) is the neutral element.
   \[x \dotplus 0 = [(x_1, x_2)] \dotplus [(0, 0)] = [(x_1 + 0, x_2 + 0)] = [(x_1, x_2)] = x\]

   For \(x := [(x_1, x_2)]\) is \(x^{-1} := [(x_2, x_1)]\) the inverse since
   \[x \dotplus x^{-1} = [(x_1, x_2)] \dotplus [(x_2, x_1)] = [(x_1 + x_2, x_2 + x_1)]\]
   but since
   \[x_1 + x_2 = x_2 + x_1 \implies (x_1, x_2) \sim (x_2, x_1) \implies (x_1 + x_2, x_2 + x_1) \in [(0, 0)]\]
   hence \([(x_1 + x_2, x_2 + x_1)] = [(0, 0)] \implies x \dotplus x^{-1} = 0\).
\end{proof}
% 
%    \begin{definition}[Whole Numbers]
%       Given \(\sim\) on \(\mathbb{N} \times \mathbb{N}\) defined through
%       \[(m_1, n_1) \sim (m_2, n_2) :\iff m_1 + n_2 = m_2 + n_1\]
%       \[\mathbb{Z} := (\mathbb{N} \times \mathbb{N})/\sim\]
%    \end{definition}
%    \begin{remark}
%       This way some \(n \in \mathbb{N}\) is identified through \([(n, 0)] \in \mathbb{Z}\).
%    \end{remark}
%    \begin{example}
%       \((2, 1) \sim (4, 3)\) because \(2 + 3 = 4 + 1\)
%    \end{example}

\subsubsection{General Linear Group}\label{sssec:gen_lin_grp}
\begin{proposition}
   Given a \(K\)-vector space \(V\), the set of invertible linear maps
   \[\GL(V) := \{f: V \to V \mid f~\text{is linear and bijective}\}\]
   is a group \((\GL(V), \circ, \id_V)\).
\end{proposition}
\begin{remark}
   As \(f\) is bijective we refer to this group more generally as the \emph{automorphism group} denoted \(\Aut(V)\).
\end{remark}

In the case where \(V\) is finite dimensional we can represent \(f\) with a matrix and as \(f\) is bijective we know that the corresponding matrix also must be invertible.
This brings us to the group of invertible matrices.

\begin{proposition}[General Linear Group]\label{pro:glm}
   The set of invertible matrices
   \[\GL_n(K) := \{A \in \Mat_n(K) \mid \det(A) \neq 0\}\]
   is a group \((\GL_n(K), \cdot, I_n)\).
\end{proposition}

\subsubsection{Orthogonal Group}
\begin{proposition}
   Given a Euclidean vector space \(V\), the set
   \[\OG(V) := \{f \in \GL(V) \mid f~\text{is an isometry}~V \to V\}\]
   is a group \((\OG(V), \circ, \id_V)\).
\end{proposition}

With the rules in the remark of \cref{def:std_scal_prod} we see for \(A \in \OG(n)\) and some \(x, y \in \mathbb{R}^n\)
\[\langle Ax, Ay\rangle = x^TA^TAy = x^Ty = \langle x, y\rangle\]
So we define
\[\OG(n) := \{A \in \GL_n(\mathbb{R}) \mid x \mapsto Ax~\text{is an isometry}~\mathbb{R}^n \to \mathbb{R}^n\}\]

\begin{proposition}[Orthogonal Group]
   The set of orthogonal matrices
   \[\OG(n) := \{A \in \GL_n(K) \mid \det(A) \in \{\pm 1\}\}\]
   is a group \((\OG(n), \cdot, I_n)\).
\end{proposition}
\begin{remark}
   It is the group of distance-preserving transformations of a Euclidean space of dimension \(n\) that preserve a fixed point
   \[\OG(2) = \SO(2) \cup \left\{\begin{pmatrix}\cos\theta & \sin\theta\\ \sin\theta & -\cos\theta\end{pmatrix} ~\middle|~ \theta \in \mathbb{R}\right\}\]
\end{remark}

\begin{proposition}
   Let \(G < \OG(2)\), then \(G\) is generated by rotation by \(\frac{2\pi}{n}\) and we have one of the following
   \begin{enumerate}[label=\roman*, align=Center]
      \item \(G \subset \SO(2)\) and \(n := |G|\), i.e. \(G\) is cyclic
      \item \(G \not\subset \SO(2)\) and \(n := \frac{1}{2} \lvert G \rvert\).
         Furthermore \(G\) is generated by an additional reflection, i.e. \(G \cong D_n\)
   \end{enumerate}
\end{proposition}

\subsubsection{Unitary Group}
\begin{proposition}
   Given a unitary vector space \(V\)
   \[\U(V) := \{f \in \GL(V) \mid f~\text{is an isometry}~V \to V\}\]
\end{proposition}

With the rules of the remark in \cref{def:compl_scal_prod} we have for \(A \in \U(n)\)
\[\langle Az, Aw\rangle = z^TA^T\overline{A}\overline{w} = z^T \overline{w} = \langle z, w\rangle\]
As before we define for the orthogonal group for finite dimensional unitary vector spaces
\[\U(n) := \{A \in \GL_n(\mathbb{C}) \mid z \mapsto Az~\text{is an isometry}~\mathbb{C}^n \to \mathbb{C}^n\}\]
\begin{proposition}[Unitary Group]
   The set of unitary matrices
   \[\U(n) := \{A \in \GL_n(K) \mid \det(A) \in \{\pm 1\}\}\]
   is a group \((\OG(n), \cdot, I_n)\).
\end{proposition}

\subsubsection{Special Orthogonal Group}
\begin{proposition}[Special Orthogonal Group]
   \[\SO(n) := \{A \in \OG(n) \mid \det(A) = 1\}\]
\end{proposition}
\begin{remark}[Terminology]
   In the cases where \(n = 2\) or \(n = 3\) we refer to \(\SO(n)\) as the \emph{rotation group}, since its elements are rotation matrices.
   \[\SO(2) = \left\{\begin{pmatrix}\cos\theta & -\sin\theta\\ \sin\theta & \cos\theta\end{pmatrix} ~\middle|~ \theta \in \mathbb{R}\right\}\]
\end{remark}

\begin{theorem}
   The non-trivial subgroups of \(\SO(3)\) are
   \begin{enumerate}[label=\roman*, align=Center]
      \item Cyclic groups generated by \(\frac{2\pi}{n}\) rotation for \(n \geq 2\)
      \item Dihedral groups generated by \(\frac{2\pi}{n}\) rotation for \(n \geq 2\) and a rotation by \(\pi\) where both generators have orthogonal axes of rotation.
      \item Tetrahedral rotation group (\(\cong A_4\))
      \item Octahedral rotation group (\(\cong S_4\))
      \item Icosahedral rotation group (\(\cong A_5\))
   \end{enumerate}
\end{theorem}
\begin{remark}
   Cases (iii)-(v) refer to symmetry group of the platonic solids
\end{remark}

Finally, we obtain a description of \(\SO(3)\) by relating it to the special unitary group \(\SU(2)\).

\begin{proposition}
   The conjugation action of \(\SU(2)\) on trace-zero skew-Hermitian \(\Mat_2(\mathbb{C})\) induces an isomorphism
   \[\SU(2)_{/\{\pm I_2\}} \cong \SO(3)\]
\end{proposition}

\begin{proposition}[Special Unitary Group]
   \[\SU(n) := \{A \in \U(n) \mid \det(A) = 1\}\]
\end{proposition}
\begin{remark}
   Trace-zero, skew-hermitian matrices?
\end{remark}

\subsubsection{Symmetric Group}
\paragraph{Permutations}
\begin{definition}[Permutation]\label{def:permutation}
   Given a set \(X\),
   \[\sigma: X \xrightarrow{\sim} X\]
\end{definition}
\begin{example}
   Let \(X = \{1, 2, 3\}\),
   \[\sigma_{\id} := \begin{cases}1 \mapsto 1\\ 2 \mapsto 2 \\ 3 \mapsto 3 \end{cases} \qquad \tau := \begin{cases}1 \mapsto 3\\ 2 \mapsto 2 \\ 3 \mapsto 1 \end{cases}\]
\end{example}
\begin{remark}[Notation]
   We denote the set of all permutations as \(S(X)\).
\end{remark}

\begin{definition}[Symmetric Group]
   Given a set \(X\),
   \[\big(S(X), \circ, \sigma_{\id}\big)\]
\end{definition}

In the special case when \(X = \{1, 2, \ldots, n\}\), the symmetric group on \(X\) is denoted \(S_n\), the \emph{symmetric group of degree n}.

\begin{definition}[\(n\)th Symmetric Group]
   Given a set \(X\) with \(n\) elements,
   \[S_n := \big(S(X), \circ, \sigma_{\id}\big)\]
\end{definition}
\begin{remark}[Notation]
   We have multiple notations to write \(\tau\) from the example above.
   \[\tau = \begin{bmatrix}
         1 & 2 & 3\\
         3 & 2 & 1
   \end{bmatrix} = [3~2~1] = (1,3)(2) = (1,3)\]
   The last equality is called \emph{cycle decomposition}.
   A \emph{cycle} is a string of integers which represents the element of \(S_n\) which cyclically permutes these integers and fixes all others.
   The cycle \((a_1~a_2~\ldots~a_m)\) is the permutation where
   \[\forall 1 \leq i \leq m-1: a_i \mapsto a_{i+1} \qquad\text{and}\qquad a_m \mapsto a_1\]
   If \(\sigma(a) = a\) we say \(a\) is a \emph{fixpoint} which we omit in cycle decomposition.
\end{remark}
\begin{example}
   Let \(\sigma = (2, 5, 1)\) and \(\tau = (1, 3, 4, 2)\) be two permutations.
   What is \(\sigma \circ \tau\)?
      \[\sigma = \begin{bmatrix}1&2&3&4&5\\2&5&3&4&1\end{bmatrix} \qquad \tau = \begin{bmatrix}1&2&3&4&5\\3&1&4&2&5\end{bmatrix}\]
   The following table is useful to determine the composition.
   We start of with 1 and then follow the permutations step by step until we reach a cycle.
   Then we take the least \(x\) we haven't covered so far (in our case 2) and repeat the process.
   \[\begin{array}{c|c|c|c}
         x & \tau(x) & \sigma(\tau(x)) & m \\
         \hline
         1 & 3 & 3 & 1\\
         3 & 4 & 4 & 2\\
         4 & 2 & 5 & 3\\
         5 & 5 & 1 & 4\\
         \hline
         2 & 1 & 2
      \end{array}\]
      so we get of the first column: \(\sigma \circ \tau = (1, 3, 4, 5)(2) = (1, 3, 4, 5)\).
\end{example}

\begin{proposition}[Order of \(S_n\)]
   Let \(X\) be a set with \(|X| = n\), then is
   \[|S(X)| = n!\]
\end{proposition}

\paragraph{Inversions}
\begin{definition}[Inversion]
   Given a permutation \(\sigma \in S_n\) where \(i < j\) and \(\sigma(i) > \sigma(j)\), we call the pair \((i,j)\) an \emph{inversion} of \(\sigma\).
\end{definition}
\begin{remark}[Intuition]
   An inversion of a permutation is a pair of elements whose order was changed by the permutation.
\end{remark}

\begin{definition}[Set of Inversions]
   The set of all inversions of \(\sigma \in S_n\) is denoted \(\inv(\sigma)\).
\end{definition}
\begin{remark}[Terminology]
   We call \(|\inv(\sigma)|\) the \emph{inversion number}, which is a common measure of the sortedness of a permutation.
\end{remark}

\begin{definition}[Even \& Odd Permutation]
   Given \(\sigma \in S_n\),
   \begin{enumerate}[label=\roman*, align=Center]
      \item \(\sigma\) is \emph{even} if it performs an even number of two-element swaps.
      \item \(\sigma\) is \emph{odd} if it performs an odd number of two-element swaps.
   \end{enumerate}
\end{definition}

\begin{definition}[Signum of Permutations]
   Given \(\sigma \in S_n\),
   \[\sign(\sigma) := \begin{cases}1 & \sigma~\text{is even}\\ -1 & \sigma~\text{is odd}\end{cases}\]
\end{definition}
\begin{remark}
   The map \(S_n \to \{\pm 1\}\) where \(\sigma \mapsto \sign(\sigma)\) is a group homomorphism.
\end{remark}

\begin{proposition}
   Let \(\sigma \in S_n\), then
   \[\sign(\sigma) = (-1)^{|\inv(\sigma)|}\]
\end{proposition}
\begin{example}
   Let \(\sigma = \begin{bmatrix} 1 & 2 & 3\\ 3 & 2 & 1\end{bmatrix}\), then is
   \[\sigma(1) > \sigma(2),\quad \sigma(1) > \sigma(3) \quad\text{and}\quad \sigma(2) > \sigma(3) \quad\text{hence}\quad \inv(\sigma) = \{(1,2), (1, 3), (2, 3)\}\]
   and so \(\sign(\sigma) = (-1)^3 = -1\).
\end{example}

\begin{definition}[Alternating Group]
   For \(n \geq 2\) is \(A_n\) the group of all even permutations of \(S_n\).
\end{definition}

\begin{definition}[Permutation Matrix]\label{def:perm_mat}
   Given a field \(K\) and a permutation \(\sigma \in S_n\).
   \[P \in \Mat_n(K)~\text{where}~\forall i \in \{1, \ldots, n\}: P_{i, \sigma(i)} = 1\]
   and all other entries are 0.
\end{definition}
\begin{remark}
   In other words, the \textit{i}th column vector of \(P\) is \(e_{\sigma(i)}\).
   \(P_{ji}\) of \cref{def:elementary_matrix} is a special kind of permutation matrix (hence the german name ``Vertauschungsmatrix'').
\end{remark}
\begin{example}
   Let \(\sigma = \begin{bmatrix}1&2&3&4&5\\2&5&1&3&4\end{bmatrix}\)
   Then is
   \[P_\sigma = (e_{\sigma(1)}~e_{\sigma(2)}~e_{\sigma(3)}~e_{\sigma(4)}~e_{\sigma(5)}~) = (e_2~e_5~e_1~e_3~e_4) = \begin{pmatrix}0&0&1&0&0\\1&0&0&0&0\\0&0&0&1&0\\0&0&0&0&1\\0&1&0&0&0\end{pmatrix}\]
   So it is trivial to see that \(P_{\sigma_{\id}} = I_n\).
\end{example}

\begin{proposition}
   Let \(P\) be a permutation matrix, then
   \[P^{-1} = P^T\]
\end{proposition}

\subsubsection{Dihedral Group}\label{ssec:dihedral_groups}
An important family of examples of groups is the class of groups whose elements are symmetries.
\emph{Symmetry} is the intrinsic property of a mathematical object which causes it to remain invariant under certain classes of transformations (such as rotation, reflection, inversion, or more abstract operations).

So an approachable introduction are the symmetries of geometric objects in the plane.
For some \(n \in \mathbb{N}_{\geq 3}\) we denote the set of symmetries of a regular \(n\)-gon with \(D_{2n}\).
So for \(n=3\) we regard the symmetries of a regular triangle i.e. a triangle with sides of equal length.

We can describe its symmetries by first choosing a labelling \(1, \ldots, n\) of the \(n\) vertices.
Then each symmetry can be describe uniquely by the corresponding permutation \(\sigma\) of \(\{1, \ldots, n\}\).
For example if the symmetry puts vertex \(i\) in the place where vertex \(j\) was originally, then \(\sigma\) is the permutation sending \(i\) to \(j\).

\begin{center}
   \input{drawings/D3_example.tex}
\end{center}

We define for \(r,s \in D_{2n}\), \(rs\) to be the symmetry obtained by first applying \(s\) then \(r\).
Note that we are viewing symmetries as functions on the \(n\)-gon, so \(rs\) is just function composition.
This way we can make \(D_{2n}\) into a group.

Let \(r\) be the rotation clockwise about the origin through \(\frac{2\pi}{n}\) radian.

Let \(s\) be the reflection about the line of symmetry through vertex 1 and the origin.

This way we get
\[D_{2n} = \{1, r, r^2, \ldots, r^{n-1}, s, sr, sr^2, \ldots, sr^{n-1}\}\]
i.e. each element can we written uniquely in the form \(s^kr^i\) with \(k = 0\) or 1 and \(0 \leq i \leq n-1\).
\begin{proposition}
   For the dihedral group \(D_{2n}\) holds the following
   \begin{enumerate}[label=\roman*, align=Center]
      \item \(1, r, r^2, \ldots r^{n-1}\) are all distinct and \(r^n = 1\).
      \item \(s^2 = 1\)
      \item \(\forall 0 \leq i \leq n: s \neq r^i\)
      \item \(\forall 0 \leq i,j \leq n-1: i \neq j \implies sr^i \neq sr^j\)
      \item \(\forall 0 \leq i \leq n: r^is = sr^{-i}\)
   \end{enumerate}
\end{proposition}
From (i), (ii) and (v) above follows that any product of two elements in the form \(s^kr^i\) can be reduced to another in the same form.
For example if \(n = 12\)
\[(sr^9)(sr^6) = s(r^9s)r^6 = s(sr^{-9})r^6 = s^2r^{-9+6} = 1 r^{-3} = r^9\]
This means that all elements of \(D_{2n}\) can be represented by just 2 elements \(r, s\) and the relations (i), (ii) and (v).

\subsubsection{Quaternion Group}
\begin{definition}[Quaternion Group]
   \[Q_8 := \{1, -1, i, -i, j, -j, k, -k\}\]
   where
   \begin{enumerate}[label=\roman*, align=Center]
      \item \((-1)(-1) = 1\)
      \item \(\forall a \in Q_8: 1a = a = a1 \quad\text{and}\quad (-1)a = -a = a(-1)\)
      \item \(ii = jj = kk = -1\)
      \item \(ij = k \qquad\text{and}\qquad ji = -k\)
      \item \(jk = i \qquad\text{and}\qquad kj = -i\)
      \item \(ki = j \qquad\text{and}\qquad ik = -j\)
   \end{enumerate}
\end{definition}

\subsection{Group Presentation}
Regarding \cref{ssec:dihedral_groups}, the use of the generators \(r\) and \(s\) for the dihedral group provides a simple an succinct way of computing in \(D_{2n}\).
We can similarly introduce the notions of generators and relations for arbitrary groups.

\subsubsection{Generators}
Given a subset \(S \subset G\) of a group \(G\) we can obtain a subgroup \(\langle S \rangle < G\) by combining all elements of \(S\) in all possible ways; we say \(\langle S \rangle\) is \emph{generated} by \(S\) or \(S\) \emph{generates} \(\langle S\rangle\).

\(\langle S \rangle\) is the smallest subgroup of \(G\) containing every element of \(S\).
This means that \(S\) is not contained in any subgroup of the group other than the entire group.
Hence \(\langle S \rangle\) is equal to the intersection over all subgroups containing \(S\).
Equivalently, \(\langle S \rangle\) is the subgroup containing all elements of \(G\) that can be expressed as the finite product of elements in \(S\) and their inverses.

If \(\langle S \rangle = G\) (\(S\) generates \(G\)) we say \(S\) is a \emph{generating set} or \emph{system of generators} of \(G\).
The elements of \(S\) are called \emph{generators} and if \(S\) is finite we say \(G\) is \emph{finitely generated}.

\begin{definition}[Generating Set of Groups]\label{def:gen_set_group}
   Given a group \(G\), a subset \(S \subset G\), where every \(g \in G\) can be written as a product of elements of \(S\) and their inverses.
\end{definition}
\begin{example}[Group of Integers]
   1 is a generator for \(\mathbb{Z}\) since every integer is a finite sum of \(\pm 1\).
\end{example}
\begin{example}[Dihedral Group]
   \(S = \{r, s\}\) is a set of generators of \(D_{2n}\), so \(D_{2n} = \langle r, s \rangle\).
\end{example}

\subsubsection{Cyclic Groups}
Every element \(g \in G\) generates the \emph{cyclic} subgroup \(\langle g \rangle < G\).

\begin{definition}[Cyclic Group]\label{def:cyclic_group}
   A group \(G\) which is generated by a single \(g \in G\).
   \[G = \langle g \rangle = \{\ldots, g^{-2}, g^{-1}, e_G, g, g^2, \ldots\}\]
\end{definition}
\begin{remark}
   Differently put, looking at \cref{def:gen_set_group} we see that
   \[\exists a \in G: (\forall b \in G: b = a^i) \quad\text{with}~i \in \mathbb{Z}\]
   With this follows that \(\mathbb{Z} \to G\) where \(i \mapsto g^i\) is a surjective homomorphism.
\end{remark}
\begin{example}
   Subgroups of \(\mathbb{Z}\) are the same as ideals.
   They are cyclic subgroups, of the form \(g\mathbb{Z} < \mathbb{Z}\) for some unique \(g \in G\).
   If \(|G| = n\), then is \(G \cong \mathbb{Z}_{/n\mathbb{Z}}\) by the first isomorphism theorem.
\end{example}

% TODO: merge with Order of Group-Elements definition
If \(\langle g \rangle \cong \mathbb{Z}_{/n\mathbb{Z}}\) for some \(n \in \mathbb{N}\), then \(n\) is the smallest positive integer for which \(g^n = e\), and \(n\) is called the \emph{order of} \(g\).
Differently put we can also say that \(|\langle g \rangle| = n\)
If \(\langle g \rangle \cong \mathbb{Z}\), then \(g\) is said to have \emph{infinite order}.

\begin{definition}[Order of Group-Elements]
   Given a group \(G\) and \(g \in G\)
   \[|g| := n\]
   where \(n \in \mathbb{N}\) is the smallest number s.t. \(g^n = e_G\).
\end{definition}
\begin{remark}[Notation]
   If no positive power of \(g\) is the identity, we write \(|g| = \infty\).
\end{remark}

\subsubsection{Relations}
Any equations in a general group \(G\) that the generators satisfy are called \emph{relations} in \(G\).
Thus in \(D_{2n}\) we have relations \(r^n = 1\), \(s^2 = 1\) and \(rs = sr^{-1}\).
Moreover, these three relations have the additional property that any other relation between elements of the group may be derived from these three.
This is not immediately obvious; it follows from the fact that we can determine exactly when two group elements are equal by using only these three relations.

In general, if some group \(G\) is generated by a subset \(S\) and there is some collection of relations \(R_1, R_2, \ldots, R_m\) (where each is an equation in the elements from \(S \cup \{1\}\)), such that any relation among the elements of \(S\) can be deduced from these, we shall call these generators and relations a \emph{presentation} of \(G\) and write
\[G = \langle S \mid R_1, R_2, \ldots, R_m \rangle\]
For example
\[D_{2n} = \langle r, s \mid r^n = s^2 = 1, rs = sr^{-1}\rangle\]

\subsubsection{Free Groups}
In this section we introduce the basic theory of so-called \emph{free groups}.
This will enable us to make precise the notions of generators and relations in a more general way.

The basic idea of a free group \(F_S\) generated by a set \(S\) is that there are no relations satisfied by any of the elements in \(S\) (\(S\) is ''free`` of relations).

For example if \(S = \{a, b\}\), then the elements of \(F_S\) are of the form \(a, aa, ab, abab, bab, \ldots\) and their inverses.
If we group like terms together then we obtain the familiar form \(a, b^{-3}, aba^{-1}b^{-2}, \ldots\).
We call those elements \emph{words} in \(a\) and \(b\), respectively in the \emph{alphabet} \(S\).

The notion of ''freeness`` occurs in many algebraic systems.
When the algebraic systems are vector spaces, \(F_S\) is simply the vector space which has \(S\) as a basis.
Every vector in this spce is a unique linear combination of the elements of \(S\) which is the analogue of a word.

\begin{definition}[Word]
   Given a group \(G\) and \(S \subset G\)
   \[s_1^{\alpha_1}s_2^{\alpha_2}\ldots s_n^{\alpha_n}\]
   with \(s_i \in S\) and \(\alpha_i \in \{\pm 1\}\).
\end{definition}
\begin{remark}[Terminology]
   \(n\) is known as the \emph{length} of a word.
   The \emph{empty word} is the unique word of length 0.
   A single symbol may be viewed as a word of length 1.
\end{remark}
\begin{remark}[Intuition]
   We can think of a word as a finite product of elements of \(S\) and their inverses (where repetitions are allowed).
\end{remark}

In order to assure uniqueness of expressions we consider only words which have no obvious ''cancellations`` between adjacent terms (such as \(baa^{-1}b = b^2\)).

\begin{definition}[Reduced Word]
   A word \(s_1^{\alpha_1}s_2^{\alpha_2}\ldots s_n^{\alpha_n}\) where
   \begin{enumerate}[label=\roman*, align=Center]
      \item \(\forall i: s_{i+1} \neq s_i^{-1}\)
      \item \(s_k = 1 \implies s_i = 1 \forall i \geq k\)
   \end{enumerate}
\end{definition}
\begin{remark}
   The reduced word \(1~1~1~\cdots\) is called the \emph{empty word}.
\end{remark}

Words are mutliplied by concatenation, for example \((aba)(b^{-1}a^3b) = abab^{-1}a^3b\).
To prove the associative property for multiplication of words we return to the most basic level where all the exponents in the words of \(S\) are \(\pm 1\).

\begin{definition}[Free Group]
   Given a set \(S\), the set \(F_S := S \times \{\pm 1\}\) with the operation
   \[(s_1^{\alpha_1}\cdots s_k^{\alpha_k}) \ast (t_1^{\beta_1}\cdots t_l^{\beta_l}) := s_1^{\alpha_1}\cdots s_{k-j}^{\alpha_{k-j}}~t_{j+1}^{\beta_{j+1}}\cdots t_l^{\beta_l}\]
\end{definition}
\begin{remark}[Notation]
   If \(S = \{1, \ldots, n\}\) we denote \(F_n\).
\end{remark}
\begin{remark}[Terminology]
   A group \(G\) is a \emph{free group} if there is some set \(S\) such that \(G = F_S\).
   In this case we call \(S\) a set of \emph{free generators} of \(G\).
\end{remark}

One important property reflecting the fact that there are no relations that must be satisfied by the generators in \(S\) is that any map from the \emph{set} \(S\) to a group \(G\) can be uniquely extended to a homomorphism from the group \(F_S\) to \(G\).
The fact that there are no relations to worry about menas that we can specify the images of the generators arbitrarily.
This is frequently reffered to as the \emph{universal property of the free group} and in fact characterizes the group \(F_S\).

\begin{proposition}[Universal Property of Free Groups]\label{pro:univ_prop_free_groups}
   Let \(S\) be a set and \(G\) be a group, for any map \(f: S \to G\) there is a unique group homomorphism \(\varphi: F_S \to G\).
   \begin{center}
      \begin{tikzcd}
         S \arrow[dr, swap, "f"]\arrow[r, "i"] & F_S \arrow[d, dashrightarrow, "\varphi"]\\
                                                    & G
      \end{tikzcd}
   \end{center}
\end{proposition}

\begin{proposition}
   Let \(G\) be a group, \(S \subset G\) and \(\varphi: F_S \to G\) from \ref{pro:univ_prop_free_groups}, then
   \[\langle S \rangle = \im(\varphi)\]
   In particular is \(S\) a generating set iff \(\varphi\) is surjective.
\end{proposition}

\subsubsection{Presentations}
% TODO: finish
Let \(G\) be any group, take \(S = G\) and \(f = \id_G\), then by the universal property of free groups we have a surjective homomorphism \(\varphi: F_G \to G\).
This means that \(G\) is the homomorphic image of a free group.

More generally if \(S \subset G\) generates \(G\) then there is a unique surjective homomorphism \(\varphi: F_S \to G\) where \(\varphi = \id_S\).

\begin{definition}[Normal Closure]
   Given a group \(G\), the \emph{normal closure} of \(R \subset G\) -- denoted \(\langle R \rangle^G\) -- is the smallest normal subgroup of \(G\) containing \(R\).
\end{definition}
\begin{remark}
   The normal closure is the intersection of all normal subgroups of \(G\) containing \(R\).
\end{remark}

\subsection{Cosets}
\begin{definition}[Left- \& Right Coset]
   Given a group \(G\), a subgroup \(H \subset G\) and \(g \in G\), then is
   \[gH := \{gh \mid h \in H\}~\text{a left coset}\]
   \[Hg := \{hg \mid h \in H\}~\text{a right coset}\]
   of \(H\) in \(G\).
\end{definition}
\begin{remark}
   \(H\) itself is also a coset when we take \(g = e_G\).
\end{remark}

\begin{proposition}
   Let \(G\) be a group and \(H \subset G\) a subgroup.
   \begin{enumerate}[label=\roman*, align=Center]
      \item \[\forall g, g' \in G: (gH = g'H) \lor (gH \cap g'H = \emptyset)\]
      \item \[\bigcup_{g \in G} gH = G\]
   \end{enumerate}
   analogously for right cosets.
\end{proposition}
\begin{remark}
   From this follows that ''belong to the same left \(H\)-coset`` defines an equivalence relation on \(G\), whose equivalence classes are the left cosets of \(H\).
\[g \sim g' :\iff \exists h \in H: g' = gh\]
\end{remark}

\begin{definition}[Set of Cosets]
   \[G/H := \{gH \mid g \in G\} \qquad\text{and}\qquad H\backslash G := \{Hg \mid g \in G\}\]
\end{definition}
\begin{remark}
   The map \(G \xrightarrow{\sim} G\) where \(g \mapsto g^{-1}\) induces a map \(G/H \xrightarrow{\sim} H\backslash G\) because for \(a \in G\) we have
   \[\{g^{-1} \mid g \in aH\} = Ha^{-1} \qquad\text{and}\qquad \{g^{-1} \mid g \in Ha\} = a^{-1}H\]
   It follows that if one of the sets of cosets is finite, then so is the other and they have the same number of elements.

   We could also say that \(H\) gives rise to finitely many cosets.
\end{remark}

\subsection{Subgroups}
\begin{definition}[Subgroup]
   Given a group \((G, \ast)\), \(H \subset G\) iff
   \begin{enumerate}[label=\roman*, align=Center]
      \item Closure: \(\forall a, b \in H: a \ast b \in H\)
      \item Inverse Closure: \(\forall a \in H: \exists a^{-1} \in H\).
   \end{enumerate}
\end{definition}
\begin{remark}[Notation]
   We may denote \(H < G\).
\end{remark}
\begin{remark}
   Subgroup of abelian groups are also abelian.
\end{remark}

\begin{proposition}[Cayley's Theorem]
   Every finite group is isomorphic to a subgroup of \(S_n\).
\end{proposition}

\subsubsection{Normal Subgroups}
% TODO: Introduce notation \trianglelefteq to distinguish subgroups and proper subgroups
\begin{definition}[Normal Subgroup]
   \(H < G\) where \(\forall g \in G: gH = Hg\).
\end{definition}
\begin{remark}[Notation]
   We may denote \(H \triangleleft G\).
\end{remark}
\begin{remark}
   It follows that if \(G\) is abelian, every subgroup is normal.
\end{remark}

\begin{proposition}
   Let \(G\) be a group and \(H < G\).
   \[H \triangleleft G \iff \forall g \in G: gH = Hg\]
\end{proposition}

\begin{remark}
   The equality \(gH = Hg\) may be expressed in the equivalent form \(gHg^{-1} = H\), i.e. \(H\) is invariant under conjugation.
   In fact, it is equivalent to require \(\forall g \in G: gHg^{-1} \subset H\), i.e. conjugation sends elements of \(H\) to elements of \(H\).
   So if we need to prove \(gH = Hg\) we can just prove \(gHg^{-1}\).
\end{remark}

\begin{proposition}\label{pro:ker_subgrp}
   Let \(G, G'\) be groups and \(f: G \to G'\) a group homomorphism.
   \[\ker(f) \triangleleft G\]
\end{proposition}
\begin{example}
   For \(n \in \mathbb{N}\), we have that for \(\sign: S_n \to \{\pm 1\}\) that \(\ker(\sign) = A_n\), so \(A_n \triangleleft S_n\).
\end{example}

\begin{corollary}
   Suppose \(G\) from above is finite.
   \[|G| = |\im(f)| \cdot |\ker(f)|\]
\end{corollary}

\begin{proposition}
   Let \(G, G'\) be groups, \(H' < G'\) and \(f: G \to G'\) a homomorphism, then is
   \begin{enumerate}[label=\roman*, align=Center]
      \item \(f^{-1}(H') < G\)
      \item \(f^{-1}(H') \triangleleft G \iff H' \triangleleft G'\)
   \end{enumerate}
\end{proposition}

\begin{definition}[Simple Group]
   Given a non-trivial group \(G\), whose only normal subgroups are \(G\) and \(\{e_G\}\).
\end{definition}

\begin{proposition}
   \(A_n\) is simple for every \(n \geq 5\).
\end{proposition}

\begin{proposition}
   Let \(G\) be a finite group and \(H < G\) where \(p := [G:H]\) is prime.

   If \(p\) is the smallest prime factor of \(|G|\) then \(H\) is normal.
\end{proposition}

\subsubsection{Index of Subgroups}
\begin{definition}[Quotient Group]
   Given \(H \triangleleft G\), the set \(G_{/H}\) with the operation
   \[\ast: G_{/H} \times G_{/H} \to G_{/H} \qquad\text{where}\qquad aH \ast bH := abH\]
\end{definition}
\begin{remark}[Notation]
   We denote \(\bar{a} := aH \in G_{/H}\) hence we have \(\bar{a}\bar{b} = \overline{ab}\).
\end{remark}
\begin{remark}
   The definition above is only valid if \(H\) is normal, since only then \(G/H = H\backslash G\).

   Then we can check that the operation above is well-defined
   Suppose \(a' \in aH\) and \(b' \in bH\), i.e. \(a' = ah\) and \(b' = bk\) with \(h,k \in H\).
   \[a'b' = ahbk = ab(b^{-1}hb)k\]
   since \(H\) is normal, we have \(b^{-1}hb \in H\), hence \(a'b' \in abH\).

   Associativity is imediate as
   \[(\bar{a}\bar{b}) \bar{c} = \overline{ab}\bar{c} = \overline{abc} = \bar{a}\overline{bc} = \bar{a}(\bar{b}\bar{c})\]
   \(\bar{e_G}\) is the identity and \(\bar{a^{-1}}\) is the inverse to \(\bar{a}\).
\end{remark}

There is the surjective canonical homomorphism \[G \to G_{/H} \qquad\text{where}\qquad a \mapsto \bar{a}\]
with kernel equal to \(H\).
In fact the kernel of any group homomorphism is normal.

\begin{definition}[Index of Subgroup]
   Given a group \(G\) and \(H < G\),
   \[[G:H] := |G_{/H}|\]
   is the \emph{index of \(H\) in \(G\)}.
\end{definition}
\begin{remark}[Intuition]
   The index of a subgroup is the relative size of \(H\) in \(G\).
   Differently put, it is the number of cosets of \(H\) which fill up \(G\).
   For example, if \(H\) has index 2, then half of the elements of \(G\) lie in \(H\).
\end{remark}
\begin{example}
   Let \(2\mathbb{Z}\) be the subgroup of \(\mathbb{Z}\).
   Then \(2\mathbb{Z}\) has two cosets in \(\mathbb{Z}\), namely the even integers and the odd integers.
   So the index of \(2\mathbb{Z}\) is two.
   To generalize,
   \[[\mathbb{Z} : n\mathbb{Z}] = n\]
   for any \(n \in \mathbb{N}_{>0}\).
\end{example}
\begin{remark}
   So if \(G\) is finite then every \(H \subset G\) is of finite index.
\end{remark}

\begin{proposition}\label{pro:index_formula}
   Let \(G\) be finite and \(H < G\).
   \[|G| = [G:H] \cdot |H|\]
\end{proposition}

\begin{corollary}[Lagrange's Theorem]
   Let \(G\) be finite and \(H < G\).
   \[|H|~\text{divides}~|G|\]
\end{corollary}

\begin{proposition}
   Let \(G\) be a group and \(H < G\).
   \[[G:H] = 2 \implies H~\text{normal}\]
\end{proposition}

\subsection{Direct \& Semidirect Product}
\begin{definition}[Product of Group Subsets]
   Given a group \(G\) and \(H, K \subset G\)
   \[HK := \{hk \mid h \in H, k \in K\}\]
\end{definition}
\begin{remark}
   As we have closure in \(G\) it holds that \(HK \subset G\).
\end{remark}

Isomorphism Theorems exist for every algebraic structure and describe the relationship between quotients, homomorphisms and subobjects.
\begin{proposition}[First Isomorphism Theorem]
   Let \(G, G'\) be groups, \(f: G \to G'\) a homomorphism with \(\ker(f) = H\), then is
   \[G_{/H} \to G' \qquad\text{where}\qquad \bar{a} \mapsto f(a)\]
   an automorphism.
\end{proposition}

\begin{proposition}[Second Isomorphism Theorem]
   Let \(G\) be a group, \(H < G\) and \(K \triangleleft G\), then is
   \begin{enumerate}[label=\roman*, align=Center]
      \item \(H \cap K \triangleleft H\)
      \item \(HK < G\)
      \item \(H/(H \cap K) \cong HK/K\) by \(\bar{a} \mapsto \bar{a}\)
   \end{enumerate}
\end{proposition}

\begin{proposition}[Third Isomorphism Theorem]
   Let \(G\) be a group and \(K \triangleleft H \triangleleft G\), then is
   \begin{enumerate}[label=\roman*, align=Center]
      \item \(H/K \triangleleft G/K\)
      \item \((G/K)/(H/K) \cong G_{/H}\) by \(\bar{a}(H/K) \mapsto \bar{a}\)
   \end{enumerate}
\end{proposition}

\begin{definition}[Direct Product]
   Given the groups \(G, H\), the set \(G \times H\) with the operation
   \[\ast: (G \times H) \times (G \times H) \to G \times H \qquad\text{where}\qquad (g, h) \ast (g', h') := (g \ast g', h \ast h')\]
\end{definition}
\begin{remark}[Notation]
   For abelian groups the product may also be denoted with \(\oplus\) (c.f. \(\mathbb{Z}\)-modules).
\end{remark}
\begin{remark}
   With a direct product, we get some natural group homomorphisms for free: the projection maps defined by
   \[G \times H \to G \qquad \text{where}\qquad (g, h) \mapsto g\]
   \[G \times H \to H \qquad \text{where}\qquad (g, h) \mapsto h\]
\end{remark}

\begin{definition}[Outer Semidirect Product]
   Given the groups \(G, H\) and the group homomorphism \(H \to \Aut(G)\) where \(h \mapsto \varphi(h) =: \varphi_h\), the set \(G \rtimes_\varphi H := G \times H\) with operation
   \[\ast: (G \rtimes_\varphi H) \times (G \rtimes_\varphi H) \to G \rtimes_\varphi H \qquad\text{where}\qquad (g_1, h_1) \ast (g_2, h_2) := \big(g_1 \ast \varphi_{h_1}(g_2), h_1 \ast h_2\big)\]
\end{definition}
\begin{remark}
   Since \(\varphi_h\) is a homomorphism we have
   \[\varphi_{g \ast h}(k) = \varphi_g\big(\varphi_h(k)\big) \qquad\text{and}\qquad \varphi_g(h) \ast \varphi_g(k) = \varphi_g(h \ast k)\]
\end{remark}

\begin{proposition}\label{pro:dir_semidir_prod}
   Let \(G\) be a group and \(H, K < G\) furthermore let
   \[f: H \times K \to HK \quad\text{where}\quad (h,k) \mapsto hk \qquad\text{and}\qquad \varphi_k(h) := k \ast h \ast k^{-1}\]

   \begin{enumerate}[label=\roman*, align=Center]
      \item \(H \cap K = \{e_G\} \iff f~\text{is bijective}\)
      \item \(HK = KH \iff H \ast K < G\)
      \item If \(H \cap K = \{e_G\}\) and \(G\) is finite with \(|G| = |H| \cdot |K|\), then \(HK = G\).
      \item If \(H \cap K = \{e_G\}\) and \(H, K\) are normal, then is \(HK \triangleleft G\) and \(f\) is an isomorphism. \\
      - if also \(HK = G\) then is \(H \times K \cong G\) by \(f\).
      \item If \(\forall k \in K: kH = Hk\) then is \(H \triangleleft HK) < G\) and \(k \mapsto \varphi_k\) defines a homomorphism \(K \to \Aut(H)\) such that we have a surjective homomorphism \(H \rtimes_{\varphi} K \to H \ast K\) by \((h,k) \mapsto h \ast k\) which is an isomorphism iff \(H \cap K = \{e_G\}\). \\
      - if also \(H \ast K = G\), then is \(H \rtimes K \cong G\).
   \end{enumerate}
\end{proposition}
\begin{remark}
   In (iv) we may describe \(G\) as the direct product of \(H\) and \(K\).
   Alternatively we can say that
   \(G \cong M \times N \iff \exists H, K < G\) such that
   \begin{enumerate}[label=\roman*, align=Center]
      \item \(H \cong M\) and \(K \cong N\)
      \item \(H \triangleleft G\) and \(K \triangleleft G\)
      \item \(H \cap K = \{e_G\}\)
      \item \(G = HK\)
   \end{enumerate}

   In (v) we may describe \(G\) as the semidirect product of \(H\) and \(K\).
   Alternatively we can say that
   \(G \cong M \rtimes N \iff \exists H, K < G\) such that
   \begin{enumerate}[label=\roman*, align=Center]
      \item \(H \cong M\) and \(K \cong N\)
      \item \(H \triangleleft G\)
      \item \(H \cap K = \{e_G\}\)
      \item \(G = HK\)
   \end{enumerate}
\end{remark}

\subsection{Isomorphism Classes of Small Groups}
The goal of this section is to determine the isomorphism classes of groups with order of at most 10.
An isomorphism class is a collection of mathematical objects isomorphic to each other.
Isomorphism classes are often defined if the exact identity of the elements of the set is considered irrelevant, and the properties of the structure of the mathematical object are studied.

\begin{center}
   \begin{tabular}{|c|l|c|}
      \(|G|\) & \(G\) abelian & \(G\) non-abelian \\
      \hline
      1 & trivial                                & none\\
      2 & \(\mathbb{Z}_{/2\mathbb{Z}}\)          & none \\
      3 & \(\mathbb{Z}_{/3\mathbb{Z}}\)          & none \\
      4 & \(\mathbb{Z}_{/4\mathbb{Z}}\), \(V_4\) & none \\
      5 & \(\mathbb{Z}_{/5\mathbb{Z}}\)          & none \\
      \hline
      6  & \(\mathbb{Z}_{/6\mathbb{Z}}\)         & \(S_3 \cong D_3\) \\
      7  & \(\mathbb{Z}_{/7\mathbb{Z}}\)         & none \\
      8  & \(\mathbb{Z}_{/8\mathbb{Z}}\), \(\mathbb{Z}_{/2\mathbb{Z}} \oplus \mathbb{Z}_{/4\mathbb{Z}}\), \(\mathbb{Z}_{/2\mathbb{Z}} \oplus \mathbb{Z}_{/2\mathbb{Z}} \oplus \mathbb{Z}_{/2\mathbb{Z}}\)           & \(D_4\), \(Q_8\) \\
      9  & \(\mathbb{Z}_{/9\mathbb{Z}}\), \(\mathbb{Z}_{/3\mathbb{Z}} \oplus \mathbb{Z}_{/3\mathbb{Z}}\) & none \\
      10 & \(\mathbb{Z}_{/10\mathbb{Z}}\)        & \(D_5\) \\
   \end{tabular}
\end{center}

We can fill in the bulk of this table abstractly.
In the case where \(G\) is abelian, we can utilize the fundamental theorem of abelian groups to imediately fill in the left column of the table.

By Lagrange's theorem, if \(|G| = p\) where \(p\) is prime, then \(G\) is generated by any of its non-identity elements.
This means that \(G\) is cyclic for every \(g\).

\begin{proposition}
   Let \(G\) be a group where every element is of order 2, then
   \begin{enumerate}[label=\roman*, align=Center]
      \item \(|G| = 2^m\) % TODO: true? its a power of 2... but 2^m?
      \item \(G \cong (\mathbb{Z}_{/2\mathbb{Z}})^m\)
      \item \(G\) is abelian
   \end{enumerate}
\end{proposition}

This means that every group of order 4 is abelian.

\begin{definition}[Klein Four-Group]
   \[V_4 := \mathbb{Z}_{/2\mathbb{Z}} \oplus \mathbb{Z}_{/2\mathbb{Z}}\]
\end{definition}

\begin{proposition}
   Every group of order 9 is abelian.
\end{proposition}

\begin{proposition}
   For \(p \in \{3, 5\}\), every non-abelian group of order \(2p\) is isomorphic to \(D_p\).
\end{proposition}

\begin{definition}[Dihedral Group]
   Given \(\mathbb{Z}_{/2\mathbb{Z}} \to \Aut(\mathbb{Z}_{/n\mathbb{Z}})\) where \(i \mapsto ([m] \mapsto (-1)^i \cdot [m])\)
   \[D_n := \mathbb{Z}_{/n\mathbb{Z}} \rtimes \mathbb{Z}_{/2\mathbb{Z}}\]
\end{definition}
\begin{remark}[Intuition]
   A regular polygon with \(n\) sides has \(2n\) different symmetries: \(n\) rotational symmetries and \(n\) reflection symmetries.
   Usually, we take \(n \geq 3\) here.
   Then \(D_n\) is non-abelian.
   Also \(D_3 \cong S_3\).

   The associated rotations and reflections make up the dihedral group \(D_n\).

   \begin{center}
      \input{drawings/dn_example.tex}
   \end{center}

   If \(n\) is odd, each axis of symmetry connects the midpoint of one side to the opposite vertex.
   If \(n\) is even, there are \(\frac{n}{2}\) axes of symmetry connecting the midpoints of opposite sides and \(\frac{n}{2}\) axes of symmetry connecting opposite vertices.
   In either case, there are \(n\) axes of symmetry and \(|D_n| = 2n\) elements in the symmetry group.
\end{remark}

\begin{proposition}
   Every non-abelian group of order 8 is isomorphic to \(D_4\) or \(Q_8\).
\end{proposition}

\begin{definition}[Quaternion Group]
   \[Q_8 = \langle \bar{e},i,j,k \mid \bar{e}^{2} = e,~ i^2 = j^2 = k^2 = ijk = \bar{e}\rangle\]
\end{definition}

\subsection{Group Actions on Sets}
A \emph{group action} is a formal way of interpreting the manner in which the elements of a group correspond to transformations of some set.
For a given (finite) set, \(S_n\) is an abstraction used to describe the permutations of elements of that set.

\begin{definition}[Group Action on Set]
   Given a group \(G\) and a set \(X\),
   \[\cdot: G \times X \to X \qquad\text{where}\qquad (g, x) \mapsto g \cdot x\]
   if the following holds
   \begin{enumerate}[label=\roman*, align=Center]
      \item \(\forall g, h \in G, x \in X: g \cdot (h \cdot x) = (gh) \cdot x\)
      \item \(\forall x \in X: e_G \cdot x = x\)
   \end{enumerate}
\end{definition}
\begin{remark}
   We say a group \(G\) \emph{is acting on} a set \(X\).
   The definition above is a \emph{left} group action; we would define a \emph{right} group action analogously.
\end{remark}
\begin{example}[Left-Multiplication]
   \(a \cdot b := ab\).
\end{example}
\begin{example}[Inverse Right-Multiplication]
   \(a \cdot b := ba^{-1}\)
\end{example}
\begin{example}[Conjugation]
   \(a \cdot b := aba^{-1}\)
\end{example}
% TODO
% \(x \in G\) defines a homomorphism
% \[\phi_x: G \to G \qquad\text{where}\qquad g \mapsto xgx^{-1}\]
% This is a homomorphism since
% \[\phi_x(g) \phi_x(h) = xgx^{-1}xhx^{-1}=xghx^{-1} = \phi_x(gh)\]
% We call the operation of \(\phi_x\) on \(G\) by \(\phi_x\) \emph{conjugation by x}.

\subsubsection{Orbits of Group Actions}
\begin{definition}[Orbit]
   Given a group \(G\) acting on a set \(X\) and some \(x \in X\),
   \[G \cdot x := \{g \cdot x \mid g \in G\}\]
\end{definition}
\begin{remark}[Notation]
   The set of all orbits is denoted \(G\backslash X\).
\end{remark}
\begin{remark}[Intuition]
   The orbit of \(x\) is everything that can be reached from \(x\) by an action of something in \(G\).
\end{remark}
\begin{remark}
   An orbit under the conjugation action is called a \emph{conjugacy class}.
\end{remark}

\begin{proposition}
   Let \(G\) be a group acting on a set \(X\).
   \begin{enumerate}[label=\roman*, align=Center]
      \item \[\forall x, x' \in X: \big((G \cdot x) = (G \cdot x')\big) \lor \big((G \cdot x) \cap (G \cdot x') = \emptyset\big)\]
      \item \[\bigcup_{x \in X} (G \cdot x) = X\]
   \end{enumerate}
\end{proposition}

\begin{definition}[Transitive Group Action]
   A group action which has exactly one orbit.
\end{definition}
\begin{remark}
   Equivalently we could define \(\exists x \in X: G \cdot x = X\) or \(\forall x \in X: G \cdot x = X\).
\end{remark}

\begin{definition}[Equivariant Map]
   Given group actions on sets \(X\) and \(Y\), \(f: X \to Y\) is \emph{equivariant} iff
   \[\forall g \in G, x \in X: f(g \cdot x) = g \cdot f(x)\]
\end{definition}

\begin{definition}[Invariant Subset]
   Given a group \(G\) acting on a set \(X\), \(X' \subset X\) if
   \[\forall g \in G, x' \in X': g \cdot x' \in X'\]
\end{definition}
\begin{remark}
   For some \(x \in X\) we say \(G \cdot x\) the orbit of \(x\).
   The set of all orbits is denoted \(G\backslash X\).
   This means, the original action \(G \times X \to X\) restricts to an action \(G \times X' \to X'\), in which case the inclusion map \(X' \to X\) is equivariant.
\end{remark}
\begin{example}
   If \(H < G\) then we have this situation for \(a \cdot b := ab\) with \(X' := G_{/H}\).
   The induced action of \(G\) on \(G_{/H}\) is transitive.

   For a general action, any orbit \(G \cdot x\) is invariant.
\end{example}

\begin{proposition}[Orbit Formulas]
   Let \(G\) be a group acting on \(X\) and \(x, x' \in X\)
   \begin{enumerate}[label=\roman*, align=Center]
      \item \(G \cdot x \to G/G_x\) where \(g \cdot x \mapsto gG_x\) is well-defined, bijective and equivariant.
      \item For \(g \in G: g \cdot x = x' \implies G_{x'} = gG_xg^{-1}\)
      \item If \(G\) is finite, then \(|G \cdot x| = [G : G_x]\)
      \item If \(G\) and \(X\) are finite, then
         \[|X| = \sum_{G \cdot x \in G \backslash X} [G : G_x]\]
   \end{enumerate}
\end{proposition}
\begin{remark}
   Applied to the conjugation action of a finite group \(G\), we obtain the \emph{class equation}
   \[|G| = |Z(G)| + \sum_{i=1}^n [G : C(x_i)]\]
   with representatives \(x_i\) of the conjugacy classes of non-central elements of \(G\).
\end{remark}

\begin{proposition}[Cauchy's Theorem]
   Let \(G\) be a finite group and \(p \in \mathbb{N}\) prime.
   \[p~|~|G| \implies \exists g \in G: |\langle g\rangle| = p\]
\end{proposition}

\subsubsection{Important Families of Subgroups}
\begin{definition}[Centralizer]
   Given a group \(G\) and some \(a \in G\),
   \[C(a) := \{g \in G \mid ga = ag\} < G\]
\end{definition}
\begin{remark}
   \(ga = ag \iff gag^{-1} = a\)
\end{remark}

\begin{definition}[Center]
   Given a group \(G\),
   \[Z(G) := \{g \in G \mid \forall a \in G: ga = ag\} < G\]
\end{definition}
\begin{remark}
   \[Z(G) = \bigcap_{a \in G} C(a)\]
\end{remark}

\begin{definition}[Stabilizer]
   Given a group \(G\), acting on a set \(A\) and some \(a \in A\)
   \[G_a := \{g \in G \mid g \cdot a = a\} < G\]
\end{definition}
\begin{remark}[Terminology]
   We say a \emph{fixed point} of some \(g \in G\) is some \(a \in A\) s.t. \(g \cdot a = a\).
   To express the condition \(g \cdot a = a\) we may say, \emph{\(g\) fixes \(a\)}.
\end{remark}
\begin{remark}[Intuition]
   The stabilizer of \(x\) is the set of all elements in \(G\) which don't move \(x\) when they act on it.
\end{remark}

A stabilizer for the conjugation action on \(\mathcal{P}(G)\) is a \emph{normalizer}.

\begin{definition}[Normalizer]
   Given a group \(G\) and \(A \in \mathcal{P}(G)\)
   \[N(A) := \{g \in G \mid gAg^{-1} = A\} < G\]
\end{definition}
\begin{remark}
   \(gAg^{-1} := \{gag^{-1} \mid a \in A\}\)
\end{remark}

\begin{definition}[Faithful Group Action]
   Given a group \(G\) acting on a set \(X\)
   \[\{g \in G \mid \forall x \in X: g \cdot x = x\} = \{e_G\}\]
\end{definition}
\begin{remark}
   Equivalently we can say that \(G \to \Perm(X)\) is injective.
\end{remark}

\begin{definition}[Free Group Action]
   A group action whose stabilizers are all trivial.
\end{definition}

\begin{definition}[Simply Transitive Group Action]
   A group action which is transitive and free.
\end{definition}

\subsubsection{Group Actions \& Permutations}
Let the group \(G\) act on the set \(X\).
For each fixed \(g \in G\) we get a map
\[\sigma_g: X \to X \qquad\text{where}\qquad x \mapsto g \cdot x\]
which is a \emph{permutation} of \(X\).
To see this we show that we have an inverse \(\sigma_{g^{-1}}\), so let \(x \in X\) be arbitrary, then
\[(\sigma_{g^{-1}} \circ \sigma_g)(x) = \sigma_{g^{-1}}\big(\sigma_g(x)\big) = g^{-1} \cdot (g \cdot x) = (g^{-1}g) \cdot x = e_G \cdot x = x\]
since \(g\) was arbitrary we also see that \(\sigma_g \circ \sigma_{g^{-1}} = \id_X\).
This means \(\sigma_g\) has a two-sided inverse, hence is a permutation of \(X\).

The next observation is that
\[\varphi: G \to S_X \qquad\text{where}\qquad g \mapsto \sigma_g\]
is a group homomorphism.
Note that the first observation shows that \(\sigma_g\) is indeed an element of \(S_X\).
So we only need to show compatibility with the group operations, so let \(x \in X\) be arbitrary
\[\varphi(gg')(x) = \sigma_{gg'}(x) = (gg') \cdot x = g \cdot (g' \cdot x) = \sigma_g\big(\sigma_{g'}(x)\big) = \big(\varphi(g) \circ \varphi(g')\big)(x)\]

So intuitively, a group action of \(G\) on a set \(X\) just means that every element \(g \in G\) acts as a permutation on \(X\) in a manner consistent with the group operations in \(G\), which is precisely what the two observations state.

The homomorphism \(\varphi\) above is called the \emph{permutation representation} associated to the given action.
It is easy to see that this process is reversible in the sense that if \(\varphi\) is any homomorphism, then
\[\cdot: G \times X \to X \qquad\text{where}\qquad (g, x) \mapsto g \cdot x := \varphi(g)(x)\]
satisfies the properties of a group action of \(G\) on \(X\).
Thus actions of \(G\) on \(X\) and the homomorphisms \(\varphi\) are in bijective correspondece (i.e. they are essentially the same notion, phrased in different terminology).

\begin{proposition}
   Let \(G\) be a group and \(X\) a set.

   There is a bijection between the actions of \(G\) on \(X\) and the homomorphisms \(\varphi: G \to S_X\).
\end{proposition}
\begin{remark}
   We have that \(\{g \in G \mid \forall x \in X: g \cdot x = x\} \triangleleft G\) since it is the kernel of the associated permutation representation.
\end{remark}

\subsection{Sylow Theorems}
\begin{definition}[\(p\)-Group]
   A group \(G\) where \(|G| = p^a\) with \(a \in \mathbb{N}_{>0}\).
\end{definition}
\begin{remark}
   A group in which each element has a power of \(p\) as order.
\end{remark}

\begin{definition}[Sylow \(p\)-Subgroup]
   Given a \(p\)-group \(G\), \(H < G: \lvert H\rvert = p^k\) if \(p^k\) is the highest power of a prime \(p\) s.t. \(p^k \mid \lvert G\rvert\).
\end{definition}
\begin{remark}
   A subgroup of a \(p\)-group is again a \(p\)-group.
\end{remark}

Let \(p\) be a prime s.t. \(p \mid \lvert G\rvert\) and largest \(a \in \mathbb{N}_{>0}\) s.t. \(p^a \mid \lvert G\rvert\).
\begin{theorem}[First Sylow Theorem]
   There exists a Sylow \(p\)-subgroup, i.e. a subgroup of \(G\) with order \(p^a\).
\end{theorem}
\begin{theorem}[Second Sylow Theorem]
   Any pair of Sylow \(p\)-subgroups are conjugate to each other, and any \(p\)-subgroup sits inside some Sylow \(p\)-subgroup.
\end{theorem}
\begin{theorem}[Third Sylow Theorem]
   Let \(n_p\) be the number of Sylow \(p\)-subgroups of \(G\), then
   \[n_p \mid \frac{|G|}{p^a} \qquad\text{and}\qquad n_p \equiv 1 \mod p\]
\end{theorem}
\begin{remark}
   \(n_p = 1 \iff\) there exists a unique, normal Sylow \(p\)-subgroup.
   Also \(q \not\equiv 1 \mod p \implies n_p = 1\)
\end{remark}

\begin{definition}[Composition Series]
   A sequence of subgroups in a group \(G\),
   \[\{e_G\} =: G_0 \triangleleft G_1 \triangleleft \ldots \triangleleft G_k = G\]
   where the \emph{composition factors} \({G_j}_{/G_{j-1}}\) are simple \(\forall j = 1, \ldots, k\)
\end{definition}
\begin{remark}
   We call \(k\) the \emph{composition length}.
\end{remark}

\begin{definition}[Solvable Group]
   A group \(G\) with a composition series such that \(\forall j = 1, \ldots, k: {G_j}_{/G_{j-1}}\) is abelian.
\end{definition}

\section{Rings}
As we don't like to list all necessary axioms (closure, associativity, inverse, identity) again, but still be reminded of them we collapse them into axiom (i).
\begin{definition}[Ring]
   A set \(R\) with two operations
   \[+: R \times R \to R \quad\text{where}\quad (a, b) \mapsto a \dotplus b\]
   \[\cdot: R \times R \to R \quad\text{where}\quad (a, b) \mapsto a \cdot b\]
   if the following holds:
   \begin{enumerate}[label=\roman*, align=Center]
      \item \((R, +, 0)\) is an abelian group, with \(a^{-1} := -a\).
      \item Associativity: \(\forall a, b, c \in R: (a \cdot b) \cdot c = a \cdot (b \cdot c)\)
      \item Distributivity: \(\forall a, b, c \in R: a \cdot (b + c) = ab + bc\) and \((a + b) \cdot c = ac + bc\)
   \end{enumerate}
\end{definition}
\begin{remark}
   Further properties follow from the definition, such as
   \[\forall a \in R: 0 \cdot a = 0 = a \cdot 0 \quad\text{and}\quad (-1) \cdot a = -a = a \cdot (-1)\]
\end{remark}

\begin{definition}[Commutative Ring]
   A ring where the multiplication is commutative
   \[\forall a, b \in R: a \cdot b = b \cdot a\]
\end{definition}

\begin{definition}[Unitary Ring]
   A ring which contains a neutral element of the multiplication
   \[\exists 1 \in R: 1 \cdot a = a = a \cdot 1\]
\end{definition}

\begin{definition}[Units of a Ring]\label{def:units_ring}
   Given a ring \((R, +, \cdot, 0, 1)\), its \textit{units} is the set of all multiplicative invertible elements
   \[R^\times := \{r \in R \mid \exists r' \in R: r \cdot r' = 1 = r' \cdot r\}\]
\end{definition}
\begin{remark}
   \((R^{\times}, 1, \cdot)\) is a group (abelian if \(R\) is commutative).
\end{remark}
\begin{remark}
   We have the situation
   \[0 \in R^\times \iff 1 = 0 \iff R = \{0\}\]
   where we call \(R\) the \emph{zero ring}.
\end{remark}
\begin{example}
   We've proven that \((\mathbb{Z}, +, \cdot, 0, 1)\) is a ring.
   Its units are
   \[\mathbb{Z}^\times = \{-1, 1\}\]
   since only \((-1) \cdot (-1) = 1\) and \(1 \cdot 1 = 1\).
\end{example}

\begin{definition}[Subring]
   Given a ring \(R\), \(Q \subset R\) is a subring iff
   \begin{enumerate}[label=\roman*, align=Center]
      \item \((Q, \dotplus)\) is a subgroup of \((R, \dotplus)\)
      \item Closure: \(\forall a, b \in R: a \cdot b \in R\)
   \end{enumerate}
\end{definition}

\begin{definition}[Unitary Subring]
   A subring \(Q \subset R\) where \(1 \in Q\).
\end{definition}

\begin{definition}[Quotient Ring]
   Given a commutative ring \(R\) with an ideal \(I \subset R\), then is
   \[R_{/I} := \{a + I \mid a \in R\}\]
   where
   \begin{enumerate}[label=\roman*, align=Center]
      \item \(\forall a, b \in R_{/I}: (a + I) + (b + I) = (a + b) + I\)
      \item \(\forall a, b \in R_{/I}: (a + I) \cdot (b + I) = (a \cdot b) + I\)
   \end{enumerate}
   a ring.
\end{definition}
\begin{remark}
   \[I~\text{is maximal} \iff R_{/I}~\text{is a field}\] % TODO: residue field
   \[I~\text{is prime} \iff R_{/I}~\text{is an integral domain}\]
   With the remark of \cref{def:max_ideal} we also have
   \[R_{/I}~\text{is a field} \implies R_{/I}~\text{is an integral domain}\]
\end{remark}

\subsection{Important Examples}
\subsubsection{Whole Numbers}
\begin{proposition}[Commutative Unitary Ring \(\mathbb{Z}\)]\label{pro:ring_Z}
   \(\mathbb{Z}\) is a commutative unitary ring
\end{proposition}
% TODO
\begin{proof}
   We write \(\mu_a\) for the unique endomorphism of \(\mathbb{Z}\) which maps 1 to \(a\).
   This allows us to use \(\mu_a\) as multiplication with \(a\):
   \[\cdot: \mathbb{Z} \times \mathbb{Z} \to \mathbb{Z} \quad\text{where}\quad a \cdot b := \mu_a(b)\]
   \[\mu_a(1) = 1 \cdot a, \quad \mu_a(2) = 2 \cdot a, \quad\ldots\]
   The following rules apply
   \[(\mu_b + \mu_c)(1) = \mu_{b + c}(1)\]
   \[(\mu_a \circ \mu_b)(1) = \mu_a(\mu_b(1)) = \mu_a(b) = a \cdot b\]

   From \cref{pro:z_abelian} is \((\mathbb{Z}, +, 0)\) an abelian group.

   The multiplication is associative
   \[a \cdot (b \cdot c) = \mu_a(\mu_b(c)) = (\mu_a \circ \mu_b)(c) = \mu_{a \cdot b}(c) = (a \cdot b) \cdot c\]

   Distributivity holds for \(a, b, c \in \mathbb{Z}\)
   \[a \cdot (b + c) = \mu_a \cdot (b + c) = \mu_a(b) + \mu_a(c) = a \cdot b + a \cdot c\]
   \[(b + c) \cdot a = \mu_{b + c}(a) = (\mu_b + \mu_c)(a) = b \cdot a + c \cdot a\]

   It is a unitary ring with the neutral element \(\mu_1 := \id_{\mathbb{Z}}\)
   \[\forall a \in \mathbb{Z}: \mu_a(1) = a \implies 1 \cdot a = a = a \cdot 1\]

   It is a commutative ring \(\forall m, n \in \mathbb{Z}: m \cdot n = n \cdot m\).
   We prove this by induction over \(n\).

   \textit{IB:} \(m \cdot 0 = 0 = 0 \cdot m\)

   \textit{IH:} For some \(n \in \mathbb{Z}\) holds \(m \cdot n = n \cdot m\).

   \textit{IS:} Assume IH holds.
   \[m \cdot (n + 1) = m \cdot n + m = n \cdot m + m = (n + 1) \cdot m\]
\end{proof}

\subsubsection{Matrix Ring}
\begin{proposition}[Matrix Ring]
   \((\Mat_n(K), +, \cdot, I_n, 0_n)\) is a ring.
\end{proposition}
\begin{remark}
   The endomorphism which is represented by \(I_n\) is \(\id_{K^n}\).
\end{remark}

\subsubsection{Polynomial Ring}
\begin{example}[Polynomial Rings]
   Given a field \(K\) we define
   \[K[x] := \{p(x) \mid \text{polynomials with coefficients in}~K\}\]
   Then is \((K[x], +, \cdot, 0_K, 1_K)\) an integral domain, where we define
   \[+: K[x] \times K[x] \to K[x] \quad\text{through}\quad (p, q) \mapsto \sum_{i=0}^n p_i \cdot x^i + \sum_{j=0}^m q_j \cdot x^j = \sum_{k=0}^{\max(m,n)} (p_k + q_k) \cdot x^k\]
   \[\cdot: K[x] \times K[x] \to K[x] \quad\text{through}\quad (p, q) \mapsto \sum_{i=0}^n p_i \cdot x^i \cdot \sum_{j=0}^m q_j \cdot x^j = \sum_{k=0}^{m+n} \left(\sum_{i=0}^k (p_i \cdot q_{k-i}) \cdot x^k\right)\]
   It holds that \(K[x]^{\times} = K^{\times}\).
\end{example}

\subsection{Integral Domains}
\begin{definition}[Integral Domain]\label{def:integral_domain}
   A commutative unitary ring \(R\) for which holds
   \[\forall a, b \in R \setminus \{0\}: a \cdot b \neq 0\]
\end{definition}
\begin{remark}
   Any ring which admits an injective ring homomorphism to a field is an integral domain.
\end{remark}
\begin{remark}
   If \(\exists a, b \in R \setminus \{0\}: a \cdot b = 0\) we call \(a\) and \(b\) \emph{zero-divisor}, hence an integral domain is \emph{free} of zero dividers.
   \[\forall a, b \in R: a \cdot b = 0 \implies a = 0 \lor b = 0\]
\end{remark}
\begin{remark}
   In abstract algebra, the \emph{field of fractions} of an integral domain is the smallest field in which it can be embedded.
   For example, the field of fractions of the ring of integers \(\mathbb{Z}\) is the field of rationals \(\mathbb{Q}\).
\end{remark}
\begin{example}[Integral Domain \(\mathbb{Z}\)]
   According to \cref{pro:ring_Z} is \(\mathbb{Z}\) a commutative unitary ring.
   \[\forall a, b \in \mathbb{Z} \setminus \{0\}: a \cdot b \neq 0\]
   hence \((\mathbb{Z}, +, \cdot)\) is an integral domain.
\end{example}

\subsubsection{Factorization}
\begin{proposition}
   Let \(a \in R \setminus (R^\times \cup \{0\})\) of an integral domain \(R\).
   \begin{enumerate}[label=\roman*, align=Center]
      \item \(\forall b \in R: a\nmid b \exists c \in R: a | cb -1\)
      \item \(a\) is prime.
      \item \(a\) is irreducible.
   \end{enumerate}

   If \(R\) is an integral domain, then holds \((i) \implies (ii) \implies (iii)\).

   If \(R\) is a principal ideal domain, then holds \((i) \iff (ii) \iff (iii)\).
\end{proposition}

\begin{definition}[Divisor]
   Given an integral domain \(R\), \(a \in R\) is a divisor of \(x \in R\) iff \(a | x\).
\end{definition}

\begin{definition}[Proper Divisor]
   Given an integral domain \(R\), \(a \in R \setminus R^\times\) is a \emph{proper} divisor of \(x \in R\) iff \(a | x\) and \(a \neq ux\) with \(u \in R^\times\).
\end{definition}

\begin{lemma}
   Let \(R\) be an integral domain satisfying the ascending chain condition on principal ideals.

   Every \(a \in R \setminus R^\times: a \neq 0\) possesses an irreducible divisor.
\end{lemma}
\begin{remark}
   The ascending chain condition on principal ideals is satisfied if there is no infinite strictly ascending chain of principal ideals in the ring.
   \[a_1R \subset a_2R \subset a_3R \subset \ldots\]
   Any principal ideal domain satisfies this property for instance.
\end{remark}

\begin{proposition}
   Let \(R\) be an integral domain satisfying the ascending chain condition on principal ideals.

   Every \(a \in R \setminus R^\times: a \neq 0\) may be expressed as a finite product of irreducible elements of \(R\).
\end{proposition}

\subsection{Euclidean Domains}
A \emph{euclidean domain} is endowed with a \emph{euclidean function} which allows a suitable generalization of the euclidean algorithm for the division of integers or polynomials which is called \emph{euclidean division}.

\begin{definition}[Euclidean Function]
   Given an integral domain \(R\).
   \(f: R \setminus \{0\} \to \mathbb{N}\) is a euclidean function if
   \[\forall a, b \in R: b \neq 0~\exists q, r \in R: a = qb + r \quad\text{where}\quad r = 0 \lor f(r) < f(b)\]
\end{definition}
\begin{remark}
   It will be handy require an euclidean value to fullfill
   \begin{equation}\label{eq:extended_euclidean}
      \forall a,b \in R \setminus \{0\}: f(a) \leq f(ab)
   \end{equation}
   this means that
   \[f(a) = f(ab) \iff b \in R^{\times}\]
   \(\impliedby\) holds because
   \[(ab)b^{-1} = a \implies f(ab) \leq f(a) \land f(a) \leq f(ab) \implies f(a) = f(ab)\]
   and \(\implies\) holds because
   \[a = qab + r~\text{with}~r=0 \lor f(r) = f(a(1-qb)) < f(ab) = f(a)\]
   is a contradiction so it must hold that \(r = 0 \implies q \in R^{\times}\).

   We can always assume that an euclidean value suffices \cref{eq:extended_euclidean}, because given any euclidean value \(f_0\)
   \[f: R \setminus \{0\} \to \mathbb{N} \quad\text{where}\quad a \mapsto \min\{f_0(ab) \mid b \in R \setminus \{0\}\}\]
   defines a euclidean value that fullfills (\ref{eq:extended_euclidean}).
\end{remark}

\begin{definition}[Euclidean Domain]
   An integral domain \(R\) equipped with an euclidean function.
\end{definition}

\begin{proposition}[Euclidean Function of Whole Numbers]
   The absolute value \(|\ldots|\) on \(\mathbb{Z}\) is a euclidean value.
   \[|a| := \begin{cases} a & \text{if}~a \geq 0\\ -a & \text{if}~a < 0\end{cases}\]
\end{proposition}


\begin{proposition}[Euclidean Function of Polynomial Rings]
   The degree of a polynomial on \(K[x]\) is an euclidean value.
   For \(f \in K[x]: a_nx^n + a_{n-1}x^{n-1} + \ldots + a_1x + a_0\)
   \[\deg(f) = n\]
\end{proposition}

\subsubsection{Euclidean Division}
\begin{definition}[Greatest Common Denominator (GCD)]
   Given an integral domain \(R\) and \(a, b \in R\), \(d \in R\) is the greatest common denominator of \(a\) and \(b\) if
   \[d|a \land d|b \quad\text{and}\quad \forall r \in R: r|a \land r|b \implies r|d\]
\end{definition}
\begin{remark}
   If \(1 \in R\) is the greatest common denominator of \(a\) and \(b\) then are \(a\) and \(b\) coprime.
\end{remark}

\begin{proposition}[GCD is Unique]\label{pro:factor_unique}
   Given a euclidean domain \(R\) and \(a, b \in R\)
   \[\exists s, t, u, v \in R: (s \cdot a + t \cdot b) \cdot u = a~\text{and}~(s \cdot a + t \cdot b) \cdot v = b\]
   where \(d := s \cdot a + t \cdot b\) is unique except for multiplication with units.
\end{proposition}
\begin{remark}
   In this proposition holds that \(du = a \land dv = b \implies d|a \land d|b\).
   Since \(d\) is unique except for multiplication with units follows that all common divisors of \(a\) and \(b\) also divide \(d\), \(\forall r \in R: r|a \land r|b \implies r|d\), hence \(d\) is the GCD of \(a\) and \(b\).
\end{remark}

\begin{definition}[Euclidean Algorithm]
   Given a euclidean domain \(R\), let \(a, b \in R: b < a\), we define \(\gcd(a, b)\) to calculate the greatest common denominator of \(a\) and \(b\).
   \begin{enumerate}
      \item If \(a < b\) exchange \(a\) and \(b\).
      \item Divide \(a\) and \(b\) and get the remainder \(r\).\\
         If \(r = 0\) then is \(b = \gcd(a, b)\).
      \item Replace \(a\) by \(b\) and \(b\) by \(r\)\\
         If \(f(a) \geq f(b)\) return to the previous step.
   \end{enumerate}
\end{definition}

\begin{example}[GCD of Whole Numbers]
   Given \(a = 210\) and \(b = 45\)
   \begin{enumerate}
      \item Calculate \(a / b = q_0 + r_0 \implies q_0 = 4,~r_0 = 30\)
         \[210 = 4 \cdot 45 + 30\]
         We see \(r_0 \neq 0\) and \(|b| \geq |r_0|\), we continue
      \item Calculate \(b / r_0 = q_1 + r_1 \implies q_1 = 1,~r_1 = 15\)
         \[45 = 1 \cdot 30 + 15\]
         We see \(r_1 \neq 0\) and \(|r_0| \geq |r_1|\), we continue
      \item Calculate \(r_0 / r_1 = q_2 + r_2 \implies q_2 = 2,~r_2 = 0\)
         \[30 = 2 \cdot 15 + 0\]
         We see \(r_2 = 0\) hence we're finished and \(\gcd(a, b) = 15\).
   \end{enumerate}
\end{example}

\begin{example}[GCD of Polynomials]
   Given the polynomials \(a, b \in \mathbb{Q}[x]\)
   \[a(x) := 6x^4 + 3x^3 + x^2 + 1 \quad\text{and}\quad b(x) := 2x^2 + x + 1\]
   We use the euclidean algorithm.

   \begin{enumerate}
      \item Calculate \(a / b = q_0 + r_0 \implies q_0 = 3x^2,~r_0 = -2x^2 + 1\)
         \[6x^4 + 3x^3 + x^2 + 1 = 3x^2 (2x^2 + x + 1) + (-2x^2 + 1)\]
         We see \(r_0 \neq 0\) and \(\deg(b) \geq \deg(r_0)\), we continue
      \item Calculate \(b / r_0 = q_1 + r_1 \implies q_1 = -1,~r_1 = x+2\)
         \[2x^2 + x + 1 = (-1) (-2x^2 + 1) + (x + 2)\]
         We see \(r_1 \neq 0\) and \(\deg(r_0) \geq \deg(r_1)\), we continue
      \item Calculate \(r_0 / r_1 = q_2 + r_2 \implies q_2 = -2x,~r_2 = 4x+1\)
         \[-2x^2 + 1 = (-2x) (x+2) + (4x + 1)\]
         We see \(r_2 \neq 0\) and \(\deg(r_1) \geq \deg(r_2)\), we continue
      \item Calculate \(r_1 / r_2 = q_3 + r_3 \implies q_3 = \frac{1}{4},~r_3 = \frac{7}{4}\)
         \[x + 2 = \frac{1}{4} (4x + 1) + \frac{7}{4}\]
         We see \(r_3 \neq 0\) and \(\deg(r_2) \geq \deg(r_3)\), we continue
      \item Calculate \(r_2 / r_3 = q_4 + r_4 \implies q_4 = \frac{16}{7}x,~r_4 = 1\)
         \[4x + 1 = \left(\frac{16}{7}x\right) \frac{7}{4} + 1\]
         We see \(r_4 = 1\) which means that \(a\) and \(b\) are coprime hence \(\gcd(a, b) = 1\), we stop.
   \end{enumerate}
\end{example}
\begin{remark}
   Lastly we could use the \textit{extended} euclidean algorithm to calculate \(s\) and \(t\) of
   \[\gcd(a, b) = s \cdot a + t \cdot b\]
   This is a certifying algorithm, because the \(\gcd\) is the only number that can simultaneously satisfy this equation and divide the inputs.
   (This fact is used to prove that \(\gcd\) is unique.)

   The algorihtm is particularly usefull when \(a\) and \(b\) are coprime.
   With that provision, \(x\) is the modular multiplicative inverse of \(a \mod b\), and \(y\) is the modular multiplicative inverse of \(b \mod a\).
\end{remark}

\begin{definition}[Irreducible/Prime]
   Given an integral domain \(R\) and \(a \in R \setminus (R^{\times} \cup \{0\})\), then is \(a\)

   \emph{irreducible} if \(\forall b, c \in R: a = b \cdot c\) holds \(b \in R^{\times} \lor b' \in R^{\times}\).

   \emph{prime} if \(\forall b, c \in R: a | b \cdot c \implies a | b \lor a | c\).
\end{definition}
\begin{remark}
   In an integral domain every prime element is irreducible.
   In a euclidean domain every irreducible element is prime.
\end{remark}

\begin{proposition}[Prime \(\iff\) Irreducible]\label{pro:irreducible}
   Given a euclidean domain \(R\), let \(a \in R \setminus (R^\times \cup \{0\})\), the following statements are equivalent.
   \begin{enumerate}[label=\roman*, align=Center]
      \item \(\forall b \in R: a \nmid b \implies \exists c \in R: a | b \cdot c - 1\)
      \item \(a\) is prime.
      \item \(a\) is irreducible.
   \end{enumerate}
\end{proposition}
\begin{remark}
   This proposition states, that in a euclidean every irreducible element is prime.
\end{remark}
\begin{remark}
   Looking at the next chapter concerning residue fields we can regard \cref{pro:irreducible} as follows
   \begin{enumerate}[label=\roman*, align=Center]
      \item means that \(R_{/aR}\) is a field.
      \item says that \(R_{/aR}\) is an integral domain.
      \item means that \(a\) is irreducible.
   \end{enumerate}
\end{remark}

\begin{lemma}[Prime Factorization Exists]\label{lem:factorization}
   Let \(R\) be an euclidean domain and \(a \in R \setminus (R\times \cup \{0\})\).
   \[\exists m \in \mathbb{N}_{>0},~p_1, \ldots, p_m \in R: a = p_1 \cdot \ldots \cdot p_m\]
\end{lemma}
\begin{remark}
   We define the uniqueness of prime factors such that one representative of \(\{up \mid u \in R^\times\}\) where \(p\) is prime is chosen.
\end{remark}

\begin{proposition}[Prime Factorization is Unique]
   Let \(R\) be an euclidean domain and \(P \subset R\) a set of prime elements which contains exactly one representative of all sets \(\{up \mid u \in R^\times\}\).
   Then it holds that
   \[\forall a \in R \setminus \{0\} \exists u \in R^\times, p_1, \ldots, p_m \in P: a = u \cdot p_1 \cdot \ldots \cdot p_m\]
   where \(u\), \(m\) and \(p_1, \ldots, p_m\) are unique.
\end{proposition}

\subsection{Ideals}
Ideals generalize certain subsets of the integers, such as the even numbers or the multiples of 3.
Addition and subtraction of even numbers preserves evenness, and multiplying an even number by any other integer results in another even number; these closure and absorption properties are the defining properties of an ideal.

\begin{definition}[Ideal]
   Given a euclidean domain \(R\) the subset \(I \subset R\) is an ideal if
   \begin{enumerate}[label=\roman*, align=Center]
      \item \(0 \in I\)
      \item \(\forall a, b \in I: a - b \in I\)
      \item \(\forall a \in I, r \in R: r \cdot a \in I \land a \cdot r \in I\)
   \end{enumerate}
\end{definition}
\begin{remark}
   The set \(R\) itself forms an ideal of the ring \(R\), since it is generated by the identity element \(1_R\), hence we call it the \emph{unit ideal}.
\end{remark}
\begin{remark}
   A \emph{zero ideal} or \emph{trivial ideal} is the ring \(R = \{0\}\) and \(R = \{0, 1\}\) is a \emph{one ideal}.
\end{remark}
\begin{remark}
   In terms of modules we can also say that an ideal is a submodule, where we view \(R\) as a module over itself.
   Looking at the definition of a submodule, this translates into the following two requirements
   \begin{enumerate}[label=\roman*, align=Center]
      \item \((I, +)\) is a subgroup of \((R, +)\)
      \item \(\forall a \in R, x \in I: ax \in I\)
   \end{enumerate}
\end{remark}

\begin{definition}[Radical of Ideal]
   Given an ideal \(I \subset R\) of a commutative ring \(R\)
   \[\sqrt{I} := \{a \in R \mid a^n \in I\}\]
\end{definition}

An ideal in a ring that is generated by a single ring element through multiplication by every element of the ring is a \emph{principal ideal}.
\begin{definition}[Principal Ideal]
   Given a commutative ring \(R\) and \(a \in R\)
   \[aR := \{a r \mid r \in R\}\]
\end{definition}
\begin{remark}[Notation]
   When clear from context we may write the ideal \(aR\) as \((a)\).
   So \((1)\) denotes the unit ideal.

   A finitely generated ideal is one of the form \(a_1R + \ldots + a_nR\) which also may be written as \((a_1, \ldots, a_n)\).
\end{remark}

Among the euclidean domain \(\mathbb{Z}\), the ideals correspond to the positive integers.
Every ideal is a principal ideal consisting of the multiples of a single positive number.
However, in other rings, the ideals may be distinct from the ring elements.
Also certain properties of integers, when generalized to rings, attach more naturally to the ideals.
For instance, the prime ideals of a ring are analogous to prime numbers.

\begin{definition}[Prime Ideal]
   Given a commutative ring \(R\), an ideal \(I \subset R\) is \emph{prime} if
   \begin{enumerate}[label=\roman*, align=Center]
      \item \(I \neq R\)
      \item \(\forall a, b \in I: ab \in I \implies a \in I \lor b \in I\)
   \end{enumerate}
\end{definition}
\begin{remark}
   This generalizes the following property of prime numbers
   \[p \in R~\text{prime} \iff (\forall a,b \in R: p | ab \implies p | a \lor p | b)\]
   We can therefore say that a positive integer \(n\) is a prime number iff \(nZ\) is a prime ideal in \(\mathbb{Z}\).
\end{remark}

\begin{definition}[Maximal Ideal]\label{def:max_ideal}
   Given a commutative ring \(R\), an ideal \(I \subset R\) is \emph{maximal} if
   \begin{enumerate}[label=\roman*, align=Center]
      \item \(I \neq R\)
      \item \(\nexists J \subset R: I \subset J \subset R\)
   \end{enumerate}
\end{definition}
\begin{remark}
   \[I~\text{is maximal} \implies I~\text{is prime}\]
   This can be seen as follows.
   If \(I\) is maximal and \(ab \in I\) with \(b \notin I\) follows that
   \[\big(\exists c \in R, d \in I: ca + d = 1\big) \implies b = cab + bd \in I\]
\end{remark}

Ideals are significant because if \(R\) is a commutative ring, then the kernel of any ring homomorphism \(R \to S\) is an ideal in \(R\).
Given an ideal \(I\) of a commutative ring \(R\) the quotient \(R_{/I}\) inherits a ring structure from that of \(R\).
Furthermore there is the canonical homomorphism \(R \to R_{/I}\) where \(a \mapsto \overline{a}\) whose kernal is equal to \(I\).

\subsection{Principal Ideal Domains (PID)}
\begin{definition}[Principal Ideal Domain (PID)]
   An integral domain \(R\) in which every ideal is principal.
\end{definition}
\begin{example}
   The ring of integers \(\mathbb{Z}\).
\end{example}

\begin{lemma}
   Let \(R\) be a PID and \(a_1, \ldots, a_n \in R\) be elements which generate the unit ideal.
   \[\exists A \in GL_n(R): A = \begin{pmatrix}a_1 & \ldots & a_n\\ & \ldots & \\ & \ldots & \end{pmatrix}\]
\end{lemma}

% TODO
\begin{theorem}[Smith-Normalform over PID]
   Let \(A \in \Mat_{m,n}(R)\) over a PID \(R\)
   \begin{enumerate}[label=\roman*, align=Center]
      \item \(\exists r \in \mathbb{N}, S \in \GL_m(R), T \in \GL_n(R)\) such that
         \[SAT = \left(\begin{array}{c|c} A_r & 0\\ \hline 0 & 0 \end{array}\right) \quad\text{where}\quad
            A_r := \begin{pmatrix}
               a_1 & 0      & 0 \\
               0   & \ddots & 0 \\
               0   & 0      & a_r \\
         \end{pmatrix}\]
         with \(a_1, \ldots, a_r \in R \setminus \{0\}\) such that \(\forall i: a_i | a_{i+1}\).
      \item  If \(S' \in GL_m(R)\) and \(T' \in GL_n(R)\) are such that \(S'AT'\) is in Smith-Normalform with entries \(a'_{ii}\) then \(\exists u_1, \ldots u_r \in R^\times: a'_{ii} = u_i \cdot a_{ii}\).
   \end{enumerate}
\end{theorem}

% TODO difference to other CRT?
\begin{lemma}[Chinese Remainder Theorem]
   Let \(R\) be a commutative ring and \(a, b \in R\) be elements that generate the unit ideal.
   Then we have the isomorphism
   \[R_{/abR} \xrightarrow{\sim} R_{/aR} \oplus R_{/bR} \quad\text{where}\quad [c] \mapsto ([c], [c])\]
\end{lemma}

% TODO: difference to other structure theorem? --> only proof
\begin{theorem}[Structure Theorem for R-Moduln]
\end{theorem}

\subsection{Unique Factorization Domains (UFD)}
\begin{definition}[Unique Factorization Domain]\label{def:ufd}
   Given an integral domain \(R\) in which every \(a \in R \setminus (R^\times \cup \{0\})\) admits a factorization into irreducible elements \((y_i)_{i \in \mathbb{N}}, (z_j)_{j \in \mathbb{N}} \subset R\)
   \[a = y_1 \cdot y_2 \cdot \ldots \cdot y_n = z_1 \cdot z_2 \cdot \ldots \cdot z_m\]
   such that \(n = m\) and
   \[\exists \sigma \in S_n, (u_k)_{k \in \mathbb{N}} \subset R^\times: u_i \cdot y_i = z_{\sigma(i)}\]
\end{definition}

\begin{proposition}
   Every PID is a UFD.
\end{proposition}

\begin{proposition}\label{pro:ufd_irred_prime}
   In a UFD, every irreducible element is prime.
\end{proposition}
% TODO: where to put the notation?
\begin{example}
   We regard the integral domain \(R := \mathbb{Z}[\sqrt{10}] := \{a + b\sqrt{10} \mid a, b \in \mathbb{Z}\}\) of the field \(\mathbb{Q}(\sqrt{10})\).
   We use \cref{pro:ufd_irred_prime} to show that \(R\) is \emph{not} a unique factorization domain.

   We take the irreducible element \(2 \in \mathbb{Z}[\sqrt{10}]\)
\end{example}

\begin{definition}[Primitive Polynomial]
   Given an integral domain \(R\), \(p \in R[x]\) is primitive if \(p(x) \neq 0\) and its coefficients have no common divisor in \(R \setminus R^\times\).
\end{definition}
\begin{example}
   % TODO: verify
   In \(\mathbb{Z}[x]\), \(2x^2 + 3x + 5\) is primitive while \(4x^2 + 6x + 10\) is not.
\end{example}

\begin{lemma}\label{lem:make_poly_prim}
   Let \(R\) be a UFD with field of fractions \(K\) and \(p \in K[x]: p \neq 0\).
   \[\exists!c \in K^\times: c^{-1} \cdot p~\text{has coefficients in}~R~\text{and is primitive in}~R[x]\]
   Furthermore if \(p\) has coefficients in \(R\), then is \(c \in R\).
\end{lemma}
\begin{remark}
   \(c\) is unique up to multiplication with a unit of \(R\).
\end{remark}

\begin{lemma}[Gauss's Lemma]
   Let \(R\) be a UFD and \(p, q \in R[x]\) be primitive, then is \(p \cdot q\) also primitive.
\end{lemma}

For the proof of this lemma we need some additional knowledge.
Let \(R\) be a commutative ring and \(I\) and ideal.

The polynomial ring \(R[x]\) contains \(R\) as a subring \(R \subset R[x]\), this way \(I\) generates an ideal which we denote by
\[IR[x] := \left\{\sum_{i=0}^n a_i x^i \in R[x] ~\middle|~ a_i \in I~\forall i\right\}\]

Since the quotient ring \(R_{/I}\) again is a commutative ring we can also consider the polynomial ring \(R_{/I}[x]\).

To a general homomorhpism of commutative rings \(\varphi: R \to S\) we also have
\[R[x] \to S[x] \qquad\text{where}\qquad \sum_{i=0}^n a_i x^i \mapsto \sum_{i=0}^n \varphi(a_i) x^i\]
wich is injective respectively surjective if \(\varphi\) is.

Applied to the canonical homomorphism \(R \to R_{/I}\), which is surjective we get a surjective ring homomorphism \(R[x] \to R_{/I}[x]\).
Now by the First Isomorphism Theorem we have
\[R[x]_{/IR[x]} \cong R_{/I}[x]\]
where for \(f := \sum_{i=0}^n a_i x^i\) we have \(\overline{f} \mapsto \sum_{i=0}^n \overline{a_i}x^i\).

\begin{proposition}
   Let \(R\) be a UFD with field of fractions \(K\).

   A primitive \(p \in R[x]: \deg(p) > 0\) is irreducible in \(R[x]\) iff \(p\) is irreducible in \(K[x]\).
\end{proposition}

\begin{proposition}
   Let \(R\) be a UFD with field of fractions \(K\).

   \[\{p \in R[x] \mid p~\text{irreducible}\}\]
   consists of elements from
   \[\{p \in R[x] \mid \deg(p) = 0 \land p~\text{irreducible in}~R\}\]
   and
   \[\{p \in R[x] \mid \deg(p) > 0 \land p~\text{irreducible in}~K[x] \land p~\text{primitive}\}\]
\end{proposition}
\begin{remark}
   This proposition states, that the irreducible elements in \(R[x]\) are made up of the constant polynomials which are irreducible in \(R\) and the primitive polynomials which are irreducible in \(K[x]\).
   However this gives us duplicated elements, so we need to make sure that we retain the original set membership as a distinguishing characteristic of the irreducible elements of \(R[x]\).
\end{remark}

\begin{theorem}
   Given a UFD \(R\), \(R[x]\) is a also a UFD.
\end{theorem}
\begin{proof}
   Let \(p \in R[x] \setminus (R^\times \cup \{0\})\), we have the following three cases.

   \textbf{Case 1 (\(\deg(p) = 0\))}: In this case we know that the factorizations in \(R[x]\) will be the same as in \(R\).
   Since \(R\) is a UFD, we know that there exists a unique factorization into irreducible elements, hence \(R[x]\) is a UFD.

   \textbf{Case 2 (\(\deg(p) > 0\) and \(p\) is primitive)}: Let \(K\) denote the fraction field of \(R\).
   We apply unique factorization in the euclidean domain \(K[x]\)
   \[p = q_1 \cdot \ldots \cdot q_n \qquad\text{with}~q_1, \ldots, q_n \in K[x]\]
   \Cref{lem:make_poly_prim} allows us to write \(q_i = c_i \cdot r_i\) with \(c_i \in K^\times\) and \(r_i \in R[x]: r_i~\text{is primitive}\), so
   \[p = c_1 \cdot \ldots \cdot c_n \cdot r_1 \cdot \ldots \cdot r_n\]
   By Gauss's lemma \(r_1 \cdot \ldots \cdot r_n\) is primitive and since we assumed \(p\) to be primitive we have \(c_1 \cdot \ldots \cdot c_n \in R^\times\).
   Hence we have shown that a factorization as in \cref{def:ufd} exists.
   We know it is unique up to multiplication with a unit, since we used \cref{lem:make_poly_prim} and because we know that the factorization of \(p \in R[x]\) in \(K[x]\) is unique.

   \textbf{Case 3 (\(\deg(p) > 0\) but \(p\) is not primitive)}: We apply \cref{lem:make_poly_prim}
   \[p = c \cdot q\]
   with \(c \in R\) and primitive \(q \in R[x]\).
   Applying the previous cases to \(c\) and \(q\) we obtain a factorization of \(p\) into irreducible elements.

   Consider an arbitrary factorization.
   By rearranging the factors such that all constants are in the front we have \(p = \tilde{c} \cdot \tilde{q}\) where \(\tilde{c} \in R\) and \(\tilde{q} \in R[x]\) is primitive.
   By the uniqueness assertion of \cref{lem:make_poly_prim}, \(\exists u \in R^\times: \tilde{q} = u\cdot g\) and hence such that \(\tilde{c} = u^{-1} \cdot c\).
   We deduce uniqueness from the two cases above.
\end{proof}
\begin{example}
   We follow the proof above, in the UFD \(\mathbb{Z}[x]\).

   Let \(p := 8x^4 + 8x^3 + 18x^2 + 16x + 4\).
   We see \(\deg(p) > 0\) and that \(p\) is not primitive so we rewrite
   \[p = 2(\underbrace{4x^4 + 4x^3 + 9x^2 + 8x + 2}_{:= \tilde{p}})\]
   where \(\tilde{p}\) is primitive, so we factor it in \(\mathbb{Q}[x]\).
   \[\tilde{p} = 4(x + \frac{1}{2})^2 (x^2 + 2)\]
   now we replace each factor by its unique primitive multiple (unique up to sign)
   \[\tilde{p} = (2x + 1)^2 (x^2 + 2)\]
   so we have the decomposition
   \[p = 2(2x + 1)^2(x^2 + 2)\]
   into irreducible factors in \(\mathbb{Z}[x]\).
\end{example}
\begin{remark}
   This theorem may be iterated.
   We see that \(\mathbb{Z}[x_1, \ldots, x_n]\) is a UFD for every \(n \in \mathbb{N}\).
   If \(K\) is a field, then \(K[x_1, \ldots, x_n]\) is a UFD for every \(n \in \mathbb{N}\).
\end{remark}

\subsection{Noetherian Rings}
In the previous section we have seen that ascending chain condition on principal ideals, with relation to the existence of factorizations into irreducible elements.
\begin{definition}[Noetherian Ring]
   Given a commutative ring \(R\) with no strictly increasing chain of ideals.
\end{definition}
\begin{remark}
   The condition is equivalent to the finite generation of every ideal in \(R\).
\end{remark}

\begin{proposition}
   Let \(R\) be a commutative ring.

   \(R\) is noetherian iff every ideal in R is finitely generated.
\end{proposition}
\begin{remark}
   It follows that every PID is noetherian.
\end{remark}

\begin{theorem}[Hilbert Basis Theorem]
   Given a neotherian ring \(R\), \(R[x]\) is also noetherian.
\end{theorem}
\begin{example}
   For any \(n \in \mathbb{N}\) the rings \(\mathbb{Z}[x_1, \ldots, x_n]\) and \(K[x_1, \ldots, x_n]\) for any field \(K\) are Noetherian.
\end{example}

If \(I \subset R\) is an ideal, then the ideals of \(R_{/I}\) are in order-presering bijective correspondence with the ideals of \(R\) containing \(I\).
As a consequence, if \(R\) is Noetherian, then so is \(R_{/I}\) for any ideal \(I\).
\begin{example}
   The ring \(\mathbb{Z}[x_1, \ldots, x_n]_{/I}\) is Noetherian for every \(n \in \mathbb{N}\) and ideals \(I \subset \mathbb{Z}[x_1, \ldots, x_n]\).
   In other words, every finietly generated commutative ring is Noetherian.
   So
   \[\mathbb{Z}[\sqrt{d}] \cong \mathbb{Z}[x]_{/(x^2 - d)}\]
   is Noetherian.
\end{example}

\begin{proposition}
   Let \(R\) be noetherian and \(M\) be a finitely generated \(R\)-module.

   Every \(N \subset M\) is finitely generated.
\end{proposition}
\begin{remark}
   If \(R\) is a PID, then any submodule \(M'\) of a finitely generated free module \(M\) is free, with \(\rk(M') \leq \rk(M)\).
   This fact, which was established for Euclidean domains, only made use of the PID property.
\end{remark}

\section{Fields}
Rings don't have multiplicative inverses, therefor we ''extend`` their definition with multiplicative inverses and closure to form fields.
We can say that a field is a commutative unitary ring \(R\) in which every non-zero element has a multiplicative inverse i.e.
\[\forall a \in R\setminus\{0\}: \exists a^{-1} := \frac{1}{a}\]
which is to say that the units (\ref{def:units_ring}) of \(R\) is all of \(R\) except for the additive identity
\[R^\times = R\setminus\{0\}\]
This is summarized in axiom (ii) as a multiplicative abelian group.

\begin{definition}[Field]\label{def:field}
   Given a set \(K\) with two operations
   \[+: K \times K \to K \quad\text{where}\quad (a, b) \mapsto a + b\]
   \[\cdot: K \times K \to K \quad\text{where}\quad (a, b) \mapsto a \cdot b\]
   if the following holds:
   \begin{enumerate}[label=\roman*, align=Center]
      \item \((K, +, 0)\) is an abelian group where \(a^{-1} := -a\).
      \item \((K^{*}, \cdot, 1)\) is an abelian group where \(K^{*} := K \setminus \{0\}\) and \(a^{-1} := \frac{1}{a}\).
      \item Distributivity: \(\forall a, b, c \in K: a \cdot (b + c) = ab + bc~\text{and}~(a + b) \cdot c = ac + bc\)
   \end{enumerate}
\end{definition}
\begin{remark}[Intuition]
   A \emph{field} is a set on which addition, subtraction, multiplication, and division are defined and behave as the corresponding operations on \(\mathbb{Q}\) and \(\mathbb{R}\).
\end{remark}
\begin{remark}
   As \(K\) is \emph{free of zero divisors} (required in (ii)) we have that \(K\) is an integral domain (\ref{def:integral_domain}).
\end{remark}

A field \(K\) contains the elements \(1, 1+1, 1+1+1, \ldots\) of the additive subgroup of \(K\), generated by \(1\).
For \(n \in \mathbb{N}\) let \(n \cdot 1 := \underbrace{1 + \ldots + 1}_{n~\text{times}}\).
Then two possibilities arise: eigher all elements \(n \cdot 1\) are distinct or \(n \cdot 1 = 0\).
\begin{definition}[Characteristic of a Field]
   Given a field \(K\),
   \[\chark(K) := p \in \mathbb{N}\]
   the smallest number such that \(p \cdot 1 = 0\).
\end{definition}
\begin{remark}
   If such \(p\) doesn't exist we define \(\chark(F) := 0\).
\end{remark}

% TODO: create section "ordered algebraic structures"
\begin{definition}[Ordered Field]
   A field \(K\) is ordered if \((K, \leq)\) is totally ordered and
   \[\forall x, y, z \in K: x < y \implies x + z < y + z\]
   \[\forall x, y \in K: x, y > 0 \implies x \cdot y > 0\]
\end{definition}

\begin{definition}[Subfield]
   Given a field \(K\), the subset \(L \subset K\) where \(L\) is a field in respect to the field operations of \(K\).
\end{definition}

Given two fields \(K, L\) it follows from the field definition, that ring homomorphisms \(f: K \to L\) are injective (see remark of \cref{def:ring_homo}).
% TODO: what?
\[\forall \alpha \in K^\times: \big(f(\alpha) \cdot f(\alpha^{-1}) = f(\alpha \cdot \alpha^{-1}) = f(1) = 1 \implies f(\alpha) \neq 0\big)\]

\subsection{Important Examples}
% TODO this is Field of Fractions of Z
\subsubsection{Field of Fractions}
\begin{proposition}[\((\mathbb{Q}, +, \cdot)\) is a Field]\label{pro:Q_field}
   With the operations
   \[+: \mathbb{Q} \times \mathbb{Q} \to \mathbb{Q}\]
   \begin{equation}\label{eq:Q_add}
      \big([(a, b)], [(c, d)]\big) \mapsto \frac{a}{b} + \frac{c}{d} := \frac{a \cdot d + c \cdot b}{b \cdot d}
   \end{equation}
   and
   \[\cdot: \mathbb{Q} \times \mathbb{Q} \to \mathbb{Q}\]
   \begin{equation}\label{eq:Q_mult}
      \big([(a, b)], [(c, d)]\big) \mapsto \frac{a}{b} \cdot \frac{c}{d} := \frac{a \cdot c}{b \cdot d}
   \end{equation}
   is \(\mathbb{Q}\) a field.
\end{proposition}
% TODO
\begin{proof}
   Let \(a_1, b_1, a_2, b_2 \in \mathbb{Z}\) with \(b_1, b_2 \neq 0\) and \((a_1, b_1) \sim (a_2, b_2)\)
   \[a_1 \cdot b_2 = a_2 \cdot b_1 \implies \frac{a_1}{b_1} = \frac{a_2}{b_2} \implies \frac{a_1}{b_1} + \frac{a'}{b'} = \frac{a_2}{b_2} + \frac{a'}{b'} \implies \frac{a_1 \cdot b' + a' \cdot b_1}{b_1 \cdot b'} = \frac{a_2 \cdot b' + a' \cdot b_2}{b_2 \cdot b'} \implies\]
   \(\implies +~\text{is well-defined}\), the process would be similar to prove the same for \(\cdot\).
   From the operation definitions \cref{eq:Q_add}, \cref{eq:Q_mult} follows that \(\cdot, +\) are associative, commutative and also distributive.
   They also produce the additive neutral element
   \[0_{\mathbb{Q}} := \frac{0}{b} \forall b \in \mathbb{Z} \setminus \{0\}\]

   and the multiplicative neutral and inverse
   \[1_{\mathbb{Q}} := \frac{1}{1}\]
   \[\forall a, b \in \mathbb{Z} \setminus \{0\}: \frac{a}{b} \cdot \frac{1}{1} = \frac{a}{b} \implies \frac{a}{b} \cdot \frac{b}{a} = \frac{1}{1}\]
   from which follows that
   \[\mathbb{Q}^{\times} = \mathbb{Q} \setminus \{0\} \implies \frac{a}{b} \in \mathbb{Q}: \frac{a}{b} \in \mathbb{Q}^{\times} \iff \frac{a}{b} \neq 0 \iff a \neq 0\]
\end{proof}

\subsection{Field Extensions}
An \emph{embedding} is one instance of some mathematical structure contained within another instance, such as a group that is a subgroup.
When some object \(X\) is said to be embedded in another object \(Y\), the embedding is given by some injective homomorphism \(f: X \hookrightarrow Y\).

\begin{definition}[Field Embedding]
   Given the fields \(K, L\), a ring homomorphism \(f: K \hookrightarrow L\).
\end{definition}
\begin{remark}[Terminology]
   We say \(f\) is an \emph{embedding} of \(K\) in \(L\) or \(K\) is \emph{embedded} in \(L\).
   %TODO: does this actually mean that every field is embedded in every other field somehow?
   As we know from the remark of \ref{def:ring_homo} every ring homomorphism is injective.
\end{remark}

When we speak of a \emph{field extension} we mean that an embedding of fields has been fixed.
As \(K\) is a field we know that \(\im(f)\) is a subfield of \(L\) as the embedding is an injective homomorphism.
This way we can identify \(K\) with a subfield of \(L\).

\begin{definition}[Field Extension]
   The superfield \(L\) of a subfield \(K \subset L\), denoted \(L/K\).
\end{definition}
\begin{remark}[Terminology]
   \(L\) is an \emph{extension field} or simply \emph{extension} of \(K\) and this pair of fields is a \emph{field extension}.
\end{remark}

\(L\) acquires a structure of \(K\)-vector space, i.e. elements of \(L\) can be viewed as vectors and elements in \(K\) as scalars.
So it makes sense to ask if \(L\) is finite-dimensional as \(K\)-vector space.

\begin{definition}[Degree of Field Extension]
   Given a field extension \(L/K\), its \emph{degree} is
   \[[L:K] := \dim_K(L)\]
\end{definition}
\begin{remark}[Terminology]
   The extension is said to be \emph{finite} if \([L:K]\) is finite.
\end{remark}

% TODO !!!!!!!!!!!!!!!!!!!!!!!!!
% https://en.wikipedia.org/wiki/Field_extension#Examples

\subsubsection{Algebraic and Transcendental Field Extensions}
We will add the field \(\mathbb{Q} \subset \overline{\mathbb{Q}} \subset \mathbb{C}\), neither contained in nor containing \(\mathbb{R}\).
This is the field consisting of all roots of polynomials with rational coefficients.
This means that \(\overline{\mathbb{Q}}\) is the field of \emph{algebraic numbers} and \(\mathbb{C} \setminus \overline{\mathbb{Q}}\) the field of \emph{transcendental numbers}.

\begin{definition}[Algebraic/Transcendental Field Element]
   Given a field extension \(L/K\), \(l \in L\) is \emph{algebraic} over \(K\) iff
   \[\exists p(x) \in K[x]: p(x) \neq 0 \land p(k) = 0\]
   otherwise we say \(k\) is \emph{transcendental} over \(K\).
\end{definition}
\begin{remark}
   Given an algebraic element \(l \in L\), then is the set of polynomials \(p\) above a nonzero ideal of \(K[x]\), generated by a monic \(q \in K[x]\).
   \(q\) is necessarily irreducible and the \emph{minimal polynomial} of \(l\).
   We call \(d := \deg(q)\) the \emph{degree} of \(l\).
\end{remark}

\begin{definition}[Algebraic/Transcendental Field Extension]
   An extension \(L/K\) is \emph{algebraic} iff every element of \(L\) is algebraic over \(K\), otherwise we call \(L/K\) \emph{transcendental}.
\end{definition}
\begin{remark}
   Every finite extension is algebraic.
   Indeed if \([L:K] = d\) then for \(l \in L\) there must be a linear dependence amonge \(1, x, \ldots, x^d\), which shows that \(l\) is algebraic over \(K\) of degree at most \(d\).
\end{remark}

\begin{definition}[Algebraically Closed Field]
   A field \(K\) where every non-constant polynomial in \(K[x]\) has a root in \(K\).
\end{definition}
\begin{remark}
   Equivalently, a field \(K\) is algebraically closed if every irreducible polynomial in \(K[x]\) is linear or alternatively if the only algebraic elements of \(L\) are the elements of \(K\) itself.
\end{remark}

\begin{definition}[Algebraic Closure of a Field]
   The \emph{algebraic closure} of a field \(K\) is an algebraic extension \(L/K\) which is algebraically closed.
\end{definition}
\begin{remark}[Intuition]
   The algebraic closure of \(K\) is the set of algebraic elements of \(L\).
\end{remark}

We may also speak of an algebraic closure of \(K\) without mentioning any extension field \(L\).
In that case we mean an algebraically closed, algebraic extension \(\overline{K}\).
One of the tasks of a more comprehensive treatment of fields is to prove that every field admits an algebraic closure, and any pair of such are isomorphic to each other.
Given \(L/K\) where \(L\) is algebraically closed, then \(\overline{K}\) is an algebraic closure of \(K\).

% TODO: really proposition?
\begin{proposition}
   Let \(\overline{K}\) be an algebraic closure of \(K\).
   For every finite field extension \(K'/K\) there exists an embedding \(K' \hookrightarrow K\).
\end{proposition}

\begin{proposition}
   Let \(L/K\) be a field extension.
   The algebraic closure of \(K\) is a subfield of \(L\).
\end{proposition}

\begin{proposition}
   Let \(L/K\) and \(M/L\) be finite field extensions.
   Then \(M/K\) is also a finite field extension and
   \[[M:K] = [L:K] \cdot [M:L]\]
\end{proposition}

\subsection{Residue Fields}
% TODO: Include Field of Fractions --> field of fractions = quotient field
\begin{definition}[Quotient Field]
   Given an integral domain (\ref{def:integral_domain}) \(R\), the quotient field of \(R\) is the set \(K := R \times (R \setminus \{0\})/\sim\) where \(\sim\) is defined as in \cref{def:rat_num}
   with the operations as in \cref{pro:Q_field}
   \[[(a_1, b_1)] + [(a_2, b_2)] := [(a_1b_2 + a_2b_1, b_1b_2)]\]
   \[[(a_1, b_1)] \cdot [(a_2, b_2)] := [(a_1a_2, b_1b_2)]\]
\end{definition}
\begin{remark}
   \(K\) is also called \emph{field of fractions} and satisfies the universal property of quotient fields.
\end{remark}

\begin{definition}[Residue Field]
   Given a eucliean domain \(R\) and a prime element \(p\), \(R_{/pR}\) is a residue field.
\end{definition}
\begin{remark}
   There exists and epimorphism \(R \twoheadrightarrow R_{/pR}\).
\end{remark}

\begin{definition}[Prime Field]
   Given the euclidean domain \(R = \mathbb{Z}\) and any prime element \(p \in \mathbb{Z}\)
   \[\mathbb{F}_p := \mathbb{Z}_{/p\mathbb{Z}}\]
   is a field.
\end{definition}

% TODO
For an \(n \in \mathbb{N}\) are \(a, b \in \mathbb{Z}\) called \textit{congruent modulo n}, if
\[a \equiv b \mod n\]
which means that \(n\) divides \(a - b\).
Or differently put, there exists \(k \in \mathbb{Z}\) such that
\[\frac{a - b}{n} = k \implies a - b = k \cdot n\]
It can be shown that this relation forms an equivalence relation
\[a \sim b :\iff a - b \in nR\]
where \(nR\) denotes the set of all multiples of \(n\).
The equivalence classes of the equvalence relation are called congruence classes modulo n or \textit{residue classes modulo n}.
\[\bar{a} = \{\ldots, (a-2n), (a-n), a, (a+n), (a+2n), \ldots\}\]
\[\bar{a} = \bar{b} \implies n | (a - b)\]

Integer division is only is possible in an euclidean domain.
So given a euclidean domain \(R\) we can construct such residue classes as follows.
We have the set of all multiples of \(n\) in \(R\).
\[nR := \{nb \mid b \in R\}\]
which we then add to \(a \in R\)
\[\bar{a} = a + nR\]

The set of all such residue classes is a quotient set \(R_{/\sim}\) denoted as \(R_{/nR}\) since \(\sim\) depends on \(n\).
From \cref{pro:irreducible} follows that \(R_{/nR}\) is a field \(\iff n \in R\) is prime, hence we can add, subtract, multiply and divide with remainder.

\begin{example}
   Lets look at the residue field \(\mathbb{Z}_{/2\mathbb{Z}}\).
   \[2\mathbb{Z} = \{\ldots, -6, -4, -2, 0, 2, 4, 6, \ldots\}\]
   So we look at what residue classes there are.
   \[\bar{0} = 0 + 2\mathbb{Z} = 2\mathbb{Z} = \{\ldots, -6, -4 -2, 0, 2, 4, 6, \ldots\}\]
   \begin{equation*}
      \begin{split}
   \bar{1} & = \{\ldots, (1+(-6)), (1+(-4)), (1+(-2)), (1+0), (1+2), (1+4), (1+6), \ldots\} \\
                & = \{\ldots, -5, -3 -1, 1, 3, 5, \ldots\}
      \end{split}
   \end{equation*}
   \begin{equation*}
      \begin{split}
         \bar{2} &= \{\ldots, (2+(-6)), (2+(-4)), (2+(-2)), (2+0), (2+2), (2+4), (2+6), \ldots\} \\
                      &= \{\ldots, -4, -2, 0, 2, 4, 6, 8, \ldots\} = \bar{0}
      \end{split}
   \end{equation*}
   \begin{equation*}
      \begin{split}
         \bar{3} &= \{\ldots, (3+(-6)), (3+(-4)), (3+(-2)), (3+0), (3+2), (3+4), (3+6), \ldots\} \\
                      &= \{\ldots, -3, -1, 1, 3, 5, 7, 9, \ldots\} = \bar{1}
      \end{split}
   \end{equation*}
   So the quotient set contains only two residue classes
   \[\mathbb{Z}_{/2\mathbb{Z}} = \{\bar{0}, \bar{1}\}\]
   which can be equipped with two operations to form a field.

   \begin{center}
      \renewcommand\arraystretch{1.3}
      \begin{tabular}{c|c c}
         +   & 0   & 1 \\ \hline
         0   & 0   & 1 \\
         1   & 1   & 0 \\
      \end{tabular}
      \quad
      \begin{tabular}{c|c c}
         \(\cdot\) & 0   & 1 \\ \hline
         0         & 0   & 0 \\
         1         & 0   & 1 \\
      \end{tabular}
   \end{center}
   Note that we cannot produce another element as \(0\) or \(1\) since we are restricted by the modulo operation.
   \[\bar{1} + \bar{1} = \bar{2} = \bar{0}\]
\end{example}

In a next step we can use \(\mathbb{F}_2\) to define a polynomial ring \(\mathbb{F}_2 [x]\) of polynomials with coefficients 0 or 1.
Its prime elements are \(T\), \(T+1\) and \(T^2 + T + 1\).

When we now take \(R = (\mathbb{Z}_{/2\mathbb{Z}})[x]\) and choose a prime element \(n = x^2 + x + 1\) we can define a new residue field of polynomials.
When we use the euclidean algorithm to divide with remainder we use the euclidean value \(\deg\) to determine if the remainder is small enough, that is, if we have found a proper quotient such that \(\deg(r) < \deg(b)\).
Since we chose \(x^2 + x + 1\) as prime element we know that all remainders must have a degree less than 2 and because of how \(R\) is define we also know that the remainders coefficients can only be 1 or 0.
\[R_{/nR} = \{\bar{0}, \bar{1}, \bar{x}, \bar{x+1}\}\]

\begin{center}
   \begin{tabular}{c|c c c c}
      \(+\)   & \(0\)   & \(1\)   & \(x\)   & \(x+1\) \\ \hline
      \(0\)   & \(0\)   & \(1\)   & \(x\)   & \(x+1\) \\
      \(1\)   & \(1\)   & \(0\)   & \(x+1\) & \(x\)   \\
      \(x\)   & \(x\)   & \(x+1\) & \(0\)   & \(1\)   \\
      \(x+1\) & \(x+1\) & \(x\)   & \(1\)   & \(0\)   \\
   \end{tabular}
   \quad
   \begin{tabular}{c|c c c c}
      \(\cdot\) & \(0\) & \(1\)   & \(x\)   & \(x+1\) \\ \hline
      \(0\)     & \(0\) & \(0\)   & \(0\)   & \(0\)   \\
      \(1\)     & \(0\) & \(1\)   & \(x\)   & \(x+1\) \\
      \(x\)     & \(0\) & \(x\)   & \(x+1\) & \(1\)   \\
      \(x+1\)   & \(0\) & \(x+1\) & \(1\)   & \(x\)   \\
   \end{tabular}
\end{center}
Keeping in mind that there are only coefficients with 0 or 1 we can see easily that for example
\[x + x = 2x = 0 \quad\text{and}\quad  (x+1) + 1 = x + 2 = x \quad\text{and}\quad x + (x+1) = 2x + 1 = 1\]
With the multiplication we write down a polynomial division with remainder of the prime element.
\[x \cdot x = x^2 = 1 \cdot (x^2 + x + 1) + (x+1)\]
so we see that
\[\bar{x} \cdot \bar{x} = \bar{x^2} = \bar{x+1}\]
which is essentially the same as above where \(\bar{2} = \bar{0}\).

\begin{proposition}[\(\mathbb{Z}\) Homomorphism]
   Let \(R\) be a unitary ring.
   \[\exists! f: \mathbb{Z} \to R\]
   a homomorphism of unitary rings (\ref{def:ring_homo}).
\end{proposition}

\begin{proposition}
   Let \(K\) be a field and \(f: \mathbb{Z} \to K\) the homomorphism of unitary rings.

   If \(f\) is not injective, the smallest positive whole number \(p \in \ker\) is prime.
   Further induces \(f\) a field embedding \(\mathbb{F}_p \to K\).
   \[\ker(f) = p\mathbb{Z}\]

   Is \(f\) injective it follows that \(p = 0\), which means that \(\ker(f)\) is trivial.
   Also there exists a unique extension to a field embedding \(\mathbb{Q} \to K\).
\end{proposition}


\section{K-Vector Spaces}
\begin{definition}[K-Vector Space]\label{def:vector_spaces}
   A field \(K\), a set \(V\) and two operations
   \[\dotplus V \times V \to V \quad\text{where}\quad (v, w) \mapsto v \dotplus w\]
   \[\cdot: K \times V \to V \quad\text{where}\quad (\lambda, v) \mapsto \lambda \cdot v\]

   if the following holds:
   \begin{enumerate}[label=\roman*, align=Center]
      \item \((V, \dotplus, 0_V)\) is an abelian group
      \item Neutral Scalar: \(\forall v \in V: 1_K \cdot v = v\)
      \item Associativity: \(\forall \lambda, \mu \in K~v \in V: \lambda (\mu \cdot v) = (\lambda \mu) \cdot v\)
      \item Distributivity: \(\forall \lambda, \mu \in K~v, w \in V: (\lambda + \mu) v = \lambda v \dotplus \mu v \quad\text{and}\quad \lambda (v \dotplus w) = \lambda v \dotplus \lambda w\)
   \end{enumerate}
\end{definition}

\begin{definition}[Quotient Vector Space]
   Given a K-vector space \(V\), a subspace \(U \subset V\) and \(\sim\) on \(V\) defined as
   \[v_1 \sim v_2 :\iff v_1 - v_2 \in U\]
   the set \(V_{/U} = \{[v] \mid v \in V\}\) forms the quotient vector space where
   \[[v_1] + [v_2] := [v_1 + v_2] \quad\text{and}\quad \lambda [v] := [\lambda v]\]
\end{definition}
\begin{example}
   Let \(V\) be the K-vector space over \(\mathbb{R}\) which is spanned by \(v_1 := \begin{pmatrix}0\\1\\0\end{pmatrix}\) and \(v_2 := \begin{pmatrix}1\\1\\0\end{pmatrix}\).

   \(V\) corresponds to the xy-plane in \(\mathbb{R}^3\) and we want now to find a subspace.
   We can do this by choosing any vector of the plane, e.g. \(w := \begin{pmatrix}1\\2\\0\end{pmatrix}\) so we get
   \[U := \spanv\{w\} = \{\alpha \cdot w \in \mathbb{R}^3 \mid \alpha \in \mathbb{R}\}\]
\end{example}

\subsection{Important Examples}
\begin{example}
   \(K^n = \{(a_1, \ldots, a_n) \mid a_1, \ldots, a_n \in K\) is a K-vector space where
   \[(a_1, \ldots, a_n) + (b_1, \ldots, b_n) = (a_1 + b_1, \ldots, a_n + b_n)\]
   \[\lambda \cdot (a_1, \ldots, a_n) = (\lambda \cdot a_1, \ldots, \lambda \cdot a_n)\]
\end{example}

\begin{example}
   \(V := (\Mat_{m,n}(K), +, \cdot, 0_{m,n}, 1_K)\) is a K-vector space.
   Vector addition is just matrix addition and scalar multiplication is defined in the obvious way (by multiplying each entry by the same scalar).
   \[\lambda \cdot_V A := \lambda (a_{ij}) = (\lambda \cdot_K a_{ij})\]
   \[A + B := (a_{ij}) +_V (b_{Ã­j}) = (a_{ij} +_K b_{ij})\]
   \[A - B := (a_{ij}) +_V (-1)(b_{ij}) = (a_{ij} -_K b_{ij})\]

   \[\dim(V) = m \cdot n\]
   When m = n the matrix is square and matrix multiplication of two such matrices produces a third.
   This vector space of dimension \(n^2\) forms an \textit{algebra over a field}.
   One possible choice of basis are the permutation matrices (\ref{def:perm_mat}).
\end{example}

\begin{example}
   % todo: elaborate
   \(K[x]\) is an infinite dimensional K-vector space.
\end{example}

\subsection{Subspaces}
\begin{definition}[Vector Subspace]\label{def:vector_subspace}
   Subset \(U \subset V\) of a K-vector space, if the following holds:

   \begin{enumerate}[label=\roman*, align=Center]
      \item \(U \neq \emptyset\)
      \item Closed Addition: \(\forall v, u \in U: v \dotplus u \in U\)
      \item Closed Multiplication: \(\forall u \in U,~\lambda \in K: \lambda \cdot u \in U\)
   \end{enumerate}
\end{definition}

\begin{theorem}[Subspace = Vector Space]\label{thm:subspace=vecspace}
   A vector subspace \(U \subset V\) is with the induced addition and scalar multiplication also a vector space.
\end{theorem}

\begin{definition}[Sum of Subspaces]\label{def:subspace_sum}
   Given vector subspaces (\ref{def:vector_subspace}) \(U_1, U_2\) of a K-vectorspace \(V\)
   \[U_1 + U_2 := \{u_1 + u_2 \mid u_1 \in U_1, u_2 \in U_2\}\]
\end{definition}
\begin{remark}
   The sum of two subspaces is again a subspace.
\end{remark}

\begin{lemma}\label{lem:vectorspace_complement}
   Let \(U\) be subspaces of a K-vector space \(V\).
   \(\exists W \subset V\) a subspace such that
   \begin{enumerate}[label=\roman*, align=Center]
      \item \(U \cap W = \emptyset\)
      \item \(U + W = V\)
   \end{enumerate}
\end{lemma}

\begin{definition}[Direct Sum of Subspaces]\label{def:direct_sum}
   Given a K-vector space \(V\) and two subspaces \(U, W \subset V\).
   \[V = U \oplus W \quad\text{if}\quad V = U + W~\text{and}~U \cap W = \emptyset\]
\end{definition}

\begin{definition}[Subspace Complement]\label{def:complement}
   For the direct sum \(V = U \oplus W\) is
   \[U~\text{the complement of}~W~\text{and}~W~\text{of}~U~\text{in}~V\]
\end{definition}

\subsection{Bases of Vector Spaces}
\subsubsection{Linear Independence}
\begin{definition}[Linear Combination]\label{def:lin_comb}
   A vector \(v \in V\) is a linear combination of \((v_i)_{i \in I} \subset V\) if
   \[\forall i \in I~\exists \lambda_i \in K: v = \sum_{i \in I} \lambda_i v_i~\text{where}~\lambda_i = 0~\text{for almost all}~i \in I\]
\end{definition}
\begin{remark}
   \textit{almost all} means that infinite \(\lambda_i\) should be 0 while finite many \(\lambda_i \neq 0\).
\end{remark}

\begin{definition}[Linear Independent]\label{def:lin_depend}
   An indexed family \((v_i)_{i \in I} \subset V\) of a K-vector space \(V\) is linear independent if \(\forall (\lambda_i)_{i \in I} \subset K: \lambda_i = 0~\text{for almost all}~i \in I\) holds
   \[\sum_{i \in I} \lambda_i v_i = 0_V \implies \forall i \in I: \lambda_i = 0\]
\end{definition}
\begin{remark}
   In words, a set of vectors is linear independent the zero vector can only be combined trivially.

   If \((\lambda_i)_{i \in I}\) is linear independent then is the subfamily \((\lambda_j)_{j \in J}\) with \(J \subset I\) also linear independent.
\end{remark}
\begin{example}
   The vectors of the standard basis \(e_j := (\delta_{ij})_{i \in I}\) are linear independent.
\end{example}

\begin{definition}[Span]
   The set of all linear combinations of \((v_i)_{i \in I} \subset V\).
   \[\langle (v_i)_{i \in I} \rangle = \spanv_{K}(v_i)_{i \in I} := \left\{\sum_{i \in I} \lambda_i v_i \right\}\]
\end{definition}
\begin{remark}
   We say \(\spanv_{K}(v_i)_{i \in I}\) is the of \((v_i)_{i \in I}\) \textit{spanned} subspace of \(V\).
\end{remark}

\subsubsection{Basis}
\begin{definition}[Generator]\label{def:generator}
   A family \((v_i)_{i \in I}\) of vectors of a K-vector space \(V\) if
   \[\spanv_{K}(v_i)_{i \in I} = V\]
\end{definition}
\begin{remark}
   Differently put \(\forall v \in V: v = \sum_{i \in I} \lambda_i v_i\).
\end{remark}

\begin{definition}[Basis]\label{def:basis}
   A linear independent generator \(\spanv(v_i)_{i \in I}\) of a K-vector space \(V\).
\end{definition}
\begin{remark}
   If the index set \(I\) of the family is infinite we can't find a basis.
\end{remark}
\begin{example}
   For an arbitrary (finite) index set \(I\) is \((\lambda_i)_{i \in I} \subset K\) a K-vector space with basis \((e_i)_{i \in I}\).
   \[\sum_{i \in I} \lambda_i e_i = (\lambda_i)_{i \in I}\]
\end{example}
\begin{remark}[Direct Sum]
   The K-vector space in the example above can also be written as the direct sum
   \[\bigoplus_{i \in I} K := \{(\lambda_i)_{i \in I} \mid \text{for almost all}~i \in I: \lambda_i = 0\}\]

   Given a more general direct sum \(\oplus_{i \in I} V_i\) of K-vector spaces \(V_i = (v_j)_{j \in I}\), we can define for every \(i\) a basis \((v_j)_{j \in J_i}\) which together form a basis of \(\oplus V_i\).
   The index sets \(J_i\) for every basis is defined through
   \[\coprod_{i \in I} J_i := \bigcup_{i \in I} \{(j, i): j \in J_i\}\]
\end{remark}

\begin{proposition}[Linear Combination is Unique]
   Let \(V\) be a K-vector space.
   \[(b_i)_{i \in I} \subset V~\text{is a basis of}~V \iff \forall v \in V~\exists! (\lambda_i)_{i \in I} \subset K: v = \sum_{i \in I} \lambda_i b_i\]
\end{proposition}
\begin{remark}
   This proposition states that a basis of a K-vector space \(V\) represents all vectors of \(V\) as a linear combination of the basis vectors.
   Differently put corresponds \((b_i)_{i \in I}\) to the linear map
   \[f: \bigoplus_{i \in I} K \to V \quad\text{where}\quad e_i \mapsto v_i, \forall i \in I\]
   this allows us to consider that \((b_i)\) is a basis iff \(\oplus K\) and \(V\) are isomorphic.
   \begin{enumerate}
      \item \((b_i)~\text{basis} \iff \oplus K \cong V\)
      \item \((b_i)~\text{linear independent} \iff f~\text{injective}\)
      \item \((b_i)~\text{generator} \iff f~\text{surjective}\)
   \end{enumerate}
\end{remark}

\begin{lemma}\label{lem:extending_family}
   Let \((v_i)_{i \in I}\) be a family of \(V\) and \(j \in I\).
   \begin{enumerate}[label=\roman*, align=Center]
      \item Suppose \((v_i)_{i \in I \setminus \{j\}}\) is linear independent. Then is \((v_i)_{i \in I}~\text{linear independent} \iff v_j \not\in \spanv(v_i)_{i \in I \setminus \{j\}}\)
      \item Suppose \((v_i)_{i \in I}\) is a generator. Then is \((v_i)_{i \in I \setminus \{j\}}~\text{is generator} \iff v_j \in \spanv(v_i)_{i \in I \setminus \{j\}}\)
   \end{enumerate}
\end{lemma}

\begin{lemma}[Exchange Lemma]\label{lem:exchange}
   Let \((b_i)_{i \in I}\) be a basis of a K-vector space \(V\) and \(v \neq 0 \in V\) with a unique linear combination.
   \[v = \sum_{i \in I} \lambda_i b_i \neq 0 \implies \exists j \in I: \lambda_j \neq 0\]
   We define
   \[\tilde{v_i} := \begin{cases} v & i = j\\ b_i & i \neq j\end{cases}\]
   then is \((\tilde{v_i})_{i \in I}\) a basis, where we exchanged a \(b_i\) with \(v\).
\end{lemma}

\subsubsection{Vector Space Dimension}
\begin{definition}[Dimension of Vector Space]
   Given a K-vector space \(V\)
   \[\dim(V) = \dim_{K}(V) := \begin{cases}\infty & \text{if}~V~\text{has no finite basis.}\\n & \text{if}~V~\text{has a basis}~B~\text{with}~|B| = n\end{cases}\]
\end{definition}

\begin{definition}[Finite-Dimensional Vector Space]
   A K-vector space \(V\) is finite dimensional if
   \[\text{for some}~n \in \mathbb{N}~\exists f: K^n \xrightarrow{\sim} V \quad\text{and}\quad \dim(V) := n\]
\end{definition}

\begin{proposition}[Finite Space \(\cong K^n\)]\label{pro:Kn_VS_isomorphism}
   Let \(V\) be a finite dimensional K-vector space.
   \[\exists! n \in \mathbb{N}~\text{such that}~\exists f: K^n \xrightarrow{\sim} V\]
\end{proposition}
\begin{remark}
   Differently put if \(b_1, \ldots b_n\) and \(b_1', \ldots, b_m'\) are bases of \(V\) is \(m = n\).
\end{remark}

\begin{proposition}[Infinite Space Properties]
   For a K-vector space \(V\) are the following statements equivalent
   \begin{enumerate}[label=\roman*, align=Center]
      \item \(V\) is infinite dimensional.
      \item \(\exists (v_i)_{i \in \mathbb{N}} \subset V\) which is linear independent.
      \item \(\exists V \hookrightarrow V\) which is not surjective.
   \end{enumerate}
\end{proposition}

\begin{proposition}[Space Homomorphism Properties]\label{pro:dim_f}
   Let \(V\) and \(W\) be K-vector spaces with \(\dim(V) = n\), \(\dim(W) = m\) and let \(f: V \to W\)

   \begin{enumerate}[label=\roman*, align=Center]
      \item \(m = n \implies f~\text{is bijective.}\)
      \item \(m < n \implies f~\text{is not injective.}\)
      \item \(m > n \implies f~\text{is not surjective.}\)
   \end{enumerate}
\end{proposition}
\begin{remark}
   This proposition can also be restated as:
   Let \((v_i)_{i \in I} \subset V\) be a family of vectors and \(n = \dim(V)\)
   \begin{enumerate}[label=\roman*, align=Center]
      \item \(m = n \implies (v_i)~\text{is a basis.}\)
      \item \(m < n \implies (v_i)~\text{is not linear independent.}\)
      \item \(m > n \implies (v_i)~\text{is not a generator.}\)
   \end{enumerate}
\end{remark}

\begin{proposition}[Construct Bases]\label{pro:extending_family}
   Let \(V\) be a K-vector space.
   \begin{enumerate}[label=\roman*, align=Center]
      \item Is \(v_1, \ldots, v_n \in V\) a finite generator then is \(V\) finite dimensional.
         Furthermore we can construct a basis from any linear independent family in \(V\) by extending it iteratively with a vector from \(\{v_1, \ldots, v_n\}\).
      \item Is \(V\) a subspace of a finite dimensional vector space, then is \(V\) finite dimensional.
         Furthermore we can construct a basis from any linear independent family in \(V\) by extending it iteratively with a vector from \(V\).
   \end{enumerate}
\end{proposition}

\begin{proposition}[Dimension Calculation Rules]\label{pro:dimesion_formulas}
   Let \(V\) be a finite-dimensional K-vector space.
   \begin{enumerate}[label=\roman*, align=Center]
      \item A subspace \(U \subset V\) and the quotient space \(V/U\) are finite-dimesional and
         \[\dim(V) = \dim(U) + \dim(V/U)\]
      \item For \(f: V \to W\) of another K-vector pace \(W\), is \(\im(f)\) finite-dimensional and
         \[\dim(V) = \dim(\ker(f)) + \dim(\im(f))\]
   \end{enumerate}
\end{proposition}
\begin{remark}
   See \cref{cor:dim_ker}.
\end{remark}

\begin{corollary}[Subspace Sum Dimension]
   Given a K-vector space \(V\) and two finite-dimensional subspaces \(U, W \subset V\) then is
   \[\dim(U) + \dim(W) = \dim(U+W) + \dim(U \cap W)\]
\end{corollary}

\begin{proposition}[Extend to Basis]
   Let \(V\) be a K-vector space and \((v_i)_{i \in I}\) a generator.

   Any linear independent family \((w_j)_{j \in J}\) can be extended to a basis with \((v_i)_{i \in I'}\) where \(I' \subset I\).
\end{proposition}
\begin{remark}
   Especially has every vector space a basis \((v_i)_{i \in I} = (v)_{v \in V}\) when \(J = \emptyset\).
\end{remark}

\subsubsection{Change of Bases}
To define the change of bases it is important to note what follows from \cref{pro:Kn_VS_isomorphism}.
\begin{remark}
   Given a K-vector space \(V\) with the basis \(B = (v_1, \ldots, v_n)\), there exists exactly one isomorphism
   \[\varphi_{B}: K^n \to V \quad\text{which}\quad e_i \mapsto v_i\]
   where \((e_1, \ldots, e_n)\) is the canonical basis of \(K^n\).
\end{remark}

From the remark of \cref{def:left_mat_mult} follows that to transform a basis \(B\) into another basis \(B'\) there exists a linear map \(f: B \to B'\) which can be represented through the left multiplication with a matrix.
Such a matrix is called a \emph{transformation matrix for the change of bases}.

\begin{definition}[Transformation Matrix for Change of Bases]\label{def:bases_transmat}
   Given a K-vector space \(V\) with two bases \(B = (b_1, \ldots, b_n)\) and \(B' = (b'_1, \ldots, b'_n)\)
   \[\exists! T \in \GL_n(K)\]

   \begin{center}
      \begin{tikzcd}
         B \arrow{dd}[swap]{L_T} \arrow{dr}{\varphi_B} &   \\
                                                                   & V \\
         B' \arrow{ur}[swap]{\varphi_{B'}}                 &
      \end{tikzcd}
   \end{center}
   \[v \in V = \sum_{i=1}^n \lambda_i b_i = \sum_{i=1}^n \mu_i b'_i\]
\end{definition}

This means we can transform a basis \(B\) through left multiplication with a matrix into a basis \(B'\).

\begin{example}
   Let \(V = \mathbb{R}^2\) be a K-vector space, \(B = \{e_1, e_2\}\) the standardbasis and \(B' = \left\{\begin{pmatrix}2 \\1\end{pmatrix}, \begin{pmatrix}-1\\1\end{pmatrix}\right\}\), which is a basis written in the coordinate system of \(B\).

   Now suppose we have the vector \(v' = \begin{pmatrix}-1\\2\end{pmatrix}\) given in \(B'\) and we want to translate it into \(B\).
   \[v' = \lambda_1 \cdot \begin{pmatrix}1\\0\end{pmatrix} + \lambda_2 \begin{pmatrix}0\\1\end{pmatrix} \implies \lambda_1 = -1,~\lambda_2 = 2 \implies v = \lambda_1 \cdot \begin{pmatrix}2\\1\end{pmatrix} + \lambda_2 \cdot \begin{pmatrix}-1\\1\end{pmatrix} = \begin{pmatrix}-4\\1\end{pmatrix}\]
   This can also be written as a matrix product
   \[\begin{pmatrix}2 & -1\\1 & 1\end{pmatrix} \cdot v' = v\]
   Geometrically speaking this matrix describes a linear transformation, translating the coordinate system of the standard basis, into the the coordinate system of \(B_V'\).
   But numerically it is translating a vector described in \(B_V'\) into a vector written in \(B_V\).

   This means we have our matrix \(T\) where we see that \(e_1\) of \(B_V'\) is \(\begin{pmatrix}2\\1\end{pmatrix}\) in terms of \(B_V\).
\end{example}

Now consider the case where we have two bases \(B_V, B_V'\) and \(B_W, B_W'\) for each of two vector space.
Between each pair of bases of \(V\) and \(W\) we have the matrices \(A\) and \(A'\) corresponding to \(f\).
Between the bases of \(V\) exists the transformation matrix \(T \in \GL_n(K)\) for the change of bases and between the bases of \(W\) we have \(S \in \GL_m(K)\).

\begin{center}
   \begin{tikzcd}
      B_V \arrow{dd}[swap]{L_T} \arrow{dr}{\varphi_{B_V}} \arrow{rrr}{L_{A}} & & & B_W \arrow{dl}[swap]{\varphi_{B_W}} \arrow{dd}{L_S} \\
               & V \arrow{r}{f} & W \\
      B_V' \arrow{ur}[swap]{\varphi_{B_V'}} \arrow{rrr}[swap]{L_{A'}} & & & B_W' \arrow{ul}{\varphi_{B_W'}}
   \end{tikzcd}
\end{center}

Starting from \(B_V\) we have two ways to get to \(B_W'\) which are equivalent since the diagram is commutative.
\[A' \cdot T = S \cdot A \implies A' \cdot I_n = S \cdot A \cdot T^{-1} \implies A' = S \cdot A \cdot T^{-1}\]
% TODO: refer to equivalent matrices

Now from the diagram before let \(f\) be an endomorphism, then we have
\begin{center}
   \begin{tikzcd}
      B_V \arrow{dd}[swap]{L_S} \arrow{dr}{\varphi_{B_V}} \arrow{rrr}{L_{A}} & & & B_V \arrow{dl}[swap]{\varphi_{B_V}} \arrow{dd}{L_S} \\
               & V \arrow{r}{f} & V \\
      B_V' \arrow{ur}[swap]{\varphi_{B_V'}} \arrow{rrr}[swap]{L_{A'}} & & & B_V' \arrow{ul}{\varphi_{B_V'}}
   \end{tikzcd}
\end{center}
and we would get
\[S \cdot A = A' \cdot S \implies A' = S \cdot A \cdot S^{-1}\]
which means that \(A\) and \(A'\) are similar (\ref{def:sim_mat}).

\begin{example}
   As before let \(V = \mathbb{R}^2\) be a K-vector space and
   \[B = \{e_1, e_2\} \quad\text{and}\quad B' = \left\{\begin{pmatrix}2 \\1\end{pmatrix}, \begin{pmatrix}-1\\1\end{pmatrix}\right\}\]
   So we still have the transformation matrix for the change of basis
   \[S = \begin{pmatrix}2 & -1\\1 & 1\end{pmatrix}\]
   Now let \(f: V \to V\) be a 90 degree rotation, this means \(f\) can be represented by \(A = \begin{pmatrix}0 & -1\\1 & 0\end{pmatrix}\).

   \begin{center}
      \begin{tikzcd}
         B \arrow{dr}{\varphi_{B}} \arrow{rrr}{L_{A}} & & & B' \arrow{dl}[swap]{\varphi_{B'}} \\
                  & V \arrow{r}{f} & V
      \end{tikzcd}
   \end{center}

   But \(A\) is still written in terms of \(B\), we can describe the rotation also in terms of \(B'\).
   To to this we regard the previous diagram where we see that
   \[A' = S \cdot A \cdot S^{-1}\]
   So we invert \(S\) like so
   \[
      \left(\begin{array}{cc|cc}
            2 & -1 & 1 & 0\\
            1 & 1 & 0 & 1
      \end{array}\right) \rightsquigarrow \left(\begin{array}{cc|cc}
            1 & 0 & \frac{1}{3} & \frac{1}{3}\\
            0 & 1 & -\frac{1}{3} & \frac{2}{3}
      \end{array}\right)
   \]
   so we can calculate how this rotation looks in terms of \(B'\)
   \[\begin{pmatrix}2 & -1\\1 & 1\end{pmatrix} \cdot \begin{pmatrix}0 & -1\\1 & 0\end{pmatrix} \cdot \begin{pmatrix}\frac{1}{3} & \frac{1}{3}\\ -\frac{1}{3} & \frac{2}{3}\end{pmatrix} = \begin{pmatrix}\frac{1}{3} & -\frac{5}{3}\\ \frac{2}{3} & -\frac{j}{3}\end{pmatrix}\]
\end{example}

\begin{definition}[Diagonalizable Endomorphism]
   \(f: V \to V\) is diagonalizable/triangularizable if its corresponding matrix is diagonalizable/triangularizable.
\end{definition}

\newpage

\section{Metric- \& Normed Spaces}
In this section we will see that inroducing a new structure on \(K\)-vector spaces, namely the \emph{scalar product}, gives us a way to ``measure'' vectors and the distance between them.

\subsection{Definition \& Terminology}
A \emph{norm} is a function that assigns a length or size to each vector in a vector space, except for the zero vector, which is assigned a length of zero.

\begin{definition}[Norm]\label{def:norm}
   Given a \(\mathbb{R}\)-vector space \(V\), \(\|\ldots\|: V \to \mathbb{R}_0^+\) is a norm iff
   \begin{enumerate}[label=\roman*, align=Center]
      \item \(\forall v \in V: \|v\| = 0 \iff v = 0\)
      \item \(\forall \lambda \in \mathbb{R}, v \in V: \|\lambda \cdot v\| = |\lambda| \cdot \|v\|\)
      \item \(\forall v, w \in V: \|v + w\| \leq \|v\| + \|w\|\)
   \end{enumerate}
\end{definition}
\begin{remark}
   A vector space with a norm is called a \emph{normed space}.
\end{remark}

\begin{definition}[Equivalent Norms]
   Given a normed \(K\)-vector space \(V\), \(\|\ldots\|_a\) and \(\|\ldots\|_b\) are equivalent on \(V\) iff
   \[\exists c, C \in K: \big(\forall v \in V: c \|v\|_a \leq \|v\|_b \leq C \|v\|_a\]
\end{definition}

\begin{theorem}
   On \(K^n\) are all norms equivalent.
\end{theorem}

A \emph{metric} or distance function is a function that defines a distance between each pair of elements of a set.

\begin{definition}[Metric]
   Let \(X\) be a set, \(d: X \times X \to \mathbb{R}^+\) is a metric iff
   \begin{enumerate}[label=\roman*, align=Center]
      \item \(\forall x, y \in X: d(x, y) = 0 \iff x = y\)
      \item \(\forall x, y \in X: d(x, y) = d(y, x)\)
      \item \(\forall x, y, z \in X: d(x, z) \leq d(x, y) + d(y, z)\)
   \end{enumerate}
\end{definition}

\begin{definition}[Metric Space]
   A set \(X\) with a metric \(d\).
\end{definition}
\begin{remark}[Notation]
   \((X, d)\)
\end{remark}

\begin{definition}[Complete Metric Space]
   \((X, d)\) is complete iff for all cauchy sequences \((x_n)_{n \in \mathbb{N}} \in X^\mathbb{N}\) holds that \(\lim_{n \to \infty} x_n \in X\).
\end{definition}
\begin{remark}
   Also called \emph{Banach space}.
\end{remark}

\begin{theorem}[Complete Metric Spaces]
   The following normed spaces are complete
   \begin{enumerate}[label=\roman*, align=Center]
      \item \(\mathbb{C}\) and \(\mathbb{R}\)
      \item \(K^n\)
      \item \((l_1, \|\ldots\|_1)\)
      \item \((l_2, \|\ldots\|_2)\)
      \item \((l_\infty, \|\ldots\|_\infty)\)
   \end{enumerate}
\end{theorem}

\subsection{Introduction}
In the following sections we will introduce the underlying theory of normed and metric spaces.
However we have already seen the most common example of a metric space and now look at an example to see how norms an metric fit together.

We already know that for the real vector space \(\mathbb{R}^n\) we have the \emph{standard scalar product}
\[\langle v, w \rangle := \sum_{i = 1}^n v_i \cdot w_i\]
which makes it a \emph{Euclidean vector space}.
As we will see in \cref{cor:euclid_norm}, we can use this scalar product to formulate the \emph{Euclidean norm}
\[\|v\| = \sqrt{\langle v, v\rangle} = \sqrt{v_1^2 + \ldots + v_n^2}\]
which makes \(\mathbb{R}^n\) a \emph{normed vector space}.
We intuitively see how this norm is used to compute the ``length'' of a vector
\[\left\|\begin{pmatrix}1\\0\end{pmatrix}\right\| = \sqrt{1^2 + 0^2} = 1 \qquad \left\|\begin{pmatrix}3\\0\end{pmatrix}\right\| = \sqrt{3^2 + 0^2} = 3\]
Furthermore, with this norm we can define the \emph{Euclidean distance}
\[d(v, w) := \|v - w\| = \Big\|\big((v_1 - w_1)~\ldots~(v_n - w_n)\big)\Big\| = \sqrt{(v_1 - w_1)^2 + \ldots + (v_n - w_n)^2}\]
which makes \(V\) a \emph{metric space}.
Again we see intuitively how far apart two vectors (points) are
\[d\left(\begin{pmatrix}1\\0\end{pmatrix}, \begin{pmatrix}3\\0\end{pmatrix}\right) = \sqrt{(3 - 1)^2 + (0 - 0)^2} = 2\]

\subsection{Important Examples}
\subsubsection{\(K^n\) Vector Space}
For \(x \in K^n\) we define
\begin{definition}[\(l^\infty\) Norm]
   \[\|x\|_\infty := \max_{1 \leq k \leq n} |x_k|\]
\end{definition}

\begin{definition}[\(l^1\) Norm]
   \[\|x\|_1 := \sum_{k=0}^n |x_k|\]
\end{definition}

\begin{definition}[\(l^2\) Norm]
   \[\|x\|_2 := \sqrt{\sum_{k=0}^n |x_k|^2}\]
\end{definition}

We see that the following estimations hold
\[\|x\|_\infty \leq \|x\|_1 \leq m \cdot \|x\|_\infty\]
\[\|x\|_\infty \leq \|x\|_2 \leq \sqrt{m} \cdot \|x\|_\infty\]
\[\frac{1}{\sqrt{m}}\|x\|_1 \leq \|x\|_2 \leq \sqrt{m} \cdot \|x\|_1\]

\subsubsection{Sequence Spaces}
Given a field \(K\) and a set of sequences \((x_n)_{n \in \mathbb{N}}\) with the operations
\[+: \big((x_n), (y_n)\big) \mapsto (x_n + y_n)\]
\[\cdot: \big(\lambda, (x_n)\big) \mapsto (\lambda \cdot x_n)\]
and the 0-sequence \((0, 0, \ldots)\) as identity element, we can build the sequence spaces \(l^p\) which are special cases of \(L^p\) Lebesgue spaces.

\begin{definition}[Sequences]
   \[K^{\mathbb{N}} := \{(x_n)_{n \in \mathbb{N}} \mid \forall n \in \mathbb{N}: x_n \in K\}\]
\end{definition}

\begin{definition}[Bounded Sequences]
   \[l^\infty := \{(x_n)_{n \in \mathbb{N}} \mid \|(x_n)\|_\infty < \infty\}\]
\end{definition}
\begin{remark}
   \[\|(x_n)_{n \in \mathbb{N}}\|_\infty := \sup_{n \in \mathbb{N}} |x_n|\]
\end{remark}

\begin{definition}[Sequences with convergent Series]
   \[l^1 := \left\{(x_n)_{n \in \mathbb{N}} \mid \|(x_n)\|_1 < \infty\right\}\]
\end{definition}
\begin{remark}
   \[\|(x_n)_{n \in \mathbb{N}}\|_1 := \sum_{n=0}^\infty |x_n|\]
\end{remark}

\begin{definition}[Sequences with convergent Series]
   \[l^2 := \left\{(x_n)_{n \in \mathbb{N}} \mid \|(x_n)\|_2 < \infty\right\}\]
\end{definition}
\begin{remark}
   \[\|(x_n)_{n \in \mathbb{N}}\|_2 := \sqrt{\sum_{n=0}^\infty |x_n|^2}\]
\end{remark}

\subsection{Scalar Product Spaces}
\begin{definition}[Scalar Product]\label{def:scalar_product}
   A positive definite bilinear form, which we denote
   \[\langle \ldots,~ \ldots\rangle: V \times V \to \mathbb{R} \qquad\text{where}\qquad (v, w) \mapsto \langle v, w\rangle\]
\end{definition}
\begin{remark}
   Sometimes referred to as \emph{Inner Product}.
\end{remark}

\begin{definition}[Complex Scalar Product]
   A positive definite hermitian sesquilinear form, which we denote
   \[\langle \ldots,~ \ldots\rangle: V \times V \to \mathbb{C} \qquad\text{where}\qquad (v, w) \mapsto \langle v, w\rangle\]
\end{definition}

\subsubsection{Euclidean Vector Spaces}
\begin{definition}[Euclidean Vector Space]
   An \(\mathbb{R}\)-vector space with a scalar product.
\end{definition}
\begin{remark}
   Analogously to \cref{def:scalar_product}, a vector space with a scalar product is sometimes called \emph{Inner Product Space}.
\end{remark}

\begin{definition}[Standard Scalar Product]\label{def:std_scal_prod}
   Let \(V = \mathbb{R}^n\), then we have the scalar product
   \[\langle v, w \rangle := \sum_{i=1}^n v_i \cdot w_i\]
\end{definition}
\begin{remark}
   Sometimes reffered to as \emph{Dot Product}.
\end{remark}
\begin{remark}
   We easily see that this scalar product is a \emph{symmetric} bilinear form.
   \[\langle v, w \rangle = \sum_{i=1}^n v_iw_i = \sum_{i=1}^n w_iv_i = \langle w, v\rangle\]
\end{remark}
\begin{remark}
   Furtermore, we have the following important equality
   \[\langle v, w \rangle = \sum_{i=1}^n v_i w_i = (v_1~v_2~\ldots~v_n)\cdot \begin{pmatrix}w_1\\w_2\\\vdots\\w_n\end{pmatrix} = v^T w\]
   So for \(A \in \Mat_n(\mathbb{R})\)
   \[\langle Ax, y\rangle = x^TA^Ty \qquad\text{and}\qquad \langle x, Ay\rangle = x^TAy\]
\end{remark}

\begin{proposition}[Cauchy-Schwarz Inequality]\label{pro:cauchy_schwarz}
   Given a Euclidean vector space \(V\) where we define a norm
   \[\|v\| := \sqrt{\langle v, v\rangle}\]
   then is
   \[\forall v, w \in V: |\langle v, w\rangle| \leq \|v\| \cdot \|w\|\]
   and \(|\langle v, w\rangle| = \|v\| \cdot \|w\|\) iff \(v\) and \(w\) are linear dependent.
\end{proposition}

\begin{corollary}[Euclidean Norm]\label{cor:euclid_norm}
   Given a Euclidean vector space \(V\), then is
   \[\|\ldots\|: V \to \mathbb{R}_0^+ \qquad\text{where}\qquad \|v\| := \sqrt{\langle v, v\rangle}\]
   a norm on \(V\).
\end{corollary}

\subsubsection{Unitary Vector Spaces}
\begin{definition}[Unitary Vector Space]
   A \(\mathbb{C}\)-vector space with a complex scalar product.
\end{definition}
\begin{example}[Unitary Vector Space \(\mathbb{C}^n\)]
   For the complex vector space \(V = \mathbb{C}^n\) we have the standard scalar product
   \[\langle v, w \rangle := \sum_{i=1}^n v_i \overline{w_i}\]
   for \(z \in \mathbb{C}^n\) where \(z_i = x_i + iy_i\) we have the \emph{complex euclidean norm}
   \[\|z\| = \sqrt{\langle z, z\rangle} = \sqrt{\big((x_1 + y_1)~\ldots~(x_n + y_n)\big) \begin{pmatrix}(x_1 + y_1)\\\vdots\\(x_n + y_n)\end{pmatrix}} = \sqrt{x_1^2 + y_1^2 + \ldots + x_n^2 + y_n^2}\]
   with which we can define the \emph{complex euclidean metric}.
   \[d(z, z') := \|z - z'\| = \sqrt{(x_1 - x_1' + y_1-y_1')^2 + \ldots + (x_n - x_n' + y_n-y_n')^2}\]
\end{example}

\begin{definition}[Standard Complex Scalar Product]\label{def:compl_scal_prod}
   Let \(V = \mathbb{C}^n\), then we have the scalar product
   \[\langle v, w \rangle := \sum_{i=1}^n v_i \overline{w_i}\]
\end{definition}
\begin{remark}
   As for real vector spaces we have the important equalities
   \[\langle v, w\rangle = \sum_{i=1}^n v_i \overline{w_i} = (v_1~v_2~\ldots~v_n) \cdot \begin{pmatrix}\overline{w_1}\\\overline{w_2}\\\vdots\\\overline{w_n}\end{pmatrix} = v^T\overline{w}\]
   so for \(A \in \Mat_n(\mathbb{C})\)
   \[\langle Ax, y\rangle = x^TA^T\overline{w} \qquad\text{and}\qquad \langle x, Ay \rangle = x^T\overline{A}\overline{w}\]
\end{remark}

\begin{proposition}[Complex Cauchy-Schwarz Inequality]
   Given a unitary vector space \(V\) with \(\|v\| := \sqrt{\langle v, v\rangle}\), then is
   \[\forall v, w \in V: |\langle v, w\rangle| \leq \|v\| \cdot \|w\|\]
   and \(|\langle v, w\rangle| = \|v\| \cdot \|w\|\) iff \(v\) and \(w\) are linear dependent.
\end{proposition}

\begin{corollary}[Complex Euclidean Norm]
   Given a unitary vector space \(V\), then is
   \[\|\ldots\|: V \to \mathbb{R}_0^+ \qquad\text{where}\qquad \|v\| := \sqrt{\langle v, v\rangle}\]
\end{corollary}

\subsubsection{Hadamard's Inequality}
In geometrical terms, when restricted to real numbers, it bounds the volume in Euclidean space of \(n\) dimensions marked out by \(n\) vectors \(v_i\) in terms of the lengths of these vectors \(\|vi\|\).
% TODO: how is the gramian matrix a representation of s?
\begin{definition}[Gramian Matrix \& Determinant]
   Given \((v_i)_{1 \leq i \leq n} \subset V\) of a Euclidean vector space
   \[G := \big(\langle v_i, v_j\rangle\big)_{1 \leq i,j \leq n} = \begin{pmatrix} \langle v_1, v_1 \rangle & \cdots & \langle v_1, v_n\rangle\\ \vdots & \ddots & \vdots\\\langle v_n, v_1\rangle & \cdots & \langle v_n, v_n \rangle\end{pmatrix}\]
   is the representation matrix of the scalar product \(\langle \ldots, \ldots \rangle\) of \(V\).

   We define the \emph{Gramian determinant} \(G(v_i)_{1 \leq i \leq n} := \det(G)\).
\end{definition}
\begin{remark}
   The Gramian determinant can be regarded as a generalization of the Chauchy-Schwarz inequality for \(n = 2\).
   \begin{equation*}
      \begin{split}
         \det\begin{pmatrix}\langle v, v\rangle & \langle v, w\rangle\\\langle w, v \rangle & \langle w, w\rangle\end{pmatrix} = 0 & \implies \langle v, v \rangle\langle w, w\rangle - \langle v, w\rangle^2 = 0 \implies \sqrt{\langle v, w \rangle^2} = \sqrt{\langle v, v\rangle \langle w, w\rangle}\\
         & \implies |\langle v, w\rangle| = \|v\| \|w\|
      \end{split}
   \end{equation*}
\end{remark}

\begin{proposition}
   For \((v_i)_{1 \leq i \leq n}\) of a Euclidean vector space \(V\) holds
   \[G(v_i)_{1 \leq i \leq n} \geq 0\]
   where the equality holds iff \((v_i)_{1 \leq i \leq n}\) is linear dependent.
\end{proposition}

\begin{proposition}[Hadamard's Inequality]
   Given \((v_i)_{1 \leq i \leq n} \subset V\) of a Euclidean vector space, then holds
   \[\sqrt{G(v_i)_{1 \leq i \leq n}} \leq \prod_{i=1}^n \|v_i\|\]
   where the equality holds iff \((v_i)_{1 \leq i \leq n}\) is orthogonal (\ref{def:orth_vec}).
\end{proposition}
\begin{remark}[Geometric Interpretation]
   We can interpret \(\sqrt{G(v_i)}\) as the volume of the, through its column vectors, spanned parallelepiped.

   \begin{center}
      \input{drawings/hadamard-inequality.tex}
   \end{center}

   This volume is maximal for orthogonal columns and is thus at most as large as the volume of the \(n\)-dimensional cuboid \(\prod_{i=1}^n \|v_i\|\) whose edges have length \(\|v_i\|\).
\end{remark}


\subsection{Orthogonality}
Through the Cauchy-Schwarz inequality (\ref{pro:cauchy_schwarz}) we can define the angle between two vectors \(v, w \neq 0\)
\[\langle v, w\rangle \leq \|v\|\cdot\|w\| \implies -1 \leq \frac{\langle v, w\rangle}{\|v\| \cdot \|w\|} \leq 1\]
So we can define
\[\angle(v, w) := \cos^{-1}\left(\frac{\langle v, w\rangle}{\|v\| \cdot \|w\|}\right) \in [0; \pi]\]

\begin{proposition}[Law of Cosines]
   \[\|v - w\|^2 = \|v\|^2 + \|w\|^2 - 2\|v\| \cdot \|w\| \cdot \cos\big(\angle(v, w)\big)\]
\end{proposition}

\begin{definition}[Orthogonal Vectors]\label{def:orth_vec}
   Given a Euclidean vector space \(V\), \(v, w \in V\) are orthogonal
   \[v \perp w :\iff \langle v, w\rangle = 0\]
\end{definition}
\begin{remark}
   Since we derived this notation from \cref{pro:cauchy_schwarz}, we have that
   \[\langle v, w \rangle = 0 \iff \angle(v, w) = \frac{\pi}{2}\]
   since
   \[\angle(v, w) = \frac{\pi}{2} \implies \cos\big(\angle(v, w)\big) = 0 \implies \|v - w\|^2 = \|v\|^2 + \|w\|^2\]
\end{remark}

\begin{definition}[Orthogonal Complement of a Vector]
   Given \(v \in V\) of a Euclidean vector space
   \[v^\perp := \{w \in V \mid v \perp w\}\]
\end{definition}

\begin{definition}[Orthogonal Complement of a Subset]
   Given \(\Omega \subset V\) of a Euclidean vector space
   \[\Omega^\perp := \{v \in V \mid v \perp w~\forall w \in \Omega\}\]
\end{definition}

\begin{definition}[Orthogonal Family]\label{def:orthog_fam}
   A family \((v_i)_{i \in I}\) of vectors in a Euclidean vector space \(V\) where
   \[\forall i \neq j \in I: v_i \perp v_j\]
\end{definition}
\begin{remark}
   If also \(\forall i \in I: v_i \neq 0\) the family is linear independent.
   This holds since for \(a_i \neq 0 \in \mathbb{R}\) for finitely many \(i \in I\) holds that
   \[\sum_{i, j \in I: i \neq j} a_i \langle v_i, v_j \rangle = 0 \implies a_j \cdot \|v_j\|^2 = 0 \implies a_j = 0\]
\end{remark}

\begin{definition}[Orthonormal Family]
   An orthogonal family \((v_i)_{i \in I} \subset V\) where \(\forall i \in I: \|v_i\| = 1\).
\end{definition}

\begin{proposition}[Gram-Schmidt Process]\label{pro:gram-schmidt}
   Let \(V\) be a Euclidean vector space with \(\dim(V) = n\).
   Any orthonormal family \((v_i)_{1 \leq i \leq m} \subset V\) can be extended to an orthonormal basis \((v_i)_{1 \leq i \leq n}\) of \(V\).
\end{proposition}
\begin{example}[Abstract Algorithm]
   For convenience we define the following projection operator
   \[\proj_u(v) := \frac{\langle v, u\rangle}{\langle u, u\rangle} u\]
   This operator projects the vector \(v\) orthogonally onto the line spanned by vector \(u\).
   If \(u = 0\), we define \(\proj_0(v) := 0\).
   The Gram-Schmidt process then works as follows
   \begin{equation*}
      \begin{split}
         u_1 & = v_1\\
         u_2 & = v_2 - \proj_{u_1}(v_2)\\
         u_3 & = v_3 - \proj_{u_1}(v_3) - \proj_{u_2}(v_3)\\
         u_4 & = v_4 - \proj_{u_1}(v_4) - \proj_{u_2}(v_4) - \proj_{u_3}(v_4)\\
             \vdots\\
         u_n & = v_n - \sum_{i=1}^{n-1} \proj_{u_i}(v_n)
      \end{split}
   \end{equation*}
   To receive an ortho\emph{normal} basis we have to norm those orthogonal vectors
   \[e_i = \frac{u_i}{\|u_i\|}\]
\end{example}
\begin{example}[Orthogonalizing two Vectors]
   Suppose we have \(v_1 := \begin{pmatrix}3\\1\end{pmatrix}\) and \(v_2 := \begin{pmatrix}2\\2\end{pmatrix}\) which we want to orthogonalize.
   We choose \(u_1 := v_1\) and calculate
   \[u_2 = v_2 - \proj_{u_1}(v_2) = \begin{pmatrix}2\\2\end{pmatrix} - \frac{2 \cdot 3 + 2 \cdot 1}{3^2 + 1^2} \begin{pmatrix}3\\1\end{pmatrix} = \begin{pmatrix}-\frac{2}{5}\\\frac{6}{5}\end{pmatrix}\]

   \begin{center}
      \input{drawings/gram-schmidt.tex}
   \end{center}

   where we now see
   \[\langle u_1, u_2 \rangle = 3 \cdot -\frac{2}{5} + 1 \cdot \frac{6}{5} = 0 \implies u_1 \perp u_2\]
   To have a basis of \(V\) we norm both vectors
   \[e_1 = \frac{u_1}{\|u_1\|} \qquad\text{and}\qquad e_2 = \frac{u_2}{\|u_2\|}\]
\end{example}

\begin{example}[Legendre-Polynomials]
   In the Euclidean vector space \(\mathbb{R}[x]_{\leq n}\) we have the scalar product
   \[\langle p, q\rangle := \int_{-1}^1 p(x) \cdot q(x) dx\]
   and the monic standard basis \(\{1, x, x^2, \ldots, x^n\}\).
   Through Gram-Schmidt we can orthogonalize this basis
   \[v_1 := \frac{1}{\sqrt{2}} \qquad v_2 := \sqrt{\frac{3}{2}}x \qquad x^2 - \frac{1}{3} \qquad \ldots\]
   where we see
   \[\left\langle x, \frac{1}{\sqrt{2}}\right\rangle = \int_{-1}^1 x \cdot \frac{1}{\sqrt{2}} dx = 0\]
   and
   \[x^2 = w' + w'' = w \rightsquigarrow w' = \langle w, v_1 \rangle v_1 + \langle w, v_2\rangle v_2\]
   Hence we have an orthogonal basis consisting of polynomials where \(p(1) = 1\).
\end{example}

\begin{corollary}\label{cor:gram-schmidt}
   Given a Euclidean vectorspace \(V\) and \(W \subset V\) then holds
   \[V = W \oplus W^\perp\]
\end{corollary}

\begin{proposition}[Complex Gram-Schmidt Process]
   Let \(V\) be a unitary vector space with \(\dim(V) = n\). Any orthonormal family \((v_i)_{1\leq i \leq m} \subset V\)  can be extended to an orthonormal basis \((v_i)_{1 \leq i \leq n}\) of \(V\)
\end{proposition}
\begin{corollary}
   Given a unitary vector space \(V\) and \(W \subset V\) then holds
   \[V = W \oplus W^\perp\]
\end{corollary}

\begin{definition}[Cross Product]
   \[\times: \mathbb{R}^3 \times \mathbb{R}^3 \to \mathbb{R}^3\]
   \[v \times w \mapsto (v_2w_3 - v_3w_2 \quad -v_1w_3 + v_3w_1 \quad v_1w_2 - v_2w_1)\]
\end{definition}

\begin{proposition}[Cross Product Identities]
   For the cross product \(z := x \times y\) of \(x, y \in \mathbb{R}^3\) holds
   \begin{enumerate}[label=\roman*, align=Center]
      \item \(z \perp x \land z \perp y\)
      \item \(\|z\|^2 = \|x\|^2\|y\|^2 - \langle x, y \rangle^2\)
      \item \(z = 0\) iff \(x, y\) are linear dependent.
      \item \(x, y \neq 0 \implies \|z\| = \|x\|\cdot\|y\| \sin\big(\angle(x, y)\big)\)
      \item For \(w \in \mathbb{R}^3\) and \(A := \big(x~y~z\big)\) we have \(\langle z, w\rangle = \det(A)\).
      \item \(y \times x = -x \times y\)
      \item \(w \times (x \times y) = \langle w, y\rangle x- \langle w, x \rangle y\)
      \item \(w \times (x \times y) + x \times (y \times w) + y \times (w \times x) = 0\)
   \end{enumerate}
\end{proposition}

\subsection{Self-Adjoint Endomorphisms}
Symmetric or hermitian matrices can be regarded through \(f: V \to V\) of a Euclidean or unitary vector space where
\[\forall v, w \in V: \langle f(v), w\rangle = \langle v, f(w)\rangle\]

\begin{definition}[Adjoint Endomorphisms]
   Given \(f: V \to V\) of a euclidean or unitary vector space, \(f^\ast: V \to V\) is the adjoint endomorphism of \(f\) iff
   \[\forall v, w \in V: \langle f^\ast(v), w\rangle = \langle v, f(w)\rangle\]
\end{definition}
\begin{remark}
   We always have \((f^\ast)^\ast = f\).
\end{remark}

This way we choose an orthonormal basis and reduce this property to matrices.
This means we want to find \(A^\ast\) for \(A \in \Mat_n(\mathbb{R})\) such that
\[\forall x, y \in \mathbb{R}^n: \langle A^\ast x, y\rangle = \langle x, Ay\rangle\]
we see that
\[\langle A^Tx, y \rangle = x^TAy = \langle x, Ay\rangle\]
so we get
\[A \in \Mat_n(\mathbb{R}) \implies A^\ast = A^T \qquad\text{and}\qquad A \in \Mat_n(\mathbb{C}) \implies A^\ast = A^H\]

\begin{definition}[Self-Adjoint Endomorphism]
   \(f: V \to V\) where \(f = f^\ast\).
\end{definition}
\begin{remark}
   For matrices this means
   \[A \in \Mat_n(\mathbb{R})~\text{self-adjoint} \iff A~\text{symmetric}\]
   \[A \in \Mat_n(\mathbb{C})~\text{self-adjoint} \iff A~\text{hermitian}\]
\end{remark}

\begin{proposition}[Spectral Theorem]\label{pro:spectral_thm}
   Given a finite dimensional Euclidean of unitary vector space \(V\) and a self-adjoint \(f: V \to V\), then there exists an orthonormal basis of \(V\) consisting of eigenvectors with real eigenvalues.
\end{proposition}

\begin{proposition}[Principal Axis Theorem]
   Let \(s: \mathbb{R}^n\times\mathbb{R}^n \to \mathbb{R}\) be symmetric with the representation matrix \(A\) regarding the standardbasis of \(\mathbb{R}^n\) and \((v_i)_{1 \leq i \leq n}\) an orthonormal basis of \(\mathbb{R}^n\) consisting of eigenvectors, i.e. \(A v_i = \lambda_i v_i\).

   Regarding this basis \(s\) can be represented by
   \[\begin{pmatrix}\lambda_1 & & 0\\ & \ddots & \\ 0 & & \lambda_n\end{pmatrix}\]
\end{proposition}

\begin{corollary}
   Let \(\sigma(s) = (n_+, n_-, n_0)\).
   \begin{itemize}
      \item \(n_+\) is the number of \(\lambda_i > 0\)
      \item \(n_-\) is the number of \(\lambda_i < 0\)
      \item \(n_0\) is the number of \(\lambda_i = 0\)
   \end{itemize}
\end{corollary}

\begin{proposition}[Principal Minor Criterion by Hurwitz-Sylvester]
   A symmetric matrix \(A \in \Mat_n(\mathbb{R})\) is positive definite iff all principal minors of \(A\) are positive.
\end{proposition}
\begin{remark}
   A \emph{principle} minor is the determinant of a submatrix \(A_k\) which is computed by remove the \(k\)th row and column of \(A\).
\end{remark}
\begin{example}
   Suppose we have \(A = \begin{pmatrix}2&-1&0\\-1&2&-1\\0&-1&2\end{pmatrix}\).
   Then we get
   \[A_1 = \begin{pmatrix}2&-1\\-1&2\end{pmatrix} \qquad A_2 = \begin{pmatrix}2&0\\0&2\end{pmatrix} \qquad A_3 = \begin{pmatrix}2&-1\\-1&2\end{pmatrix}\]
   So we have the principle minors
   \[\det(A_1) = 3 \qquad \det(A_2) = 4 \qquad \det(A_3) = 3\]
   So we see \(A\) is positive definite.
\end{example}

\Cref{pro:eigenbasis} and \cref{pro:spectral_thm} both prove diagonalizability through orthogonal bases but with different assumptions.
The following is a generalization of both over unitary vector spaces.
\begin{definition}[Normal Endomorphism]
   Given a finite-dimensional Euclidean or unitary vector space \(V\), \(f: V \to V\) is normal iff
   \[f^\ast \circ f = f \circ f^\ast\]
\end{definition}

\begin{proposition}[Spectral Theorem for Normal Endomorphisms]
   Given a finite-dimensional unitary vector space \(V\), \(f: V \to V\) is normal iff there exists an orthonormal basis of \(V\) consisting of eigenvectors of \(f\).
\end{proposition}

\section{Moduln}
In a vector space, the set of scalars is a field and acts on the vectors by scalar multiplication, subject to certain axioms such as the distributive law.
In a module, the scalars need only be a ring, so the module concept represents a significant generalization.
In commutative algebra, both ideals and quotient rings are modules, so that many arguments about ideals or quotient rings can be combined into a single argument about modules.

\begin{definition}[R-Module]
   A unitary commutative ring \(R\), a set \(M\) and two operations
   \[\dotplus: M \times M \to M \quad\text{where}\quad (m, n) \mapsto m \dotplus n\]
   \[\cdot: R \times M \to M \quad\text{where}\quad (r, m) \mapsto r \cdot m\]
   if the following holds
   \begin{enumerate}[label=\roman*, align=Center]
      \item \((M, \dotplus, 0_M)\) is an abelian group.
      \item Associativity: \(\forall r, s \in R,~m \in M: r \cdot (s \cdot m) = (rs) \cdot m\)
      \item Distributivity: \(\forall r, s \in R,~m,n \in M\) holds
         \[(r + s) \cdot m = r \cdot m \dotplus s \cdot m\]
         \[r \cdot (m \dotplus n) = r \cdot m \dotplus r \cdot n\]
   \end{enumerate}
\end{definition}
\begin{remark}
   If additionally \(\forall m \in M: 1_R \cdot m = m\) we call the module \emph{unitary}.
   As with rings we have \(0 \cdot m = 0\) and \((-1) \cdot m = -m\).
\end{remark}
\begin{remark}
   If the module is defined over a field, it is a \emph{vector space}.
\end{remark}
\begin{example}[\(\mathbb{Z}\)-Module]
   The concept of a \(\mathbb{Z}\)-module agrees with the notion of an abelian group.
   That is, every abelian group is a module over the ring of integers (a term of algebraic number theory) in a unique way.
\end{example}

Much of the theory of modules consists of extending as many of the desirable properties of vector spaces as possible to the realm of modules over a ``well-behaved'' ring, such as a principal ideal domain (an integral domain in which every ideal is principal).
However, modules can be quite a bit more complicated than vector spaces; for instance, not all modules have a basis.
Even those that do need not have a unique \emph{rank} (analogue to the dimension of a vector space).

\begin{definition}[Free Module]
   A Module which has a basis.
\end{definition}
\begin{remark}
   This concept represents a generalization of K-vector spaces and free abeliean groups (an abelian group which, regarded as \(\mathbb{Z}\)-module, has a basis).
\end{remark}
\begin{example}
   For \(n > 0\) suppose
   \[n \cdot x = \underbrace{x + x + \ldots + x}_{n\text{-summands}} \qquad 0 \cdot x = 0 \qquad (-n) \cdot x = -(n \cdot x)\]
   Such a module need not have a basis --- groups containing torsion elements (elements annihilated by any regular element of a ring in a module) do not.
   For example, in the group of integers modulo 3, one cannot find even one element which satisfies the definition of a linearly independent set since when an integer such as 3 or 6 multiplies an element, the result is 0.
   However, if a finite field is considered as a module over the same finite field taken as a ring, it is a vector space and does have a basis.
\end{example}

\begin{definition}[Quotient Module]
   Given a module \(M\), a subspace \(N \subset M\) and \(\sim\) on \(M\) defined as
   \[m_1 \sim m_2 :\iff m_1 - m_2 \in N\]
   the set \(M_{/N} = \{[m] \mid m \in M\}\) forms the quotient vector space where
   \[[m_1] + [m_2] := [m_1 + m_2] \quad\text{and}\quad \lambda [m] := [\lambda m]\]
\end{definition}
\begin{remark}
   We call the map \(m \mapsto [m]\) a \emph{canonical homomorphism}.
\end{remark}

\subsection{Sequences of Moduln}
% TODO
\begin{definition}[Sequence of R-Modules]
   Given a commutative ring \(R\) and an index intervall \(I \subset \mathbb{Z}\)
   \[(M_i)_{i \in I} \qquad\text{with}\qquad f_i: M_i \to M_{i+1}\]
\end{definition}

\begin{definition}[Chain Complex]
   Given a sequence of \(R\)-modules iff \(f_i \circ f_{i-1} = 0\)
\end{definition}

\begin{definition}[Exact]
   A sequence is exact in the \(i\)-th position iff
   \[\im(f_{i-1}) = \ker(f_i)\]
\end{definition}

\begin{definition}[Exact Sequence]
   A sequence is exact if it is exact in every position.
\end{definition}
\begin{example}
   \[0 \to M' \xrightarrow{f} M \xrightarrow{g} M''\]
   is exact iff \(f\) is injective and \(\im(f) = \ker(g)\).
   In this case the sequence is \emph{left exact}.
   Similarly is
   \[M' \xrightarrow{f} M \xrightarrow{g} M'' \to 0\]
   \emph{right exact} iff \(g\) is surjective and \(\im(f) = \ker(g)\).

   A \emph{short exact} sequence is a left and right exact sequence.
\end{example}

\begin{proposition}
   Let \(R\) be a commutative ring, \(M, M', M''\) be \(R\)-modules and \(f: M' \to M\), \(g: M \to M''\) be homomorphisms.
   \[M' \xrightarrow{f} M \xrightarrow{g} M'' \to 0\]
   is right exact iff for every R-module \(N\), the induced homomorphisms
   \[\Hom(M,N) \to \Hom(M', N) \qquad\text{and}\qquad \Hom(M'', N) \to \Hom(M, N)\]
   fit into a left exact sequence
   \[0 \to \Hom(M'',N) \to \Hom(M, N) \to \Hom(M', N)\]
\end{proposition}

\subsection{Modules over Euclidean domains}
\begin{definition}[Submodule]\label{def:submodule}
   Given an R-module \(M\) over a ring \(R\), then is \(N \subset M\) a submodule if
   \begin{enumerate}[label=\roman*, align=Center]
      \item \((N, +)\) is a subgroup of \((M, +)\)
      \item Scalar Multiplication Closure: \(\forall r \in R, n \in N: r \cdot n \in N\)
   \end{enumerate}
\end{definition}
\begin{remark}
   A submodule of an R-module is an ideal of \(R\).
\end{remark}
\begin{remark}
   % TODO: minimal polynomial
   We can restate that the minimal polynomial of a quadratic matrix is well-defined in the following way.

   Let \(R\) be a Euclidean domain, \(I \subset R\) be a non-zero Ideal and \(a \in I: a \neq 0\) such that
   \(\forall b \in I: f(a) \leq f(b)\), then holds that \(I = a R\).
\end{remark}

\begin{definition}[Finitely Generated Module]
   A module that has a finite generating set.
\end{definition}
\begin{remark}
   More precisely: an \(R\)-module \(M\) is finitely generated if \(\exists g_1, \ldots, g_n \in M\) such that
   \[\forall m \in M: (\exists r_1, \ldots, r_n \in R: m = r_1g_1 + \ldots + r_ng_n)\]
   \(G := \{g_1, \ldots, g_n\}\) is refferred to as the \emph{generating set} for \(M\).
   The finite generators need not be a basis, since they need not be linearly independent over \(R\).
\end{remark}

% TODO: erzeugendensystem unterschiede
\begin{definition}[Generating Set of Modules]
   Given a module \(M\) over a ring \(R\) then is \(G \subset M\) its generating set if
   \[\forall m \in M: (\exists r_1, \ldots, r_n \in R, g_1, \ldots, g_n \in G: m = r_1g_1 + \ldots + r_ng_n)\]
\end{definition}
\begin{remark}
   Given a module \(M\) over a ring \(R\) then is \(G \subset M\) the generating set of \(M\) if the smallest submodule \(N \subset M\) containing \(G\) is \(M\) itself.
   The smallest submodule containing a subset is the intersection of all submodules containing the set.
   In the case where the module \(M\) is a vector space over a field \(K\), and the generating set is linearly independent, \(n\) is well-defined and is referred to as the dimension of \(M\).
\end{remark}

Let \((m_i)_{i \in I}\) be a set of element of a module \(M\).
There is an associated homomorphism
\[f: \bigoplus_{i \in I} R \to M \qquad\text{defined by}\qquad (r_i)_{i \in I} \mapsto \sum_{i \in I} a_i \cdot m_i\]
Then we can state that
\[(m_i)~\text{generates}~M \iff f~\text{is surjective}\]
\[(m_i)~\text{is linearly independent} \iff f~\text{is injective}\]
\[(m_i)~\text{is a basis of}~M \iff f~\text{is bijective}\]

\begin{proposition}[Free Submodules]\label{pro:fin_gen_mod}
   Let \(R\) be a Euclidean domain and \(M\) a free, finitely generated R-module.

   All submodules \(N \subset M\) are also free, finitely generated and \(\rk(N) \leq \rk(M)\).
\end{proposition}

\subsection{Structure Theorem for R-Moduln}
For the proof of \cref{thm:smith-normalform} (ii) we need the structure theorem for finitely generated R-moduln.
To introduce this we also need some background.

\begin{definition}[Cokernel]
   Given a linear map \(f: V \to W\) then is
   \[\coker(f) := W_{/\im(f)}\]
\end{definition}
\begin{remark}
   For \(A \in \Mat_{m, n}(K)\) we have
   \[\coker(A) := \{x \in \Mat_{m, 1} \mid x^T \cdot A = 0^T\}\]
   which is just like the kernel but with left multiplication.
\end{remark}

\begin{definition}[Torsion]
   Given an integral domain \(R\) and an R-module \(M\).
   \[M_{\tors} := \{m \in M \mid \exists a \in R: a \neq 0 \land a \cdot m = 0\}\]
\end{definition}
\begin{remark}
   For \(0 \neq a \in R\) we define
   \[M[a] := \{m \in M \mid a \cdot m = 0\}\]
   as the a-torsion and
   \[M(a) := \bigcup_{n \in \mathbb{N}^+} M[a^n]\]
   as the a-primary torsion.

   Given a Euclidean domain \(R\) with a prime element \(a \in R\), let \(K := R_{/aR}\) be a residue field.
   The structure of the \(R\)-module \(M\) gives \(M[a]\) the structure of a \(K\)-vector space.
   More general: for\(l \in \mathbb{N}^+\) \(M[a^l]_{/M[a^{l-1}]}\) is a \(K\)-vector space where
   \[l = 1 \implies M[a]_{/M[0]} \implies M[a]\]
\end{remark}

\begin{lemma}[Chinese Remainder Theorem]\label{lem:chin_remainder}
   Let \(R\) be a Euclidean domain and \(a, b \in R \setminus \{0\}\) coprime.
   Then we have the isomorphism
   \[R_{/abR} \xrightarrow{\sim} R_{/aR} \oplus R_{/bR} \quad\text{where}\quad [c] \mapsto ([c], [c])\]
\end{lemma}

\begin{theorem}[Fundamental Theorem for R-Moduln]\label{thm:fund_thm_mod}
   Let \(R\) be a Euclidean domain and \(M\) an R-module.
   \(\exists! r, s \in \mathbb{N}_0\) and \((a_i)_{i=1}^r \subset R^\times: (\forall i: a_i | a_{i+1})\) (which are unique except for multiplication with units) such that
   \[M \cong \bigoplus_{i=1}^r R_{/a_iR} \oplus R^s\]
\end{theorem}

\begin{corollary}
   Let \(f: M \to N\) be a homomorphism of \(\mathbb{Z}\)-moduln, then is

   \(|\det(f)|\) well-defined and
   \[\big(|\det(f)| \neq 0 \iff |\coker(f)| < \infty\big) \implies |\coker(f)| = |\det(f)|\]
\end{corollary}

\section{Homomorphisms}
A \emph{homomorphism} is a \emph{structure-preserving} map (meaning it preserves the operations of the structures) between two algebraic structures of the same type (e.g. two groups).

This means a map \(f: A \to B\) between two sets \(A, B\) equipped with the same structure such that, if \(\ast\) is an operation of the structure (supposed here, for simplification, to be a binary operation), then
\[f(x\ast y) = f(x) \ast f(y),~\forall x,y \in A\]
We say that \(f\) \emph{preserves} the operation or is \emph{compatible} with the operation.

\subsection{Definition \& Terminology}
\begin{definition}[Homomorphism Kernel]
   Given a homomorphism \(f\)
   \[\ker(f) = f^{-1}(0) := \{v \in V \mid f(v) = 0_W\}\]
\end{definition}
\begin{remark}[Intuition]
   The kernel of a homomorphism measures the degree to which the homomorphism fails to be injective.
   \[f~\text{injective} \iff \ker(f) = \{0\}\]
   This is to say the kernel is \emph{trivial} i.e. only contains the identity element.
   The kernel of a matrix is
   \[\ker(A) := \{v \in K^n \mid A \cdot v = 0\}\]
\end{remark}

\begin{definition}[Monomorphism]
   An injective homomorphism.
\end{definition}

\begin{definition}[Epimorphism]
   A surjective homomorphism.
\end{definition}

\begin{definition}[Isomorphism]
   A bijective homomorphism.
\end{definition}

\begin{definition}[Endomorphism]
   A homomorphism, mapping a structure onto itself.
\end{definition}
\begin{remark}[Notation]
   \(f: A \xrightarrow{\simeq} A\)
\end{remark}
\begin{example}
   Let \(G\) be a group, then is \(\id_G: G \xrightarrow{\simeq} G\) an endomorphism.
\end{example}

\begin{definition}[Automorphism]
   A bijective endomorphism.
\end{definition}
\begin{remark}[Notation]
   \(f: A \xrightarrow{\cong} A\)
\end{remark}

\subsection{Homomorphisms of Algebraic Structures}
\begin{definition}[Group Homomorphism]
   Given groups \(G, H\), a map \(f: G \to H\) where
   \[\forall a, b \in G: f(a) \ast_H f(b) = f(a \ast_G b)\]
\end{definition}
\begin{remark}
   \(\ker(f)\) is a subgroup of \(G\) and \(\im(f)\) is a subgroup of \(H\).
\end{remark}
\begin{remark}
   From the definition follows directly that
   \[f(e_G) = f(e_G +_G e_G) = f(e_G) +_H f(e_G) \implies f(e_G) = e_H\]
   \[f(a^{-1}) = \big(f(a)\big)^{-1}\]
\end{remark}

\begin{definition}[Ring Homomorphism]\label{def:ring_homo}
   Given rings \(R, S\), a map \(f: R \to S\) where
   \[\forall a, b \in R: f(a +_R b) = f(a) +_S f(b)\]
   \[\forall a, b \in R: f(a \cdot_R b) = f(a) \cdot_S f(b)\]
\end{definition}
\begin{remark}[Unitary Rings]
   For unitary rings must additionally hold that \(f(1_R) = 1_S\).
\end{remark}
\begin{remark}
   Given two fields \(K\) and \(L\), every ring homomorphism \(K \to L\) is injective.
\end{remark}
\begin{example}
   \(\forall n \in \mathbb{Z}\) is
   \[\varphi_n: \mathbb{Z} \to \mathbb{Z}/_{n\mathbb{Z}}\]
   a homomorphism of unitary rings which is always surjective.
\end{example}

\begin{definition}[Module Homomorphism]
   Given \(R\)-modules \(M, N\), a map \(f: M \to N\) where
   \[\forall a, b \in M: f(a +_M b) = f(a) +_N f(b)\]
   \[\forall r \in R, a \in M: f(r \cdot_M a) = r \cdot_N f(a)\]
\end{definition}

\subsection{Linear Maps}
\begin{definition}[Linear Map]
   Given \(K\)-vector spaces \(V, W\), a map \(f: V \to W\) where
   \[\forall v, w \in V: f(v +_V w) = f(v) +_W f(w)\]
   \[\forall \lambda \in K,~v \in V: f(\lambda \cdot_V v) = \lambda \cdot_W f(v)\]
\end{definition}
\begin{remark}
   A linear map is a homomorphism of K-vector spaces.
   As with other homomorphisms holds \(\ker(f) = \{0\} \iff f~\text{is injective}\).
\end{remark}
\begin{remark}
   It holds that
   \[\im(f) := \{w \in W \mid \exists v \in V: f(v) = w\} \subset W\]
   \[\ker(f) := \{v \in V \mid f(v) = 0\} \subset V\]
   are vector subspaces as well as
   \[f(U) = \{w \in W \mid \exists v \in U: f(v) = w\} \subset W\]
   \[f(Z)^{-1} = \{v \in V \mid f(v) \in Z\} \subset V\]
   where \(U\) is a subspace of \(V\) and \(Z\) of \(W\).
\end{remark}

\begin{definition}[Rank of Linear Maps]\label{def:linmaprank}
   Given a linear map \(f: V \to W\)
   \[\rk(f) := \dim(\im(f))\]
\end{definition}

\subsubsection{Linear Transformations as Matrices}
Given the vector spaces \(K^n, K^m\) we can regard the linear maps \(f: K^n \to K^m\) between them.
Such a linear map is uniquely identified through the images of the standard basis vectors \(e_i \in K^n\).
\[e_i = \bordermatrix{
      ~ &   \cr
        & 0 \cr
      i & 1 \cr
        & 0 \cr
        & \vdots} \rightsquigarrow f(e_i)\]
\(e_i\) regarded as a matrix is a
\begin{definition}[Column Vector]
   A matrix where \(n = 1\)
   \[A_{m,1} = \begin{pmatrix} a_1 \\ \vdots \\ a_m \end{pmatrix}\]
\end{definition}
analogous we also have the
\begin{definition}[Row Vector]
   A matrix where \(m = 1\)
   \[A_{1,n} = (a_1, \ldots, a_n)\]
\end{definition}

To illustrate the point above let \(v = (v_1, \ldots, v_m) \in K^n\)
\[v = v_1 \cdot e_1 + v_2 \cdot e_2 + \ldots + v_m \cdot e_m\]
\[v_1 \cdot e_1 = (v_1, 0, \ldots, 0)\]
\[v_2 \cdot e_2 = (0, v_2, \ldots, 0)\]
\[\ldots\]
\[f(v) = f(v_1 e_1) + f(v_2 e_2) + \ldots + f(v_m e_m) = v_1 f(e_1) + v_2 f(e_2) + \ldots + v_m f(e_m)\]

As we will see in \cref{def:matrix_mult} is the resulting vector \(f(v) \in K^m\) a product of matrices \(A \cdot v\).
To create such a matrix \(A\) we write the images of all \(e_i\) as column vectors side by side in an \(n\)-tuple of elements in \(K^m\).
Then we can regard \(f\) through the following

\begin{definition}[\(L_A\)]\label{def:left_mat_mult}
   Given \(A \in \Mat_{m,n}(K)\) and the column vector \(x \in \Mat_{m,1}(K)\)
   \[L_{A} := \Mat_{m,1}(K) \to \Mat_{1,n}(K) \quad\text{where}\quad x \mapsto A \cdot x\]
\end{definition}
\begin{remark}
   It can be proven that
   \[\forall f: K^n \to K^m~\exists A \in \Mat_{m,n}(K): f(x) = A \cdot x = L_{A}(x)\]
   This means that every linear map \(f: K^n \to K^m\) can be represented by a matrix \(A\).
   In the case of a \textit{linear transformation} \(f: \mathbb{R}^n \to \mathbb{R}^m\) we call \(A\) a \textit{transformation matrix}.
\end{remark}

\begin{proposition}[\(\Hom_K(K^n, K^m) \cong \Mat_{m,n}(K)\)]\label{pro:linmap_mat_iso}
   Let \(K\) be a field and \(m, n \in \mathbb{N}\).

   The evaluation of \(f: K^n \to K^m\) at \(e_1, \ldots, e_n \in K^n\) results in an isomorphism of K-vector spaces
   \[\Hom_K(K^n, K^m) \xrightarrow{\sim} \Mat_{m,n}(K)\]
\end{proposition}
\begin{remark}
   Because this isomorphism exists we say that the set of linear maps and matrices are isomorphic.
   \[\Hom_K(K^n, K^m) \cong \Mat_{m,n}(K)\]
   Two isomorphic objects (in our case a linear map and a matrix) cannot be distinguished by using only the properties used to define morphisms; thus isomorphic objects may be considered the same.
   This leads us to the following definitions
   \[\im(A) := \im(L_A) \qquad\text{and}\qquad \ker(A) := \ker(L_A)\]
\end{remark}

Now we can use \cref{pro:linmap_mat_iso} to see how the matrix multiplication is defined.
We can regard it as the composition of two linear maps \(f\) and \(g\) which correspond to two matrices \(A\) and \(B\).
The following linear map
\[\Hom_K(K^n, K^m) \times \Hom_K(K^l, K^n) \to \Hom_K(K^l, K^m)\]
\[(f, g) \mapsto f \circ g\]
\[K^l \xrightarrow{g} K^n \xrightarrow{f} K^m\]
represents the matrix multiplication of two matrices \(A\) and \(B\)
\[\Mat_{m,n}(K) \times \Mat_{n,l}(K) \to \Mat_{m,l}(K)\]
\[(A, B) \mapsto A \cdot B\]
to illustrate:
\[f \in \Hom_K(K^n, K^m) \cong \Mat_{m,n}(K) \ni A\]
\[g \in \Hom_K(K^l, K^n) \cong \Mat_{n,l}(K) \ni B\]
\[g \circ f \in \Hom_K(K^l, K^m) \cong \Mat_{m,l}(K) \ni A \cdot B\]

\begin{example}
   Let \(f: \mathbb{R}^2 \to \mathbb{R}^2\) be defined as \(f(x, y) := (x - y, y - x)\).
   We construct the corresponding matrix \(A\).
   \[f(e_1) = f(1, 0) = (1, -1)\]
   \[f(e_2) = f(0, 1) = (-1, 1)\]
   \[A = \bordermatrix{
         ~ & f(e_1)  & f(e_2) \cr
           & 1       & -1     \cr
           & -1      & 1      \cr
   }\]

   If we now take \(x = (2, 5) \in \mathbb{R}^2\) we see that
   \[f(x) = (2 - 5, 5 - 2) = (-3, 3)\]
   \[A \cdot x = \begin{pmatrix} 1 & -1 \\ -1 & 1\end{pmatrix} \cdot \begin{pmatrix}2 \\ 5\end{pmatrix} = \begin{pmatrix}2 + (-5) \\ (-2) + 5\end{pmatrix} = \begin{pmatrix}-3 \\ 3\end{pmatrix}\]

   Now let \(g: \mathbb{R}^3 \to \mathbb{R}^2\) be defined as \(g(x, y, z) := (x, z)\) the corresponding matrix
   \[B = \bordermatrix{
         ~ & g(e_1) & g(e_2) & g(e_3) \cr
           & 1      & 0      & 0      \cr
           & 0      & 0      & 1      \cr
   }\]
   Now we have first to calculate \(A \cdot B\)
   \[A \cdot B = \begin{pmatrix}1 & -1 \\ -1 & 1\end{pmatrix} \cdot \begin{pmatrix}1 & 0 & 0\\ 0 & 0 & 1\end{pmatrix} = \begin{pmatrix}1 & 0 & -1\\ -1 & 0 & 1\end{pmatrix}\]

   To illustrate the main point we take \(x = (2, 3, 5) \in \mathbb{R}^3\)
   \[(f \circ g)(x) = f(g(2, 3, 5)) = f(2, 5) = (-3, 3)\]
   \[(A \cdot B) \cdot x = \begin{pmatrix}1 & 0 & -1\\ -1 & 0 & 1\end{pmatrix} \cdot \begin{pmatrix}2 & 3 & 5\end{pmatrix} = \begin{pmatrix}-3\\3\end{pmatrix}\]
\end{example}

\begin{theorem}[\(\rk(A) = \rk(L_A)\)]
   Given a matrix \(A \in \Mat_{m,n}(K)\) and the linear map \(L_{A}: K^n \to K^m\) (\ref{def:left_mat_mult})
   \[\rk(A) = \rk(L_A)\]
\end{theorem}

\subsection{Isometries}
An isometry is a distance preserving transformation between metric spaces.
\begin{definition}[Isometry]
   Given metric spaces \(X, Y\), a map \(f: X \xrightarrow{\sim} Y\) where
   \[\forall a, b \in X: d_X(a, b) = d_Y\big(f(a), f(y)\big)\]
\end{definition}
\begin{remark}
   As we've seen before, this applies to normed spaces aswell since we can always define a metric based on a norm, hence for \(f: V \to W\) we write
   \[\|v - w\|_V = \|f(v) - f(w)\|_W\]
   Important to note however is that every isometry on a euclidean vector space also preserves angles.
\end{remark}
\begin{remark}
   An isometry is always injective.
\end{remark}
\begin{example}
   Any reflection, translation and rotation is a global isometry on Euclidean spaces.
   Suppose we regard a translation
   \[f: V \to W \qquad\text{where}\qquad v \mapsto v + v_0\]
   we see that
   \[\|f(v) - f(w)\|_W = \|(v + v_0) - (w + v_0)\|_V = \|v - w\|_V\]
   Note, however, that \(f\) is not a linear map since it doesn't leave the origin in place.
\end{example}

Every isometry between Euclidean spaces where \(0_V \mapsto 0_W\) is linear.
This statement holds more generally for real vector spaces and is the \emph{Mazur-Ulam theorem}.

\begin{proposition}\label{pro:orth_group}
   Let \(V\) be a finite-dimensional, Euclidean vector space and \(f \in \OG(V)\).

   There exists an orthonormal basis of \(V\), such that \(f\) can be represented through a matrix
   \[
      \begin{pmatrix}
         I_r    & 0      & \cdots & \cdots & 0\\
         0      & -I_s   & \ddots &        & \vdots \\
         \vdots & \ddots & R_1    & \ddots & \vdots \\
         \vdots &        & \ddots & \ddots & 0\\
         0      & \cdots & \cdots & 0      & R_t\\
      \end{pmatrix} \qquad\text{where}\qquad R_i := \begin{pmatrix}
         \cos(\theta_i) & -\sin(\theta_i)\\
         \sin(\theta_i) & \cos(\theta_i)
      \end{pmatrix}
   \]
   with \(r, s, t \in \mathbb{R}\) and \(\theta_i \in (0; \pi)\).
\end{proposition}
% TODO: process?
\begin{remark}
   To summarize: We know that we can represent a transformation \(f\) as a matrix \(A\).
   Then we can regard this transformation in terms of a different basis, which is represented by \(A'\).
   We can determine \(A'\) through a transformation matrix for the change of basis \(B\).
   \[B^{-1}AB = A'\]
   We get \(B\) by writing the basis vectors of the other basis as columns in \(B\).
   Note, however, that since we choose an orthonormal basis that \(B \in \OG(n)\) so
   \[B^TAB = A'\]
   The proposition states that if we do this, \(A'\) is in the form shown above.
\end{remark}
\begin{example}
   Suppose we have the isometry \(f: \mathbb{R}^3 \to \mathbb{R}^3\) given through
   \[A := \frac{1}{9}\begin{pmatrix}-4&1&8\\4&8&1\\-7&4&-4\end{pmatrix}\]
   First we check if \(A\) is indeed orthogonal, i.e. the columns of \(A\) are orthogonal
   \[\begin{pmatrix}-\frac{4}{9}&\frac{4}{9}&-\frac{7}{9}\end{pmatrix} \cdot \begin{pmatrix}\frac{1}{9}\\\frac{8}{9}\\\frac{4}{9}\end{pmatrix} = -\frac{4}{81} + \frac{32}{81} - \frac{28}{81} = 0\]
   \[\begin{pmatrix}-\frac{4}{9}&\frac{4}{9}&-\frac{7}{9}\end{pmatrix} \cdot \begin{pmatrix}\frac{8}{9}\\\frac{1}{9}\\-\frac{4}{9}\end{pmatrix} = -\frac{32}{81} + \frac{4}{81} + \frac{28}{81} = 0\]
   \[\begin{pmatrix}\frac{1}{9}&\frac{8}{9}&\frac{4}{9}\end{pmatrix} \cdot \begin{pmatrix}\frac{8}{9}\\\frac{1}{9}\\-\frac{4}{9}\end{pmatrix} = \frac{8}{81} + \frac{8}{81} - \frac{16}{81} = 0\]
   Hence \(A \in \OG(3)\).

   Next we determine an orthonormal basis of \(\mathbb{R}^3\) with the eigenvectors of \(A\).
   \[P_A = \det(A - \lambda I_3) = x^3 - 1\]
   \[(A - 1 \cdot I_3)v =
      \begin{pmatrix}
         -\frac{13}{9} & \frac{1}{9} & \frac{8}{9}\\
         \frac{4}{9} & -\frac{1}{9} & \frac{1}{9}\\
         -\frac{7}{9} & \frac{4}{9} & -\frac{13}{9}
         \end{pmatrix} \begin{pmatrix}v_1\\v_2\\v_3\end{pmatrix} = 0 \implies v_1 = \begin{pmatrix}1\\5\\3\end{pmatrix} \rightsquigarrow b_1 := \frac{v_1}{\|v_1\|} = \frac{1}{3\sqrt{3}}v_1
   \]
   % TODO: why?
   Since we only have one eigenvector we choose two vectors such that all three are orthogonal
   \[v_2 = \begin{pmatrix}1\\0\\-1\end{pmatrix} \rightsquigarrow b_2 := \frac{v_2}{\|v_2\|} = \frac{1}{\sqrt{2}}v_2 \qquad v_3 = \begin{pmatrix}5\\-2\\5\end{pmatrix} \rightsquigarrow b_3 := \frac{v_3}{\|v_3\|} = \frac{1}{3\sqrt{6}}v_3\]
   Hence we have the orthonormal basis \(\{b_1, b_3, b_2\}\) this gives us
   \[B = \begin{pmatrix}
         \frac{1}{3\sqrt{3}} & \frac{5}{3\sqrt{6}} & \frac{1}{\sqrt{2}}\\
         \frac{5}{3\sqrt{3}} & -\frac{2}{3\sqrt{6}} & 0\\
         \frac{1}{3\sqrt{3}} & \frac{5}{3\sqrt{6}} & -\frac{1}{\sqrt{2}}
   \end{pmatrix}\]
   so we can calculate \(B^TAB\)
   \[
      \begin{pmatrix}
         \frac{1}{3\sqrt{3}} & \frac{5}{3\sqrt{3}}  & \frac{1}{3\sqrt{3}}\\
         \frac{5}{3\sqrt{6}} & -\frac{2}{3\sqrt{6}} & \frac{5}{3\sqrt{6}}\\
         \frac{1}{\sqrt{2}}  & 0                    & -\frac{1}{\sqrt{2}}
      \end{pmatrix} \cdot
      \begin{pmatrix}
         -\frac{4}{9} & \frac{1}{9} & \frac{8}{9}\\
         \frac{4}{9} & \frac{8}{9} & \frac{1}{9}\\
         -\frac{7}{9} & \frac{4}{9} & -\frac{4}{9}
      \end{pmatrix} \cdot
      \begin{pmatrix}
         \frac{1}{3\sqrt{3}} & \frac{5}{3\sqrt{6}} & \frac{1}{\sqrt{2}}\\
         \frac{5}{3\sqrt{3}} & -\frac{2}{3\sqrt{6}} & 0\\
         \frac{1}{3\sqrt{3}} & \frac{5}{3\sqrt{6}} & -\frac{1}{\sqrt{2}}
      \end{pmatrix} =
      \begin{pmatrix}
         1 & 0 & 0\\
         0 & -\frac{1}{2} & -\frac{\sqrt{3}}{2}\\
         0 & \frac{\sqrt{3}}{2} & \frac{1}{2}
      \end{pmatrix}
   \]
   where we see the form from above:
   \[
      \begin{pmatrix}
         1 & 0 & 0\\
         0 & \cos\left(\frac{2\pi}{3}\right) & \sin\left(\frac{2\pi}{3}\right) \\
         0 & -\sin\left(\frac{2\pi}{3}\right) & \cos\left(\frac{2\pi}{3}\right)
      \end{pmatrix}
   \]
\end{example}

\begin{proposition}\label{pro:eigenbasis}
   Let \(V\) be a finite-dimensional, unitary vector space and \(f \in U(V)\).
   There exists an orthonormal basis consisting of eigenvectors of \(f\) and the corresponding eigenvalues have \(|\lambda| = 1\).
\end{proposition}

\subsection{Dual Space}
% TODO: duplicated
\begin{definition}[Dual Space]
   Given a K-vector space \(V\)
   \[V^* := \Hom_K(V, K)\]
\end{definition}
\begin{example}
   \[\Hom_K(V, W) := \{f: V \to W \mid f~\text{is linear}\}\]
   with the operations
   \[f \dotplus g := v \mapsto f(v) \dotplus g(v)\]
   \[\lambda \cdot f := v \mapsto \lambda \cdot f(v)\]
   is again a K-vector space.
\end{example}
\begin{remark}
   \(\dim(V^*) = \dim(V)\)
\end{remark}
\begin{remark}
   If \(V = K^n\) then has \(V^*\) also a standard basis \((e_{i}^*)_{i \in I}\) which is defined as
   \[e_j^*: K^n \to K~\text{through}~ e_i \mapsto \delta_{ij} \implies
      e^*_j(e_i) := \begin{cases}
         1:& i = j\\
         0:& i \neq j
   \end{cases}\]
   Now, given a linear map \(f: K^n \to K^m\) represented by \(A \in \Mat_{m,n}(K)\), the to \(f^*: K^{n*} \to K^{m*}\) corresponding matrix is \(A^T\).
\end{remark}

\begin{definition}[Dual Space Basis]
   Given a \(K\)-vector space \(V\) with a basis \((b_i)_{1 \leq i \leq n}\)
   \[b_i^\ast: V \to K \qquad\text{where}\qquad b_i^\ast(b_j) := \delta_{ij}\]
   forms a basis \((b_i^\ast)_{1 \leq i \leq n}\) of \(V^\ast\).
\end{definition}
\begin{remark}
   In the finite dimensional case we have \(V \cong V^\ast\).
\end{remark}
\begin{example}
   A linear map \(a: K^n \to K\) can always be written in matrix form, let \(x \in K^n\)
   \[a(x) = (a_1~\ldots~a_n)\begin{pmatrix}x_1\\\vdots\\x_n\end{pmatrix} = \sum_{i=1}^n a_ix_i\]
   which gives us the isomrphism
   \[(K^n)^\ast \to K^{1 \times n} \qquad\text{where}\qquad a \mapsto (a_1~\ldots~a_n)\]
\end{example}

\begin{definition}[Dual Map]
   Given a linear map \(f: V \to W\) between \(K\)-vector spaces
   \[f^\ast: W^\ast \to V^\ast \qquad\text{where}\qquad f^\ast(a) := a \circ f \in V^\ast\]
\end{definition}
\begin{example}
   \(A \in \Mat_{m,n}(K)\) defines the linear map \(f: K^n \to K^m\) through \(x \mapsto Ax\).
   So we have
   \[f^\ast: (K^m)^\ast \to (K^n)^\ast \quad\text{where}\quad (x \in K^m \mapsto (a_1~\ldots~a_m)x) \mapsto (y \in K^n \mapsto (a_1~\ldots~a_m)Ay)\]
   Hence we have \((K^n)^\ast \cong \Mat_{1,n}(K)\) and \((K^m)^\ast \cong \Mat_{1,m}(K)\).
   This means our dual map is
   \[\Mat_{1,n}(K) \to \Mat_{1,m}(K) \qquad\text{where}\qquad (a_1~\ldots~a_n) \mapsto (a_1~\ldots~a_n)A\]
   Thus the representation matrix of \(f^\ast: (K^n)^\ast \to (K^m)^\ast\) is \(A^T\).
\end{example}

\begin{definition}[Bi-Dual Space]
   The dual space of \(V^\ast\).
\end{definition}
\begin{remark}
   Through \(v \mapsto \big((f: V \to K) \mapsto f(v)\big)\) we have a linear map \(V \to V^{\ast\ast}\).
\end{remark}

\begin{proposition}
   Given a \(K\)-vector space \(V\), the linear map \(V \to V^\ast\) is injective.
\end{proposition}
\begin{remark}
   This comes natural: suppose \(W\) is another vector space an \(f: V \to W\) linear, then we have
   \begin{center}
      \begin{tikzcd}
         V \arrow{d} \arrow{r}{f} & W \arrow{d}\\
         V^{\ast\ast} \arrow{r}{f^{\ast\ast}} & W^{\ast\ast}\\
      \end{tikzcd}
   \end{center}
\end{remark}


\section{Multilinear Algebra}
\subsection{Multilinear Maps}
\begin{definition}[Bilinear Map]
   Given K-vector spaces \(U, V\) and \(W\).
   \[f: U \times V \to W\]
   is linear in \(U\) if
   \[\forall u, u' \in U,~v \in V: f(u + u', v) = f(u, v) + f(u', v) \quad\text{and}\quad \forall \lambda \in K: f(\lambda \cdot u, v) = \lambda \cdot f(u,v)\]
   and linear in \(V\) if
   \[\forall u \in U,~v, v' \in V: f(u, v + v') = f(u, v) + f(u, v') \quad\text{and}\quad \forall \lambda \in K: f(u, \lambda \cdot v) = \lambda \cdot f(u,v)\]
\end{definition}
\begin{remark}
   So a map \(f: V_1 \times \ldots \times V_n \to W\) with those properties is a multilinear map.
\end{remark}

\begin{definition}[Symmetric Multilinear Map]
   Given \(f: V_1 \times \ldots \times V_n \to W\) where
   \[f(\ldots, v, \ldots, v', \ldots) = f(\ldots, v', \ldots, v, \ldots)\]
\end{definition}

\begin{definition}[Skew Symmetric Multilinear Map]
   Given \(f: V_1 \times \ldots \times V_n \to W\) where
   \[f(\ldots, v, \ldots, v', \ldots) = -f(\ldots, v', \ldots, v, \ldots)\]
\end{definition}

\begin{definition}[Alternating Multilinear Map]
   Given \(f: V_1 \times \ldots \times V_n \to W\) where
   \[f(\ldots, v, \ldots, v, \ldots) = 0\]
\end{definition}

\begin{remark}
   \[s~\text{alternating} \implies s~\text{skewsymmetric}\]
   and if \(\chark(K) \neq 2\) then
   \[s~\text{alternating} \iff s~\text{skewsymmetric}\]
\end{remark}

\begin{remark}
   We can regard a matrix \(A \in \Mat_n(K)\) as an \(n\) dimensional column vector whose entries are  \(n\) dimensional row vectors.
   This way we can identify \(\Mat_n(K) \cong V^n\) where \(V := K^n\) and have a multilinear map \(\Mat_n \to V^n\).
\end{remark}

\begin{lemma}\label{lem:identity_multilinear}
   Let \(n \in \mathbb{N}_{>0}\) and \(f: \Mat_n(K) \to K\) be a skew symmetric multilinear map, then holds
   \[\forall \sigma \in S_n: f(P_\sigma) = \sign(\sigma) \cdot f(I_n)\]
\end{lemma}

\subsubsection{Bilinear Forms}
In this section we will introduce the underlying structure of the introductory example above.

\begin{definition}[Bilinear Form]
   A bilinear map \(s: V \times V \to K\) for a \(K\)-vector space \(V\).
\end{definition}
\begin{remark}[Polarization Identity]
   For a symmetric bilinear form \(s\) holds
   \[s(v, w) = \frac{s(v + w, v + w) - s(v, v) - s(w, w)}{2}\]
\end{remark}

\begin{definition}[Positive-Definite Bilinear Form]
   A symmetric bilinear form \(s\) on an \(\mathbb{R}\)-vector space \(V\) where
   \[\forall v \in V \setminus \{0\}: s(v, v) > 0\]
\end{definition}

\subsubsection{Sesquilinear Forms}
Also for complex vector spaces there is a structure from which we can derive a norm and metric.
\begin{definition}[Sesquilinear Form]
   A bilinear map \(s: V \times V \to \mathbb{C}\) on a \(\mathbb{C}\)-vector space \(V\) where
   \[\forall \lambda \in \mathbb{C}: s(v, \lambda w) = \overline{\lambda}s(v, w)\]
\end{definition}

\begin{definition}[Hermitian Sesquilinear Form]
   A sesquilinear form \(s\) where
   \[\forall v,w \in V: s(v, w) = \overline{s(w, v)}\]
\end{definition}
\begin{remark}[Polarization Identy]
   For a hermitian sesquilinear form \(s\) holds
   \[s(v,w) = \frac{s(v+w, v+w) - s(v-w, v-w) + is(v+iw, v+iw) - is(v-iw, v-iw)}{4}\]
\end{remark}

\begin{definition}[Positive Definite Sesquilinear Form]
   A sesquilinear form \(s\) where
   \[\forall v \in V\setminus\{0\}: s(v, v) > 0\]
\end{definition}

\subsubsection{Bilinear Forms \& Matrices}
By choosing a basis \((b_i)_{1 \leq i \leq n}\) for a \(K\)-vector space, we can represent bilinear forms over \(V\) through matrices over \(K\).

\begin{proposition}
   Given a basis \((b_i)_{i \leq 1 \leq n}\) of a \(K\)-vector space \(V\), then is
   \[f: \{s: V \times V \to K\} \xrightarrow{\sim} \Mat_n(K) \qquad\text{where}\qquad s \mapsto \big(s(b_i, b_j)\big)_{1 \leq i,j \leq n}\]
   a bijective map, identifying bilinear forms with matrices, where we have
   \begin{enumerate}[label=\roman*, align=Center]
      \item Symmetry: \(s(v, w) = s(w, v) \iff A = A^T\)
      \item Skewsymmetry: \(s(v, w) = -s(w, v) \iff A = -A^T\)
      \item Alternation: \(s(v, v) = 0 \iff A = -A^T~\text{and}~\forall 1 \leq i \leq n: a_{ii} = 0\)
      \item Hermitian: \(s(v, w) = \overline{s(w, v)} \iff A = A^H\)
   \end{enumerate}
\end{proposition}

Having two different bases \(B, B' = K^n\) we get the following commutative diagram
\begin{center}
   \begin{tikzcd}
      B\times B \arrow{dd}[swap]{L_U\times L_U} \arrow{drr}[swap]{\varphi_{B}\times\varphi_{B}} \arrow{drrrr}{(\lambda,\mu)\mapsto\lambda^TA\mu}& & & & \\
                                                                                                                                               & & V \times V \arrow{rr}{s} & & K \\
      B'\times B' \arrow{urr}{\varphi_{B'}\times\varphi_{B'}} \arrow{urrrr}[swap]{(\lambda, \mu) \mapsto\lambda^TA'\mu} & & & &
   \end{tikzcd}
\end{center}
As with the representation matrix of a linear map between two bases we have in this case
\[A = U^T \cdot A' \cdot U\]
with the analogue for unitary spaces
\[A = U^T \cdot A' \cdot \overline{U}\]
which means that \(A\) and \(A'\) are \emph{congruent} (\ref{def:congr_mat}).

\begin{definition}[Rank of Bilinear Forms]
   Given a bilinear form \(s\) with representation matrix \(A\).
   \[\rk(s) := \rk(A)\]
\end{definition}

\subsubsection{Degenerate Bilinear Forms}
\begin{definition}[Degenerate Bilinear Form]
   Given a bilinear form \(s: V\times V \to K\) where
   \[\exists v \in V: s(v, w) = 0~\forall w \in V\]
\end{definition}
\begin{remark}
   Equivalently if \(s(w, v) = 0\).
\end{remark}
This brings us to the subspace
\[\{v \in V \mid \forall w \in V: s(v, w) = 0\} \subset V\]
where we also have the subspace with \(s(w, v) = 0\).
In important cases both are the same subspace as we see from the following
\begin{proposition}\label{pro:degen_bilin}
   Let \(s: V \times V \to K\) be a bilinear- or sesquilinear form.
   If
   \begin{enumerate}[label=\roman*, align=Center]
      \item \(s\) is symmetric,
      \item \(s\) is skewsymmetric,
      \item \(s\) is hermitian,
   \end{enumerate}
   then holds \(s(v, w) = 0 \iff s(w, v) = 0\) for some \(v, w \in V\).
   This means that if \(s\) is degenerate, then their \emph{degenerate subspaces} are equivalent.
   \[\{v \in V \mid \forall w \in V: s(v, w) = 0\} = \{v \in V \mid \forall w \in V: s(w, v) = 0\}\]
\end{proposition}

\begin{definition}[Degenerate Subspace]
   Given a degenerate bilinear form \(s\)
   \[\{v \in V \mid \forall w \in V: s(v, w) = 0\} \subset V\]
\end{definition}
\begin{remark}
   Formally we would distinguish between the \emph{left-} and \emph{right kernel}.
\end{remark}

\begin{proposition}
   In every case of \cref{pro:degen_bilin} with the degenerate subspace \(U\) we can define
   \[\bar{s}: V_{/U}\times V_{/U} \to K \qquad\text{where}\qquad (\bar{v}, \bar{w}) \mapsto s(v, w)\]
   a \emph{nondegenerate} bilinear form, which is symmetric iff \(s\) is symmetric etc.

   Additionally if \(\chark(K) = 2\) then is \(\bar{s}\) alternating iff \(s\) is alternating.
\end{proposition}

\begin{corollary}\label{cor:degen_space_sum}
   Given a complement \(W \subset V\) of the degenerate subspace \(U\) in the situation above, then is
   \[V = U \oplus W\]
   where we have \(s\rvert_U = 0\) for \(U\) and we identify \(s\rvert_W\) for \(W\) with \(\bar{s}\) through a map \(W \xrightarrow{\sim} V_{/U}\).
\end{corollary}
\begin{example}
   We regard the vector space \(\mathbb{R}[x]_{\leq 2}\) of \(\dim = 3\) with the basis \(\{1, x, x^2\}\) with the bilinear form
   \[s(p, q) := p(0)q(1) - p(1)q(0)\]
   To calculate the degenerate subspace we regard \(p \in \mathbb{R}[x]_{\leq 2}\) where
   \[\forall q \in \mathbb{R}[x]_{\leq 2}: s(p, q) = 0 \implies p(0) = p(1) = 0 \iff p \in \spanv(x^2 - x) \implies U = \spanv(x^2 -x)\]
   We could have also looked at the representation matrix of \(s\) which is given through
   \[s(1, 1) = 0 \quad s(1, x) = 1 \cdot 1 - 1 \cdot 0 = 1 \quad s(1, x^2) = 1\]
   \[A = \begin{pmatrix}0 & 1 & 1\\-1 & 0 & 0\\-1 & 0 & 0\end{pmatrix} \rightsquigarrow \ker(A) = \spanv\begin{pmatrix}0\\-1\\1\end{pmatrix} \rightsquigarrow -x + x^2\]
\end{example}

\begin{proposition}\label{pro:nondegen_direct_sum}
   In every case of \cref{pro:degen_bilin} holds for \(W \subset V\) that
   \begin{enumerate}[label=\roman*, align=Center]
      \item if \(s\) is nondegenerate, then is \(\dim(V) = \dim(W) + \dim(W^\perp)\).
      \item if also \(s\rvert_W\) is nondegenerate, then is \(V = W \oplus W^\perp\).
   \end{enumerate}
\end{proposition}

In a situation as in the proposition above we can choose a basis \((b_i)_{1\leq i \leq n}\) of \(V\) and represent \(s\) as a matrix.
Then is the kernel of \(s\) the degenerate subspace, hence \(s\) is degenerate iff \(\rk(s) < \dim(V)\).
In the case where \(W\) is spanned by \((b_i)_{1 \leq i \leq m}\) with \(m \leq n\) and the representation matrix has the left upper \(m\times m\)-block \(A_{11}\), then is \(s\rvert_W\) degenerate iff \(\rk(A_{11}) < m\).

\begin{definition}[Quadratic Form]
   Given a bilinear form \(s\) over a \(K\)-vector space \(V\),
   \[q: V \to K \qquad\text{where}\qquad v \mapsto s(v, v)\]
   is a quadratic form iff
   \[\forall \lambda \in K, v \in V: q(\lambda \cdot v) = \lambda^2 \cdot q(v)\]
   and the map
   \[(v, w) \mapsto q(v + w) - q(v) - q(w)\]
   is bilinear
\end{definition}
\begin{remark}
   In the case where \(\chark(K) \neq 2\) we can derive a bilinear form from a quadratic form through the polarization identity.
   This way we have a bijective mapping between symmetric bilinear forms and quadratic forms.
\end{remark}

\begin{definition}[Definiteness of Bilinear Form]
   We call a symmetric bilinear form \(s\) on a real vector space \(V\)
   \begin{itemize}
      \item Negative Definite iff \(\forall v \in V \setminus \{0\}: s(v, v) < 0\).
      \item Positive Semidefinite iff \(\forall v \in V \setminus \{0\}: s(v, v) \geq 0\).
      \item Negative Semidefinite iff \(\forall v \in V \setminus \{0\}: s(v, v) \leq 0\).
   \end{itemize}
\end{definition}
\begin{remark}
   Through the bijective mapping of bilinear and quadratic forms we have that if a bilinear map is negative definite the corresponding quadratic form is negative definite as well
\end{remark}

\begin{proposition}\label{pro:diag_repr_mat}
   Given a symmetric bilinear form \(s: V \times V \to K\) where \(\chark(K) \neq 2\), then there exists a basis of \(V\), such that \(s\) can be represented by a diagonal matrix.
\end{proposition}

\begin{proposition}[Sylvester's Law of Inertia]\label{pro:sylv_law_inertia}
   Let \(V\) be a finite dimensional real vector space, \(s\) a symmetric bilinear form and \(V_0\) the degenerate subspace of \(V\).
   We regard the subspaces \(V_+, V_- \subset V\) with maximal dimension such that
   \[\forall v \in V_+ \setminus \{0\}: s(v, v) > 0 \qquad\text{and}\qquad \forall v \in V_- \setminus \{0\}: s(v, v) < 0\]
   i.e. \(s\rvert_{V_+}\) is positive definite and \(s\rvert_{V_-}\) is negative definite.
   Then is
   \[V = V_+ \oplus V_- \oplus V_0\]
   hence there exists a basis such that \(s\) is represented by a matrix of the form
   \[\begin{pmatrix}
         I_{n_+} & 0       & \cdots & \cdots & 0\\
         0       & I_{n_-} & \ddots &        & \vdots \\
         \vdots  & \ddots  & 0      & \ddots & \vdots \\
         \vdots  &         & \ddots & \ddots & 0 \\
         0       & \cdots  & \cdots & 0      & 0\\
      \end{pmatrix}\]
      where \(n_+ := \dim(V_+)\) and \(n_- := \dim(V_-)\).
\end{proposition}
\begin{remark}
   In this situation holds that \(n_0 := \dim(V_0) = \dim(V) - n_+ - n_-\).
   Then we call
   \[\sigma(s) := (n_+, n_-, n_0)\]
   the \emph{Sylvester signature} of \(s\).
   If \(s\) is non-degenerate i.e. \(n_0 = 0\) then we write \(\sigma(s) = (n_+, n_-)\).
\end{remark}

\subsection{Universal Properties}
A \emph{universal property} is a method in mathematics, especially in abstract algebra, to arrive at a desired structure, without a concrete construction.
To put it differently: Once we realise that a particular construction suffices some universal property, we can forget all the complex, technical details of the construction, since everything noteworthy about it is described by the universal property.

Furthermore, universal properties define objects except for isomrophisms.
However by showing that two objects suffice the same universal property one can prove isomorphy of the objects.

As an example we will look at how the index set of a basis for a direct sum can be constructed.
Suppose we have the \(K\)-vector spaces \(U, W\) with \(\dim(U) = m\) and \(\dim(W) = n\).
With the sets
\[I_0 := \{1, 2, \ldots, m\} \qquad\text{and}\qquad I_1 := \{1, 2, \ldots, n\}\]
we index the bases \((u_i)_{i \in I_0}\) respectively \((w_i)_{i \in I_1}\).
Since some of the indices in \(I_0\) and \(I_1\) are equal we need their \emph{disjoint union} to index the basis
\[\{u_1, \ldots, u_m, w_1, \ldots, w_n\} \quad\text{of}\quad V = U \oplus W\]
This means we want to union \(I_0\) and \(I_1\) in such a way, that their duplicated elements are not collapsed.

As you can imagine there are a few ways to construct the disjoint union.
Either we can manually construct a set with the same number of unique elements
\[A := \{1, 2, \ldots, m + n\}\]
or by using the set theory definition
\[B := \{(x, 0) \mid x \in I_0\} \cup \{(x, 1) \mid x \in I_1\}\]
However, the concept of a disjoint union can be characterized as follows:

A disjoint union \(I_0 \sqcup I_1\) is a set with the maps
\[f_0: I_0 \to I_0 \sqcup I_1 \qquad\text{and}\qquad f_1: I_1 \to I_0 \sqcup I_1\]
and the universal property, that for an arbitrary set \(T\) with the maps
\[g_0: I_0 \to T\qquad\text{and}\qquad g_1: I_1 \to T\]
there exists a unique map \(h: I_0 \sqcup I_1 \to T\) such that \(g_0 = h \circ f_0\) and \(g_1 = h \circ f_1\).

We can state the same through a commutative diagram:
\begin{center}
   \input{drawings/universal_properties.tex}
\end{center}
where the dashed arrow denotes ``there exists a unique''.

Using this universal property we can show that both \(A\) and \(B\) are disjoint unions.
We use
\[k: i \mapsto \begin{cases}(i, 0) & \text{if}~i \in [1; m]\\(i-m, 1) & \text{if}~i \in [m+1;m+n]\end{cases} \qquad\text{resp.}\qquad l: \begin{cases}(i, 0) \mapsto i\\(i, 1) \mapsto i + m\end{cases}\]
and see through the diagrams in fact that \(l \circ k = \id_A\) and \(k \circ l = \id_B\)
 \[
    \begin{tikzcd}
       I_0 \arrow[dr, near start, "i \mapsto i"] \arrow{dddr}[swap]{i \mapsto (i, 0)} &                & I_1 \arrow[dl, near start, swap, "i \mapsto i+m"] \arrow{dddl}{i \mapsto (i, 1)}\\
                                                                                    & A \arrow{dd}{k} & \\
                                                   &   & \\
                                                   & B & \\
   \end{tikzcd} \qquad\qquad
   \begin{tikzcd}
      I_0 \arrow[dr, near end, "{i \mapsto (i, 0)}"] \arrow{dddr}[swap]{i \mapsto i} &                & I_1 \arrow[dl, near start, swap, "{i \mapsto (i,1)}"] \arrow{dddl}{i \mapsto i+m}\\
                                                                                   & B \arrow{dd}{l} & \\
                                                  &   & \\
                                                  & A & \\
   \end{tikzcd}
\]
This means we have a bijective map between the two differently constructed disjoint unions.

This illustrates what universal properties are used for:
We have different ways of constructing an object and arrive through their common universal property at the conclusion that both constructions are in fact isomorphic.

\begin{definition}[Universal Property of Quotient Field]
   Given an integral domain \(R\), a field \(K\) with the ring homomorphism \(g: R \hookrightarrow K\) has the property that for every field \(L\) with a ring homomorphism \(f: R \hookrightarrow L\), there exists a unique field embedding \(h: K \to L\) such that \(f = g \circ h\).
   \begin{center}
      \begin{tikzcd}
         R \arrow[dr, hook, swap, "f"]\arrow[r, hook, "g"] & K \arrow[d, dashrightarrow, "h"]\\
                                                    & L
      \end{tikzcd}
   \end{center}
\end{definition}

\subsection{Tensor Product}
In this section we will introduce the \emph{tensor product} \(V \oplus_K W\) of two \(K\)-vector spaces \(V\) and \(W\).
We will see a description --- namely a \emph{universal property} --- which defines the tensor product uniquely except for isomorphisms.

Before we get into the thick of things we should clarify some basic terminology.
\emph{Tensors} are just vectors in a special vector space.
Weâll see that such a vector space comes about by combining two smaller vector spaces via a \emph{tensor product}.
So the tensor product is an operation combining vector spaces, and tensors are the elements of the resulting vector space.
The main intrinsic motivation for the rest of this section will be this:

We have all these interesting mathematical objects, but over the years we have discovered that the \emph{maps} between objects are the truly interesting things.
Here are some examples of nice multilinear maps that show up everywhere:
\begin{itemize}
   \item If \(V\) is an inner product space over \(\mathbb{R}\), then the inner product is bilinear.
   \item The determinant of a matrix is a multilinear map if we view the columns of the matrix as vector arguments.
   \item The cross product of vectors in \(\mathbb{R}^3\) is bilinear.
\end{itemize}
There are many other examples and itâs enough to convince us that multilinearity is worth studying abstractly.

And so what tensors do is give a sort of classification of multilinear maps.
The idea is that every bilinear map
\[h: V \times W \to T\]
to any vector space \(T\) can be written first as a bilinear map to the tensor space
\[\varphi: V \times W \to V \otimes W\]
Followed by a linear map to \(T\),
\[\tilde{h} : V \otimes W \to T\]
And the important part is that \(\varphi\) doesnât depend on the original \(h\) (but \(\tilde{h}\) does).

In fuzzy words, the tensor product is like the gatekeeper of all multilinear maps, and \(\varphi\) is the gate.
Yet another way to say this is that \(\varphi\) is the most general possible multilinear map that can be constructed from \(V_1 \times \dots \times V_n\).
Moreover, the tensor product itself is uniquely defined by having a ''most-general`` \(\varphi\) (up to isomorphism).
This notion is often referred to by mathematicians as the \emph{universal property} of the tensor product.

\begin{definition}[Universal Property of Tensor Product]\label{def:uni_prop_tensor}
   Given the \(K\)-vector spaces \(V, W\), their tensor product \(V \otimes_K W\) and the associated bilienar map
   \[\varphi: V \times W \to V \otimes W \qquad\text{where}\qquad (v, w) \mapsto v \otimes w\]
   have the property that for a vector space \(T\), any bilinear map \(h: V \times W \to T\) factors through \(\varphi\) uniquely, i.e. there exists a unique linear map \(\tilde{h}: V \otimes W \to T\) such that \(h = \tilde{h} \circ \varphi\).
   \begin{center}
      \begin{tikzcd}
         V \times W \arrow{dr}[swap]{h}\arrow{r}{\varphi} & V \otimes W \arrow[d, dashrightarrow, "\tilde{h}"]\\
                                                    & T
      \end{tikzcd}
   \end{center}
\end{definition}
\begin{remark}
   This definition can be generalized to multiple dimensions.

   The tensor product \(V_1 \otimes \ldots \otimes V_n\) with the \emph{multilinear} map \(\varphi: V_1 \times \ldots \times V_n \to V_1 \otimes \ldots \otimes V_n\) have the universal property, that for any \(T\) and multilinear map \(h: V_1 \times \ldots \times V_n \to T\) there exists the linear map \(\tilde{h}: V_1 \otimes \ldots V_n \to T\) such that \(h = \tilde{h} \circ \varphi\).
\end{remark}

So now we look at an example with \(V = K^m\) and \(W = K^n\) to see abstractly that all maps \(h: K^m \times K^n \to T\) can be decomposed into a \(\varphi\) part and a \(\tilde{h}\) part.
To do this we need to define the map \(h\) uniquely.
For usual linear maps, all we had to do was show the effect of the map on each element of a basis; the rest was uniquely determined by the linearity property.
So suppose we take the standard bases for both spaces, then is a basis for \(K^m \times K^n\) is just
\[\{(e_1, 0), \ldots, (e_m, 0), (0, e_1), \ldots, (0, e_n)\}\]

But multilinear maps are more nuanced, because they have two arguments.
In order to say ''what \(h\) does on a basis`` we really need to know how it acts on all \(n \cdot m\) possible pairs of basis elements, since \(v \in K^m\) consists of any combination of \((e_i, 0)\) and \(w \in K^n\) likewise of \((0, e_j)\).
So by determining
\[h(e_i, e_j)~\forall 1 \leq i \leq m,~1 \leq j \leq n\]
we can define \(h\).

Uncoincidentally, as \(K^m \otimes K^n\) is a vector space, its basis can also be constructed in terms of the standard bases, by simply taking all \(m \cdot n\) possible tensors \(e_i \otimes e_j\).
So by defining
\[e_{(i-1)n+j} := e_i \otimes e_j\]
we have a basis \(\{e_1, \ldots, e_{mn}\}\) for \(K^{mn}\) hence \(K^m \otimes K^n \cong K^{mn}\).
Since every \(v \in K^m, w \in K^n\) can be written in terms of their bases, itâs clear than any tensor can also be written in terms of the basis tensors \(e_i \otimes e_j\):
\[v \otimes w = \varphi(v, w) = \varphi\left(\sum_{i=1}^m v_i e_i, \sum_{j=1}^n w_j e_j\right) = \sum_{i=1}^m \sum_{j=1}^n v_iw_j \varphi(e_i, e_j) = \sum_{i=1}^m \sum_{j=1}^n v_iw_j e_i \otimes e_j\]
This defines \(\varphi\) uniquely, so all we have left to do is use the ''data`` given by \(h\) (the effect on pairs of basis elements) to define \(\tilde{h}: V \otimes W \to T\).
\[\forall 1\leq i\leq m, 1\leq j \leq n: \tilde{h}(e_i \otimes e_j) := h(e_i, e_j)\]
Through this we see that for any \(v \otimes v \in K^m \otimes K^n\) we have
\[\tilde{h}(v \otimes w) = \sum_{i=1}^m \sum_{j=1}^n v_i w_j \tilde{h}(e_i \otimes e_j) = \sum_{i=1}^m \sum_{j=1}^n v_i w_j h(e_i, e_j) = h(v, w)\]
Hence we didnât make any choices in constructing \(\tilde{h}\).
If you retrace our steps in the argument, youâll see that everything was essentially decided for us once we fixed a choice of a basis.
Since the construction would be isomorphic if we changed the basis, our choice of \(\tilde{h}\) is unique.

% TODO: Inner Product
% The analogue to the euclidean \emph{inner product} which produces a scalar from two vectors \(v, w\)
% \[v^T \cdot w = \begin{pmatrix}v_1~\ldots~v_n\end{pmatrix} \cdot \begin{pmatrix}w_1\\\vdots\\w_n\end{pmatrix} = \sum_{i=1}^n v_i \cdot w_i\]
% is the \emph{outer product} which produces a matrix from two vectors
% \[v \otimes w := v \cdot w^T = \begin{pmatrix}v_1\\v_2\\v_3\\v_4\end{pmatrix} \cdot \begin{pmatrix}w_1~w_2~w_3\end{pmatrix} = \begin{pmatrix}
%    v_1w_1 & v_1w_2 & v_1w_3\\
%    v_2w_1 & v_2w_2 & v_2w_3\\
%    v_3w_1 & v_3w_2 & v_3w_3\\
%    v_4w_1 & v_4w_2 & v_4w_3\\
% \end{pmatrix}\]

\begin{proposition}
   Given the \(K\)-vector spaces \(V, W\) with bases \((v_i)_{i \in I}\) and \((w_j)_{j \in J}\), there exists a unique bilinear map
   \[\varphi: V \times W \to \bigoplus_{(i,j) \in I \times J} K \qquad\text{where}\qquad (v_i, w_j) \mapsto e_{(i,j)}\]
   where \((e_{(i,j)})_{(i,j) \in I \times J}\) is the standard basis of the direct sum.
   \(V \times W\) with this map fullfills \cref{def:uni_prop_tensor}.
\end{proposition}

\begin{proposition}[Tensor Identities]
   For any \(v \in V\), \(u, w \in W\) and \(a \in K\) holds
   \begin{enumerate}[label=\roman*, align=Center]
      \item \(v \otimes w + u \otimes w = (v + u) \otimes w\)
      \item \(v \otimes w + v \otimes u = v \otimes (w + u)\)
      \item \(a v \otimes w = a(v \otimes w) = v \otimes aw\)
   \end{enumerate}
   i.e. \(\otimes: V \times W \to V \otimes W\) where \((v, w) \mapsto v \otimes w\) is bilinear.
\end{proposition}

\begin{example}[Kronecker Product]
   Given two linear maps \(f: V \to W\) and \(g: V' \to W'\) we have the linear map
   \[f \otimes g: V \otimes W \to V' \otimes W' \qquad\text{where}\qquad v \otimes w \mapsto f(v) \otimes g(w)\]
   If the vector spaces are finite-dimensional, we can represent \(f, g\) through matrices regarding some bases.
   This way we can write for \(f \otimes g\) (suppose in \(\mathbb{R}^2\))
   \[A \otimes B = \begin{pmatrix}a_{11}B & a_{12} B\\ a_{21} B & a_{22} B\end{pmatrix} = \begin{pmatrix}
      a_{11}b_{11} & a_{11}b_{12} & a_{12}b_{11} & a_{12}b_{12} \\
      a_{11}b_{21} & a_{11}b_{22} & a_{12}b_{21} & a_{12}b_{22} \\
      a_{21}b_{11} & a_{21}b_{12} & a_{22}b_{11} & a_{22}b_{12} \\
      a_{21}b_{21} & a_{21}b_{22} & a_{22}b_{21} & a_{22}b_{22} \\
   \end{pmatrix}\]
\end{example}

\begin{example}
   Given the \(K\)-vector spaces \(V, W\), the map
   \[v \otimes w \mapsto w \otimes v\]
   is an isomorphism, hence \(V \otimes W \cong W \otimes V\).
\end{example}

\begin{example}
   Given the \(K\)-vector spaces \(V, W\) and a basis \((v_i)_{i \in I}\) of \(V\), the map
   \[v_j \otimes w \mapsto (\delta_{ij} \cdot w)_{i \in I}\]
   and isomorphism, hence \(V \otimes W \cong \bigoplus_{j \in J} V\).
   If \((w_i)_{i \in I}\) is a basis of \(W\) we get \(V \otimes W \cong \bigoplus_{j \in J} V\).
\end{example}

\begin{proposition}
   Let \(K\) be a field, \(L/K\) a field extension and \(V, W\) \(K\)-vector spaces with a linear map \(f: V \to W\).

   \(L \otimes V\) where \(\forall a, b \in L, v \in V: b \cdot (a \otimes v) := ab \otimes v\) is an \(L\)-vector space and
   \[\id_L \otimes f: L \otimes V \to L \otimes W\]
   is \(L\)-linear.
\end{proposition}

\subsubsection{Symmetric Power \& Exterior Product}
\begin{definition}[\(n\)-th Tensor Product]
   We regard the \(K\)-vector spaces \(V_1 = \ldots = V_n\)
   \[\bigotimes_{i=1}^n V_i := V_1 \otimes \ldots \otimes V_n\]
\end{definition}

\begin{definition}[Universal Property of Symmetric Power]
   Given a \(K\)-vector space, the symmetric power \(\Sym^nV\) and its associated symmetric multilinear map
   \[\varphi: V^n \to \Sym^nV \qquad\text{where}\qquad v \mapsto v_1 \cdot \ldots \cdot v_n\]
   have the property that for a vector space \(T\), any symmetric multilinear map \(h: V^n \to T\) factors through \(\varphi\) uniquely i.e.
   \begin{center}
      \begin{tikzcd}
         V^n  \arrow{dr}[swap]{h}\arrow{r}{\varphi} & \Sym^nV \arrow[d, dashrightarrow, "\tilde{h}"]\\
                                                    & T
      \end{tikzcd}
   \end{center}
\end{definition}
\begin{remark}
   For \(V = K^m\) with the standard basis we have
   \[\Sym^nV \cong \bigoplus_{1 \leq i_1 \leq \ldots \leq i_n \leq m}K\]
\end{remark}

\begin{definition}[Universal Property of Exterior Product]
   Given a \(K\)-vector space, the exterior product \(\bigwedge^n(V)\) and its associated alternating multilinear map
   \[\varphi: V^n \to \bigwedge^nV \qquad\text{where}\qquad v \mapsto v_1 \wedge \ldots \wedge v_n\]
   have the property that for a vector space \(T\), any alternating multilinear map \(h: V^n \to T\) factors through \(\varphi\) uniquely i.e.
   \begin{center}
      \begin{tikzcd}
         V^n  \arrow{dr}[swap]{h}\arrow{r}{\varphi} & \bigwedge^nV \arrow[d, dashrightarrow, "\tilde{h}"]\\
                                                    & T
      \end{tikzcd}
   \end{center}
\end{definition}
\begin{remark}
   For \(V = K^m\) with the standard basis we have
   \[\bigwedge^nV \cong \bigoplus_{1 \leq i_1 \leq \ldots \leq i_n \leq m}K\]
\end{remark}

\subsection{Tensor Product of Moduln}
% TODO: finish
% TODO: move to mapping spaces
\begin{proposition}
   Let \(R\) be a commutative ring and \(A, B\) be \(R\)-Moduln.

   \(\Hom(A, B)\) has the structure of an \(R\)-Module with
   \[f+g: A \to B \qquad a \mapsto f(a) + g(a)\]
   \[r \cdot f: A \to B \qquad a \mapsto r \cdot f(a)\]

   Furthermore if \(h: A' \to A\) and \(k: B' \to B\) are homomorphisms, then are
   \[f \mapsto f \circ h \qquad f \mapsto k \circ f\]
   homomorphisms.
\end{proposition}
\begin{example}
   We have \(\Hom(R, A) \xrightarrow{\sim} A\) by \(f \mapsto f(1)\).
   This is an R-module homomorphism, which is injective
   \[f(1) = 0 \implies f(r) = f(r1) = r(f(1)) = r0 = 0 \forall r \in R\]
   and surjective
   \[\text{for}~a \in A \exists (r \mapsto ra) \in \Hom(R, A)~\text{which sends 1 to a}\]
   The module \(\Hom(A, R)\) is the \emph{Dual Module} to A, often denoted by \(A^\ast\).

   There is a natural map (i.e. compatible with \(A' \to A\)) \(A \to A^{\ast\ast}\) to the bidual, sending \(a \in A\) to the evaluation map at \(a\).
   When this is an isomorphism we say that \(A\) is a \emph{reflexive} \(R\)-module.
\end{example}
